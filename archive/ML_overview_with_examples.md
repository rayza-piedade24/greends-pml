<a href="https://colab.research.google.com/github/isa-ulisboa/greends-pml/blob/main/ML_overview_with_examples.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

**Practical Machine Learning**

Masters in Green Data Science, ISA/ULisboa, 2023-2024

Instructor: Manuel Campagnolo mlc@isa.ulisboa.pt

# Overview of Machine Learning (ML)

In this course we are dealing with data sets of *labeled examples*. Examples can be scalar numbers, rows of tabular data, images, etc. For tabular data, we refer the to columns as *explanatory variables* (sometimes also called *independent* or *descriptive* variables).

Labels can be categorical, ordinal or continuous. Labels can be refered to as the *response variable* (or *dependent* variable). They are also called *targets*. Typically, we the problems are called:
1. *Regression problems*, when the labels are continuous.
2. *Classification problems*, when the labels are categorical.

The distinction is not always clear. Some problems can be considered either as regression or classification problems.

Given a ML problem, i.e. a set of labeled examples, the goal is to build a function $f$ that maps examples to labels or, in other words, that predicts the label from the example.

The outputs of $f$ are called *predictions* or *predicted values*, and the actual labels of the examples are called *actual values* or *target values*.




## Python packages

In this ML course, the main Python packages are:

1. **Scikit-learn**: A high-level package build on `NumPy`, `SciPy`, and `matplotlib` which covers most ML techniques except deep learning;  https://scikit-learn.org/stable/index.html.

2. **Pytorch**: PyTorch is an optimized tensor library for deep learning using GPUs and CPUs;  https://pytorch.org/docs/stable/index.html

3. **Tensor Flow**: the alternative to PyTorch from Google.


## Data visualization, pre-processing and feature engineering

Discriminant analysis is a linear technique that helps to visualize numerical data for classification problems. Library `scikit-learn` provides the `LinearDiscriminantAnalysis` (LDA) for that purpose. LDA determines the axis along which between-class variance over within-class variance is largest.



```python
#@title Script to project the iris data set on the 1st discriminant axis
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_iris
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
import pandas as pd

# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target
target_names = iris.target_names

# Perform Linear Discriminant Analysis
lda = LinearDiscriminantAnalysis(n_components=1)
X_lda = lda.fit_transform(X, y)

# Combine the transformed data and target labels into a DataFrame
data = {'LDA Component 1': X_lda.squeeze(), 'Class': target_names[y]}
df = pd.DataFrame(data)

# Plot the result
plt.figure(figsize=(8, 6))
sns.kdeplot(data=df, x='LDA Component 1', hue='Class', fill=True, common_norm=False)
plt.xlabel('LDA Component 1')
plt.title('Density of Classes over Linear Discriminant Axis')
plt.show()
```


    
![png](ML_overview_with_examples_files/ML_overview_with_examples_7_0.png)
    


If there are more than 2 classes, we can define more than one discriminant axis. In particular, it is easy to visualize data projected onto the first discriminant plane as in the following example.


```python
#@title Script to project the iris data set on the 1st discriminant plane

import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Perform Linear Discriminant Analysis
lda = LinearDiscriminantAnalysis(n_components=2)
X_lda = lda.fit_transform(X, y)

# Plot the result
plt.figure(figsize=(8, 6))
for i, target_name in enumerate(iris.target_names):
    plt.scatter(X_lda[y == i, 0], X_lda[y == i, 1], label=target_name)

plt.xlabel('LDA Component 1')
plt.ylabel('LDA Component 2')
plt.title('Linear Discriminant Analysis')
plt.legend()
plt.show()

```


    
![png](ML_overview_with_examples_files/ML_overview_with_examples_9_0.png)
    


Linear visualization techniques are not flexible enough to address problems where classes (example with the same labels) are not, at least approximately, linearly separable. There are many non-linear techniques which can be applied to large data sets that also perform dimensionality reduction and permit visualization of very complex data sets.

Two relevant papers that describe fast techniques for large data sets:
- [t-SNE-CUDA: GPU-Accelerated t-SNE and its Applications to Modern Data](https://arxiv.org/pdf/1807.11824)
- [UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction](https://arxiv.org/pdf/1802.03426)

The following script illustrates those techniques, and compares them with LDA, for creating 2-dimensional and 3-dimensional plots. In the examples, three data sets are compared: `digits`, `wine` and `wine quality`.


```python
#@title Script to apply dimensionality reduction techniques t-SNE, UMAP and LDA to several data sets
!pip install umap-learn[plot]
!pip install ucimlrepo
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.manifold import TSNE
from umap import UMAP
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
# Data
from sklearn.datasets import load_digits, load_wine
from ucimlrepo import fetch_ucirepo
# plotly
import plotly.express as px
import matplotlib.pyplot as plt

# constants
DATA='wine quality' # 'digits', 'wine', 'wine quality'
WINE_QUALITY_RESPONSE='quality' # 'color' or 'quality' for 'wine quality'
STANDARDIZE=True
K=3 # new data dimension
SHOW_DIGITS=False # for 'digits'
METHOD='umap' #'lda' #'umap' // 'tsne'

# read data; returns X, y (dataframes) and labels (list with the length of y)
if DATA=='digits':
    digits = load_digits(as_frame=True)
    if SHOW_DIGITS:
        fig, ax = plt.subplots(1, 4)
        for i in range(4):
            ax[i].imshow(digits.images[i], cmap='Greys')
        # plt.savefig('figures/05_12.png', dpi=300)
        plt.show()
    y = digits.target # dataframe
    X = digits.data # dataframe
    labels=['digit_'+str(i) for i in y]

if DATA=='wine':
    X, y = load_wine(return_X_y=True, as_frame=True)
    labels=['region'+str(i) for i in y]

if DATA=='wine quality':
    # URL of the white wine dataset
    URL = 'http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv'
    # load the dataset from the URL
    white_df = pd.read_csv(URL, sep=";")
    # fill the 'color' column
    white_df["color"] = 'white'
    # keep only the first of duplicate items
    white_df = white_df.drop_duplicates(keep='first')
    # URL of the red wine dataset
    URL = 'http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'
    # load the dataset from the URL
    red_df = pd.read_csv(URL, sep=";")
    # fill the `color` column
    red_df["color"] = 'red'
    # keep only the first of duplicate items
    red_df = red_df.drop_duplicates(keep='first')
    # concatenate to obtain full data set
    df = pd.concat([red_df, white_df], ignore_index=True)
    # define X and y (dataframe)
    X = df.drop(columns=['quality','color'])
    if WINE_QUALITY_RESPONSE=='quality':
        y=df[WINE_QUALITY_RESPONSE] # pandas.core.series.Series
        labels=['quality_'+str(i) for i in y]
    if WINE_QUALITY_RESPONSE=='color':
        y = pd.get_dummies(df[WINE_QUALITY_RESPONSE])['red'].replace({True: 1, False: 0}) # pandas.core.series.Series
        labels=['color_'+str(i) for i in y]

# standardize data
if STANDARDIZE:
    stdsc = StandardScaler().set_output(transform="pandas")
    X = stdsc.fit_transform(X)

# dimensionality reduction
if METHOD=='tsne':
    #In t-SNE, the perplexity may be viewed as a knob that sets the number of effective nearest neighbors. Typically, between 5 and 50. Robust
    tsne = TSNE(n_components=K)#, metric='mahalanobis')
    X_ = tsne.fit_transform(X) # dataframe with K columns
if METHOD=='umap':
    umap=UMAP(n_components=K,n_neighbors=3,min_dist=0.1)
    print(y,type(y))
    X_= umap.fit_transform(X,y) # or just umap.fit_transform(X)
if METHOD=='lda':
    K=min(len(np.unique(labels))-1,K); print('K',K)
    lda = LinearDiscriminantAnalysis(n_components=K)
    X_ = lda.fit_transform(X, y)

# prepare dataframe for plot with plotly.express
proj_names=['proj'+str(i) for i in range(X_.shape[1])]
df=pd.DataFrame(X_,columns=proj_names) # projections of X
df['label']=labels # label

# plots 2D or 3D
def plot_projection_2D(df):
    fig = px.scatter(df, x=proj_names[0], y=proj_names[1],color='label', title='dimension reduction with '+METHOD)
    fig.show()

def plot_projection_3D(df):
    fig = px.scatter_3d(df, x=proj_names[0], y=proj_names[1], z=proj_names[2],color='label', title='dimension reduction with '+METHOD)
    fig.show()

if K==2:
    plot_projection_2D(df)
if K==3:
    plot_projection_3D(df)
```

    Requirement already satisfied: umap-learn[plot] in /usr/local/lib/python3.10/dist-packages (0.5.6)
    Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from umap-learn[plot]) (1.25.2)
    Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from umap-learn[plot]) (1.11.4)
    Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn[plot]) (1.2.2)
    Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn[plot]) (0.58.1)
    Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from umap-learn[plot]) (0.5.12)
    Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn[plot]) (4.66.4)
    Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from umap-learn[plot]) (2.0.3)
    Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from umap-learn[plot]) (3.7.1)
    Requirement already satisfied: datashader in /usr/local/lib/python3.10/dist-packages (from umap-learn[plot]) (0.16.1)
    Requirement already satisfied: bokeh in /usr/local/lib/python3.10/dist-packages (from umap-learn[plot]) (3.3.4)
    Requirement already satisfied: holoviews in /usr/local/lib/python3.10/dist-packages (from umap-learn[plot]) (1.17.1)
    Requirement already satisfied: colorcet in /usr/local/lib/python3.10/dist-packages (from umap-learn[plot]) (3.1.0)
    Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from umap-learn[plot]) (0.13.1)
    Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from umap-learn[plot]) (0.19.3)
    Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn[plot]) (0.41.1)
    Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent>=0.5->umap-learn[plot]) (1.4.2)
    Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn[plot]) (3.5.0)
    Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.10/dist-packages (from bokeh->umap-learn[plot]) (3.1.4)
    Requirement already satisfied: contourpy>=1 in /usr/local/lib/python3.10/dist-packages (from bokeh->umap-learn[plot]) (1.2.1)
    Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.10/dist-packages (from bokeh->umap-learn[plot]) (24.0)
    Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from bokeh->umap-learn[plot]) (9.4.0)
    Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.10/dist-packages (from bokeh->umap-learn[plot]) (6.0.1)
    Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.10/dist-packages (from bokeh->umap-learn[plot]) (6.3.3)
    Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.10/dist-packages (from bokeh->umap-learn[plot]) (2024.4.0)
    Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->umap-learn[plot]) (2.8.2)
    Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->umap-learn[plot]) (2023.4)
    Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->umap-learn[plot]) (2024.1)
    Requirement already satisfied: dask in /usr/local/lib/python3.10/dist-packages (from datashader->umap-learn[plot]) (2023.8.1)
    Requirement already satisfied: multipledispatch in /usr/local/lib/python3.10/dist-packages (from datashader->umap-learn[plot]) (1.0.0)
    Requirement already satisfied: param in /usr/local/lib/python3.10/dist-packages (from datashader->umap-learn[plot]) (2.1.0)
    Requirement already satisfied: pyct in /usr/local/lib/python3.10/dist-packages (from datashader->umap-learn[plot]) (0.5.0)
    Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from datashader->umap-learn[plot]) (2.31.0)
    Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from datashader->umap-learn[plot]) (0.12.1)
    Requirement already satisfied: xarray in /usr/local/lib/python3.10/dist-packages (from datashader->umap-learn[plot]) (2023.7.0)
    Requirement already satisfied: pyviz-comms>=0.7.4 in /usr/local/lib/python3.10/dist-packages (from holoviews->umap-learn[plot]) (3.0.2)
    Requirement already satisfied: panel>=0.13.1 in /usr/local/lib/python3.10/dist-packages (from holoviews->umap-learn[plot]) (1.3.8)
    Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->umap-learn[plot]) (0.12.1)
    Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->umap-learn[plot]) (4.51.0)
    Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->umap-learn[plot]) (1.4.5)
    Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->umap-learn[plot]) (3.1.2)
    Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->umap-learn[plot]) (3.3)
    Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->umap-learn[plot]) (2.31.6)
    Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->umap-learn[plot]) (2024.5.10)
    Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->umap-learn[plot]) (1.6.0)
    Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.9->bokeh->umap-learn[plot]) (2.1.5)
    Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from panel>=0.13.1->holoviews->umap-learn[plot]) (3.6)
    Requirement already satisfied: markdown-it-py in /usr/local/lib/python3.10/dist-packages (from panel>=0.13.1->holoviews->umap-learn[plot]) (3.0.0)
    Requirement already satisfied: linkify-it-py in /usr/local/lib/python3.10/dist-packages (from panel>=0.13.1->holoviews->umap-learn[plot]) (2.0.3)
    Requirement already satisfied: mdit-py-plugins in /usr/local/lib/python3.10/dist-packages (from panel>=0.13.1->holoviews->umap-learn[plot]) (0.4.0)
    Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from panel>=0.13.1->holoviews->umap-learn[plot]) (6.1.0)
    Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from panel>=0.13.1->holoviews->umap-learn[plot]) (4.11.0)
    Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->umap-learn[plot]) (1.16.0)
    Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from dask->datashader->umap-learn[plot]) (8.1.7)
    Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from dask->datashader->umap-learn[plot]) (2.2.1)
    Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask->datashader->umap-learn[plot]) (2023.6.0)
    Requirement already satisfied: partd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from dask->datashader->umap-learn[plot]) (1.4.2)
    Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask->datashader->umap-learn[plot]) (7.1.0)
    Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->datashader->umap-learn[plot]) (3.3.2)
    Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->datashader->umap-learn[plot]) (3.7)
    Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->datashader->umap-learn[plot]) (2.0.7)
    Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->datashader->umap-learn[plot]) (2024.2.2)
    Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask->datashader->umap-learn[plot]) (3.18.1)
    Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=1.2.0->dask->datashader->umap-learn[plot]) (1.0.0)
    Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->panel>=0.13.1->holoviews->umap-learn[plot]) (0.5.1)
    Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.10/dist-packages (from linkify-it-py->panel>=0.13.1->holoviews->umap-learn[plot]) (1.0.3)
    Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py->panel>=0.13.1->holoviews->umap-learn[plot]) (0.1.2)
    Requirement already satisfied: ucimlrepo in /usr/local/lib/python3.10/dist-packages (0.0.6)
    0       5
    1       5
    2       5
    3       6
    4       5
           ..
    5315    6
    5316    5
    5317    6
    5318    7
    5319    6
    Name: quality, Length: 5320, dtype: int64 <class 'pandas.core.series.Series'>
    


<html>
<head><meta charset="utf-8" /></head>
<body>
    <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-2.24.1.min.js"></script>                <div id="ea9f248f-387b-434d-9c81-0c0db6f6d171" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("ea9f248f-387b-434d-9c81-0c0db6f6d171")) {                    Plotly.newPlot(                        "ea9f248f-387b-434d-9c81-0c0db6f6d171",                        [{"hovertemplate":"label=quality_5\u003cbr\u003eproj0=%{x}\u003cbr\u003eproj1=%{y}\u003cbr\u003eproj2=%{z}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"quality_5","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"quality_5","scene":"scene","showlegend":true,"x":[-5.068680286407471,-1.3627851009368896,-1.3945211172103882,-5.069521903991699,-1.5609818696975708,-0.837212860584259,-5.800090312957764,3.1142375469207764,3.3345766067504883,5.899317264556885,5.8993659019470215,9.504266738891602,2.365151882171631,-1.2619634866714478,-6.773080825805664,-6.093667984008789,0.3444761335849762,-5.0741705894470215,-5.517179489135742,-0.8085307478904724,-6.127904415130615,-0.8372553586959839,3.3321762084960938,1.8272199630737305,10.969826698303223,-4.059498310089111,-6.098127841949463,-6.119781494140625,6.286306858062744,-6.024189472198486,-1.5970370769500732,4.588147163391113,5.913158893585205,2.719931125640869,11.143325805664062,10.973105430603027,-5.184607028961182,-1.2078858613967896,2.75212025642395,2.533730983734131,5.37583065032959,5.738697052001953,5.739455699920654,5.377133369445801,-3.2339885234832764,-5.438864707946777,-6.105016231536865,-4.704259872436523,-7.207769870758057,9.511659622192383,2.665517807006836,2.7527403831481934,3.5938684940338135,-5.050126075744629,5.866237163543701,11.48866081237793,1.8452428579330444,0.3445725440979004,-1.3200981616973877,-3.9890475273132324,1.0855096578598022,-4.704270362854004,2.9908299446105957,10.997224807739258,-1.289939522743225,-0.8084014058113098,-0.8052349090576172,-1.418033480644226,-5.061418056488037,-1.1233537197113037,5.736344814300537,6.090660572052002,-0.9647505879402161,-0.9653109312057495,-6.106790542602539,11.025246620178223,6.2192864418029785,-1.3667571544647217,-1.3232587575912476,11.143754005432129,5.7350616455078125,5.726687431335449,-6.1032185554504395,11.027849197387695,3.142057418823242,10.175167083740234,-5.484586715698242,-0.838604211807251,-0.8374519348144531,-5.063888072967529,-1.3483463525772095,5.854125499725342,5.852754592895508,10.941950798034668,-5.4864501953125,9.502104759216309,1.0854551792144775,-6.1616363525390625,-5.085931777954102,-6.747890472412109,10.175309181213379,-5.080831050872803,0.7615678906440735,-4.12449312210083,-7.207630157470703,-1.5400594472885132,-5.991327285766602,-5.978940963745117,-6.00133752822876,-5.700817584991455,0.35693106055259705,5.810641288757324,2.6548960208892822,3.5941596031188965,-6.162158489227295,2.555988073348999,5.857950687408447,-7.088569641113281,5.7007856369018555,-0.837497889995575,-2.428529977798462,5.333654880523682,2.8065712451934814,5.790960311889648,-6.723135471343994,1.8140088319778442,-1.727665662765503,2.625593662261963,10.176506042480469,-1.6349209547042847,-6.760340213775635,3.1194963455200195,10.95202922821045,-6.483748912811279,-8.104134559631348,3.0291175842285156,-4.70428991317749,3.9877138137817383,3.5521204471588135,3.9816651344299316,3.9919793605804443,4.832330703735352,5.330496311187744,5.901947498321533,2.7750959396362305,6.069388389587402,4.67620325088501,-3.188171863555908,4.576114654541016,6.520110130310059,-5.86982536315918,-5.869853496551514,-5.869844436645508,-5.049832820892334,-1.5284292697906494,10.947113990783691,-3.9879631996154785,-6.023593902587891,2.2938613891601562,6.0472612380981445,5.998707294464111,-1.4879415035247803,4.4846673011779785,4.619431495666504,-2.4286136627197266,5.852200508117676,0.5766246318817139,-1.4388245344161987,8.57777214050293,5.228847980499268,-6.628456115722656,3.8238394260406494,4.190319538116455,4.476625442504883,-8.10799789428711,0.576578676700592,-6.7284464836120605,-9.707853317260742,-3.907388210296631,10.946449279785156,3.806486129760742,5.902153968811035,6.387417793273926,-1.4778515100479126,11.142461776733398,-6.523638725280762,-6.051563739776611,3.5941321849823,-5.488168239593506,0.5766305923461914,-1.2446480989456177,-3.9600582122802734,5.807953357696533,4.252627849578857,-6.968497276306152,4.455654144287109,2.0495190620422363,-3.5845301151275635,5.761995792388916,5.992338180541992,5.170882701873779,3.847367763519287,10.89978313446045,5.111956596374512,-1.3549938201904297,3.859423875808716,4.41160774230957,6.3818888664245605,-8.105781555175781,4.720241546630859,7.788331031799316,7.788544178009033,6.482857704162598,10.148670196533203,-3.660061836242676,3.5941765308380127,10.148784637451172,11.143745422363281,3.593602180480957,-3.3166611194610596,-3.3670802116394043,6.0712738037109375,6.377777099609375,-3.691174030303955,3.8675386905670166,5.982751846313477,6.364912986755371,-3.31050968170166,-1.7226611375808716,3.952738046646118,3.9527289867401123,3.879373788833618,-3.370893955230713,-3.302234649658203,3.8219103813171387,-3.3726706504821777,-3.3707242012023926,4.252612590789795,4.252599239349365,-6.540745258331299,-3.368269205093384,4.517443656921387,-1.5468089580535889,-6.090177536010742,-1.3590126037597656,-4.172779560089111,-3.260035276412964,4.359216690063477,4.543742656707764,-3.698704242706299,4.692323684692383,-6.509541988372803,-6.460679531097412,6.4123687744140625,-4.923564434051514,-4.849708557128906,-6.557330131530762,-8.106551170349121,5.90293025970459,-1.3437284231185913,6.748154163360596,6.737362861633301,4.538280963897705,4.541837692260742,-4.912800312042236,4.547921180725098,-1.5907032489776611,8.579082489013672,-3.9019052982330322,4.797430515289307,9.552363395690918,-3.691502332687378,4.824509620666504,6.478854179382324,5.869088649749756,6.39886474609375,6.675060272216797,4.881433010101318,-1.6421982049942017,-5.668030261993408,-6.788026332855225,-3.2601535320281982,5.843923091888428,6.361539840698242,6.724298000335693,0.7496767640113831,5.900773525238037,1.8981709480285645,4.609585762023926,-1.6669021844863892,-4.7042670249938965,-6.077210426330566,-6.052104949951172,5.943809509277344,-1.6781888008117676,-1.2204114198684692,2.612624406814575,6.576989650726318,-6.149205207824707,-1.0188497304916382,-4.192485809326172,6.348647594451904,-6.818398952484131,-6.920133590698242,-5.079279899597168,-6.973492622375488,-6.841777801513672,11.48898983001709,5.129805564880371,1.7015502452850342,-3.1881768703460693,1.751929521560669,-1.2635031938552856,11.143744468688965,0.611353874206543,-1.3842133283615112,-6.993801593780518,-4.982529163360596,-6.70004940032959,1.7015149593353271,-3.793332576751709,-3.801771402359009,-4.1887054443359375,-1.577211856842041,2.751871347427368,-1.3414406776428223,-7.007786750793457,-6.536839962005615,-4.850118160247803,-6.739382266998291,-6.279018878936768,-1.151256799697876,-6.700560569763184,-6.72055196762085,-1.2917019128799438,5.763355731964111,2.9266252517700195,2.860400676727295,-5.668038368225098,2.908958911895752,-3.868720531463623,-6.7748494148254395,-6.686967849731445,2.152648448944092,0.48995107412338257,4.8836212158203125,-5.7123894691467285,-6.9159626960754395,4.623668193817139,2.4948058128356934,4.983948707580566,-1.598369836807251,5.9153242111206055,-4.901136875152588,2.9602978229522705,-4.301090240478516,2.5972297191619873,-4.849888801574707,-1.6210649013519287,2.621889114379883,1.8457106351852417,10.951247215270996,2.993673801422119,2.994236469268799,-3.9803359508514404,3.5941200256347656,2.9453442096710205,-4.302253723144531,2.9638514518737793,2.9488980770111084,-6.050137519836426,2.9343128204345703,6.424612045288086,-1.7692970037460327,5.974773406982422,5.964291095733643,2.9406723976135254,1.704404354095459,-6.568414688110352,8.100110054016113,-1.5590025186538696,5.6569671630859375,-3.31805157661438,-5.869846343994141,2.9856033325195312,1.733952522277832,-3.233905553817749,2.9936983585357666,0.34900185465812683,-4.901683330535889,-6.851071834564209,2.6567623615264893,-8.02458381652832,-1.6465009450912476,-6.9008307456970215,4.283096790313721,5.370392322540283,-1.646195411682129,-6.861566066741943,-1.6463357210159302,-3.8429059982299805,5.685449123382568,5.040905952453613,-1.3298264741897583,2.7542004585266113,2.4880638122558594,1.494604229927063,-1.4706848859786987,-1.6476551294326782,-5.065443992614746,2.2369372844696045,-4.7042951583862305,1.8554091453552246,-7.0750837326049805,5.847898960113525,-5.8852057456970215,-4.038590431213379,12.72164249420166,-3.9019527435302734,-3.962583303451538,2.024744749069214,-1.5500930547714233,1.8663352727890015,-8.308834075927734,-0.8958069682121277,-7.047890663146973,5.952272891998291,-6.731107234954834,4.854775905609131,5.348790645599365,9.502800941467285,0.4899532198905945,-1.7450069189071655,-4.182675361633301,-5.848232269287109,8.099909782409668,1.590243935585022,-1.5589185953140259,-1.567708969116211,5.984009742736816,5.048060894012451,3.928945779800415,-7.005137920379639,12.722380638122559,6.154423713684082,9.552383422851562,-5.667680740356445,12.722821235656738,-8.308871269226074,-6.365111351013184,2.6653079986572266,-7.034238815307617,-5.885428428649902,-4.908947944641113,-1.170541763305664,2.379220485687256,1.7969424724578857,12.722896575927734,9.516633987426758,-8.309070587158203,-3.5611929893493652,12.725223541259766,2.5621423721313477,-3.673348903656006,1.776363730430603,3.5941925048828125,2.2713770866394043,12.720866203308105,2.4943578243255615,-6.41898775100708,5.7755351066589355,6.197308540344238,-5.884426116943359,-1.2896283864974976,6.092323303222656,-4.223273754119873,3.9090051651000977,1.0692298412322998,6.346804618835449,-1.5277467966079712,3.9054555892944336,3.157248020172119,1.733471155166626,1.9806241989135742,2.446892023086548,5.823761463165283,-1.630399465560913,1.9546782970428467,-6.063509464263916,-6.063825607299805,-10.082494735717773,5.658149242401123,-1.6798756122589111,-6.643165111541748,-7.047603130340576,9.528279304504395,1.0691360235214233,5.710988521575928,-1.3214675188064575,-1.2489814758300781,5.751826286315918,-6.430673122406006,1.0691909790039062,-4.885336399078369,8.100135803222656,12.72096061706543,6.3618927001953125,-6.9717793464660645,5.6919379234313965,-5.931302070617676,5.028592109680176,5.740373134613037,5.842070579528809,0.4899374544620514,4.4076972007751465,-3.0886409282684326,-6.929240703582764,5.853190898895264,3.9620203971862793,2.464478015899658,2.4955339431762695,2.452845335006714,2.5508127212524414,-8.108532905578613,-1.241058111190796,1.984609842300415,-6.162111282348633,1.9366449117660522,-6.160907745361328,-6.1792426109313965,-7.208164215087891,-8.024231910705566,-6.181022644042969,6.401523590087891,1.7328286170959473,1.7716431617736816,10.999822616577148,-3.2339515686035156,-8.024651527404785,-7.0583696365356445,-7.058567047119141,-3.085918426513672,1.7440046072006226,1.7492388486862183,5.828071117401123,-7.033653259277344,-6.414493560791016,2.697746515274048,2.468493700027466,-6.158394813537598,2.5768256187438965,2.509618043899536,1.843248724937439,5.8690409660339355,3.925463914871216,3.0191526412963867,-7.208286285400391,1.8405019044876099,2.471674680709839,2.7306160926818848,1.757085919380188,-3.6810455322265625,-5.078533172607422,-5.079362869262695,-5.0783185958862305,-5.076931953430176,-10.027917861938477,5.848301410675049,-7.0640106201171875,-5.889408588409424,5.837416648864746,-3.5610954761505127,1.770553708076477,5.870661735534668,-5.909931182861328,-6.050091743469238,2.178943634033203,2.721006155014038,0.8565592765808105,-1.289297103881836,6.210090160369873,-0.455221563577652,3.0236880779266357,0.4080878496170044,-1.2924158573150635,2.488856792449951,-0.008339726366102695,-3.2634389400482178,4.98118257522583,-0.8582793474197388,2.4758450984954834,-3.4937050342559814,5.574585437774658,-1.7115249633789062,7.670413017272949,7.554393291473389,3.0767111778259277,-1.9660464525222778,-0.9833515286445618,1.1442803144454956,4.867862224578857,0.9110764265060425,8.373855590820312,6.148534297943115,8.89441967010498,6.204494476318359,-0.9125785827636719,5.622685432434082,-0.8961694240570068,-0.4508761465549469,6.738283157348633,0.5752137303352356,-0.005620947107672691,5.734896183013916,-1.8183135986328125,7.342157363891602,-1.7584421634674072,7.343984127044678,5.403252601623535,5.566062927246094,2.2973194122314453,5.941901206970215,5.5817036628723145,6.860866069793701,6.860984802246094,6.565402507781982,10.838740348815918,6.867828845977783,8.395570755004883,-3.4089951515197754,-3.4233956336975098,1.7780426740646362,1.778084397315979,2.4302659034729004,5.027536392211914,14.578968048095703,14.578991889953613,-6.919395446777344,7.468429088592529,7.410540580749512,4.839751720428467,0.17046980559825897,0.5347303748130798,8.099724769592285,5.754473686218262,8.3742036819458,5.151814937591553,12.618587493896484,5.151664733886719,-0.009021664038300514,7.434994220733643,-0.22950422763824463,1.289698839187622,2.48704195022583,6.053493022918701,0.08577475696802139,6.971586227416992,6.9715399742126465,8.393623352050781,3.0669522285461426,1.7745487689971924,-1.3738244771957397,7.412075519561768,7.4214606285095215,5.504002094268799,6.170762538909912,10.32492446899414,3.0633857250213623,2.0532796382904053,5.988288402557373,-1.9410769939422607,5.835526466369629,2.353541135787964,-4.483887672424316,-0.032018549740314484,5.740271091461182,6.539161682128906,-3.06888484954834,-3.922680139541626,2.270801067352295,0.572577714920044,5.834197998046875,-6.816871166229248,-1.5976494550704956,-0.9795770645141602,-3.4930479526519775,-3.1995880603790283,12.618735313415527,2.4750161170959473,4.782474040985107,-0.6608821749687195,2.321969747543335,-6.910400390625,-0.8180708289146423,6.024409294128418,-0.8584514260292053,2.2152509689331055,-6.294915199279785,2.4749128818511963,6.05159330368042,2.80940842628479,1.9655431509017944,7.55429220199585,-1.8121734857559204,7.344106674194336,8.360889434814453,-0.9698832035064697,-3.230455160140991,6.2685322761535645,13.329546928405762,1.7785435914993286,5.60945463180542,8.389254570007324,-9.974305152893066,5.215810775756836,-6.295010566711426,3.0084846019744873,0.10501830279827118,0.7160136699676514,3.031998872756958,4.355217933654785,13.334843635559082,6.220927715301514,4.891973972320557,-0.9592928290367126,10.308460235595703,8.398810386657715,2.404223918914795,3.063688278198242,12.074239730834961,-0.9522006511688232,6.220043182373047,10.693900108337402,5.94549036026001,5.644021987915039,-0.8387018442153931,-0.8392479419708252,5.418242454528809,12.618815422058105,-5.206598281860352,-5.205834865570068,6.2339277267456055,-6.83729362487793,1.3631532192230225,2.4672176837921143,-0.009088573977351189,8.34578800201416,6.715870380401611,2.053330183029175,2.053288221359253,5.543570518493652,5.556030750274658,-1.6534373760223389,-0.6735981702804565,2.965508222579956,-0.5715920925140381,2.676830291748047,1.2251203060150146,1.702315330505371,1.7455848455429077,-3.4985976219177246,1.6699784994125366,-9.923744201660156,-4.2625579833984375,7.454578876495361,-0.6664990186691284,-0.673380970954895,7.430218696594238,-1.5630460977554321,-1.562963604927063,10.31999397277832,1.4478031396865845,-1.4700311422348022,-0.628445029258728,8.339032173156738,-1.9875743389129639,6.065041542053223,7.894063949584961,6.425627708435059,-2.47367787361145,7.554286479949951,0.19419829547405243,-9.567852973937988,5.643833637237549,8.381131172180176,3.4365131855010986,-9.233659744262695,8.258867263793945,6.220416069030762,-0.9480018019676208,12.618922233581543,7.457476615905762,-0.6441633105278015,-1.2978142499923706,5.531322956085205,1.069831371307373,4.718340873718262,1.3230624198913574,-0.9631857872009277,-3.3627119064331055,0.1282772570848465,0.5092377066612244,4.864829063415527,-0.9764195680618286,6.212724208831787,1.2833034992218018,6.209585666656494,-9.998862266540527,0.39914292097091675,7.676562786102295,3.8271496295928955,4.487310886383057,5.029468059539795,4.817544460296631,-5.898100852966309,-5.891580104827881,-5.897976875305176,1.630543828010559,1.630547285079956,2.520233392715454,1.1648751497268677,1.3079264163970947,-0.7890035510063171,5.029449462890625,-1.9821211099624634,5.029429912567139,1.241294503211975,-1.6044470071792603,-1.47981595993042,-5.159889221191406,-0.8166254758834839,-3.369358539581299,2.8319380283355713,-0.569645881652832,2.8183090686798096,-3.296996593475342,7.217740535736084,0.12847445905208588,1.6173242330551147,6.481987953186035,-6.650766372680664,5.185514450073242,1.4384690523147583,1.164804458618164,1.6603437662124634,4.273016452789307,2.9720771312713623,-6.259815216064453,1.3236526250839233,0.3459051251411438,-1.5642178058624268,-1.3794963359832764,6.189103126525879,1.207839846611023,2.428684949874878,-1.8815462589263916,-2.7488818168640137,6.528115272521973,-1.7336429357528687,-0.8449171781539917,-1.7013134956359863,6.289290904998779,-1.7055063247680664,-1.7252197265625,4.680064678192139,-5.783472537994385,4.50501012802124,12.619190216064453,-5.126540660858154,0.5816596150398254,6.4840545654296875,-0.26024049520492554,-2.088667631149292,-4.424903392791748,8.048172950744629,-4.4248833656311035,0.30434104800224304,-1.761570692062378,-1.7591546773910522,6.029255390167236,6.918907642364502,-0.8524089455604553,0.29675114154815674,-3.1565093994140625,1.456350326538086,-3.139138698577881,-2.1118617057800293,-2.107191324234009,-5.861930847167969,-1.766437292098999,0.4962403476238251,0.5814765095710754,-5.858682632446289,2.0356311798095703,-0.5760153532028198,2.9888181686401367,6.207928657531738,4.728484153747559,7.894128799438477,-3.030017375946045,0.43522974848747253,-0.9638455510139465,-1.2984418869018555,-2.724445343017578,11.590790748596191,0.4974762201309204,0.5748947858810425,-0.9273166060447693,11.03062629699707,0.08274812996387482,-4.263007164001465,5.50726842880249,-3.1407792568206787,2.503432512283325,6.216822147369385,-2.113664388656616,1.1192874908447266,3.841916799545288,5.571280479431152,4.252789497375488,-10.087173461914062,-5.146147727966309,-4.279931545257568,-4.819305419921875,5.571366786956787,-8.346305847167969,-1.765323519706726,0.4790516495704651,4.965488910675049,1.2661983966827393,4.2931437492370605,-0.8477512001991272,-0.8540709018707275,6.439337730407715,2.9726548194885254,0.3149462938308716,5.883579254150391,7.217767238616943,11.01148796081543,4.823215961456299,1.4651178121566772,1.6305217742919922,1.6699812412261963,6.04255485534668,-5.599215507507324,5.593864917755127,7.053149223327637,5.571407318115234,1.0844957828521729,1.0223928689956665,-5.164724349975586,2.9122183322906494,0.6930826306343079,-8.686574935913086,6.56890869140625,-6.243277549743652,7.979743957519531,2.6063413619995117,-1.6685079336166382,6.2161383628845215,-5.162148475646973,-2.170915126800537,5.841572284698486,1.7639564275741577,0.05423786863684654,2.7107937335968018,-1.3607068061828613,-1.358136534690857,8.330514907836914,-5.179257869720459,0.2643890678882599,0.6543399095535278,4.457034587860107,-2.0419015884399414,7.759897708892822,7.760883808135986,0.6918548941612244,-4.357132911682129,4.6797943115234375,-5.173329830169678,-3.102285146713257,-0.7827578186988831,-2.124054431915283,2.2838332653045654,6.202101707458496,0.9070205092430115,0.6782776713371277,-1.3032569885253906,6.2133989334106445,11.026864051818848,0.8252307176589966,-3.5809619426727295,7.575237274169922,-5.599103927612305,5.596071243286133,1.2536417245864868,1.2310693264007568,4.252608776092529,-10.128560066223145,-10.12228012084961,-2.5948195457458496,2.5211338996887207,8.877812385559082,8.101920127868652,-1.9308053255081177,3.4586284160614014,7.418957233428955,7.403192520141602,12.619769096374512,8.161870002746582,-1.1526936292648315,5.378500461578369,-9.221768379211426,-0.781554102897644,-1.204673171043396,-2.1012978553771973,1.1185063123703003,7.459103107452393,0.10449407249689102,3.421476125717163,-1.894469141960144,-6.901854991912842,-1.858307123184204,-5.874494552612305,-6.9051289558410645,-0.5270993709564209,-3.579033851623535,1.3156490325927734,-0.8630609512329102,-1.2450602054595947,-3.0021615028381348,1.2234636545181274,8.877853393554688,1.2238526344299316,-0.2375582456588745,-0.25073161721229553,5.543618679046631,2.8313658237457275,-2.6607682704925537,0.2893926203250885,-3.5880091190338135,11.034790992736816,7.212897300720215,0.40468090772628784,1.2471113204956055,5.571403503417969,-3.6009409427642822,0.4942871332168579,0.49387258291244507,-1.9380154609680176,6.974867820739746,5.151695251464844,5.719476222991943,-0.8829343318939209,2.0357558727264404,2.2387192249298096,8.111837387084961,12.619112014770508,3.0345797538757324,-0.8806139230728149,-0.006021042820066214,7.477608680725098,-0.12687243521213531,-1.2235406637191772,-1.3223915100097656,0.5255205631256104,7.894233703613281,0.5813729763031006,-1.2764171361923218,0.14234879612922668,-7.0385050773620605,7.659761428833008,3.642824411392212,5.53257942199707,5.571510314941406,-6.309790134429932,1.972619891166687,1.9726157188415527,7.485586643218994,4.775896072387695,0.39672568440437317,3.8014657497406006,-1.8565146923065186,-8.346329689025879,-3.610424757003784,0.07601916790008545,-9.216224670410156,7.180636405944824,7.812143325805664,-7.27447509765625,0.24241845309734344,-3.564459800720215,-3.5527493953704834,5.802393913269043,7.433495998382568,-2.226242780685425,11.010443687438965,1.3748027086257935,1.5426095724105835,0.601771354675293,0.14080195128917694,8.12527084350586,7.059621810913086,-1.8215177059173584,-2.9128942489624023,-0.8597952127456665,5.9178385734558105,7.4602837562561035,7.473603248596191,-0.005604092497378588,-2.2426397800445557,-1.416455626487732,2.9477884769439697,-6.308299541473389,2.8970112800598145,5.687872886657715,2.9054362773895264,0.09546313434839249,0.08822941035032272,-0.4988495707511902,-0.3959740400314331,-3.6001384258270264,-0.002454533241689205,10.837986946105957,0.24688798189163208,0.5310202240943909,0.565186619758606,1.164727807044983,5.940914154052734,5.930359363555908,-1.3670082092285156,5.549278736114502,4.890864849090576,7.463350772857666,-0.8427813649177551,5.571438789367676,-0.0030865801963955164,6.725302696228027,4.471602916717529,-3.215287923812866,8.127202987670898,5.499300956726074,5.66646146774292,-1.390958547592163,-6.526533603668213,6.344271183013916,6.403481960296631,0.41135188937187195,7.658097743988037,4.680418968200684,4.856979846954346,4.718963623046875,3.4593284130096436,11.170079231262207,-1.3098467588424683,0.5280705094337463,6.215668678283691,5.515864372253418,-1.0982599258422852,1.6183242797851562,2.787306070327759,4.640479564666748,0.5304259061813354,5.517975330352783,-1.3515335321426392,-1.3447192907333374,8.048042297363281,1.0727249383926392,-3.598888397216797,1.244014859199524,1.2411340475082397,-1.8853468894958496,-6.76161527633667,0.5129497647285461,0.5256487131118774,-6.836909770965576,8.105361938476562,7.458381175994873,-1.6448005437850952,0.5232371687889099,-1.555249571800232,-5.8854875564575195,7.407100677490234,-1.5359145402908325,0.30617406964302063,1.3990809917449951,4.573971271514893,-10.109807968139648,-10.087687492370605,5.186451435089111,-1.678188443183899,0.3204893171787262,-5.874966144561768,5.930179595947266,-3.007873773574829,1.257358193397522,3.1353890895843506,-1.740807294845581,7.589271545410156,-1.3145174980163574,-3.1093857288360596,8.069391250610352,-3.706737995147705,5.762632846832275,8.08372688293457,4.821658611297607,10.998461723327637,13.640564918518066,-3.2481930255889893,4.57021951675415,-9.280306816101074,-3.608879804611206,8.185538291931152,5.068408966064453,-1.301835536956787,-1.4110982418060303,1.2871944904327393,6.5822014808654785,-5.162050724029541,4.860476970672607,6.201379299163818,4.791287422180176,-8.346273422241211,0.6970142126083374,2.406186580657959,-1.0666862726211548,2.4062371253967285,2.192187547683716,12.618657112121582,-6.302340507507324,10.022589683532715,0.6973410248756409,-1.6656914949417114,0.6878644824028015,5.6385908126831055,-10.101937294006348,4.475625514984131,13.332364082336426,13.331872940063477,13.332477569580078,-0.04743783548474312,7.312389850616455,6.985565662384033,-2.7489490509033203,-2.7485921382904053,-0.8509137630462646,-5.152205944061279,-5.171256065368652,2.3032760620117188,-1.2982193231582642,-5.712751865386963,-1.408803105354309,0.653610348701477,-3.2152435779571533,-1.683264970779419,1.5921847820281982,-0.09547323733568192,-0.0071958997286856174,2.9479594230651855,-3.6223366260528564,-0.6357954740524292,0.806645393371582,6.850773334503174,-3.3078510761260986,0.3078405261039734,-2.218144416809082,6.724417209625244,-7.037749767303467,-7.037664890289307,5.548238277435303,-0.5042060613632202,6.265556335449219,7.643880844116211,-5.158041000366211,-8.686572074890137,5.540950775146484,7.483713626861572,5.98876428604126,3.8611438274383545,2.94781231880188,0.1798332929611206,-6.816833972930908,-2.5943756103515625,6.862842559814453,4.506045818328857,-2.2182252407073975,4.252446174621582,-1.3687866926193237,7.327382564544678,-6.284418106079102,-2.5947835445404053,7.598803520202637,5.916568756103516,-0.0053924499079585075,-0.8943234086036682,13.109781265258789,2.2349183559417725,7.458211898803711,10.288006782531738,11.169211387634277,-6.761507511138916,3.3136723041534424,1.8452881574630737,7.413690090179443,3.070931911468506,7.453557014465332,-1.763521671295166,-0.2657308280467987,5.8981781005859375,0.1812322735786438,2.7627623081207275,0.7594574689865112,7.046102523803711,-4.07822847366333,-1.911655068397522,4.9907450675964355,0.1293080896139145,10.289833068847656,11.16943359375,0.5354685187339783,6.615891456604004,-0.638657808303833,5.7939839363098145,-1.6380033493041992,-1.7355225086212158,-2.912778854370117,-0.8537598848342896,7.065537452697754,-1.153591275215149,5.983666896820068,7.251465320587158,7.398780345916748,7.439319610595703,0.6072538495063782,0.19423159956932068,7.595477104187012,-5.15764045715332,-5.151556491851807,3.0467920303344727,5.047917366027832,4.367082118988037,1.2866476774215698,7.86837911605835,-0.14903417229652405,-1.569799542427063,-2.217794418334961,7.495370864868164,0.12157009541988373,3.2476720809936523,-3.564260959625244,10.97732162475586,0.424697607755661,7.554239273071289,-3.3162848949432373,-1.07860267162323,11.169291496276855,0.3626353144645691,-0.1834736317396164,5.999758243560791,-1.6020514965057373,5.631350040435791,-1.8239716291427612,6.197146892547607,7.945107460021973,-1.7943944931030273,8.396242141723633,-0.25236716866493225,-1.2925654649734497,0.51270991563797,-1.4149527549743652,0.10512997955083847,5.974127292633057,-7.188255310058594,5.945418357849121,5.948929786682129,-0.2230750322341919,0.1799601912498474,-5.854892253875732,4.647963523864746,-5.854561805725098,-0.07701654732227325,-8.831765174865723,4.843159198760986,-2.918609619140625,2.8263614177703857,13.332477569580078,3.0620460510253906,-5.948384761810303,3.2031021118164062,6.187314033508301,-2.2180187702178955,-3.316524028778076,-6.830874443054199,5.9987287521362305,3.1735925674438477,4.3134026527404785,0.7594432234764099,-3.13204026222229,-5.161465167999268,-1.054379940032959,4.703139781951904,-1.0860074758529663,-2.123509645462036,4.380329608917236,-0.5791323781013489,-1.3597301244735718,-0.6850112080574036,3.0342812538146973,-1.6378670930862427,-2.621520757675171,0.05683419480919838,-1.2859286069869995,2.234954595565796,3.2343404293060303,3.206408739089966,6.827342987060547,0.28645405173301697,6.008673667907715,-4.223165988922119,5.1589765548706055,5.413437366485596,4.939066410064697,11.169605255126953,-0.566584587097168,-5.472602367401123,5.881268501281738,-0.9476103186607361,0.73005211353302,0.10241178423166275,-4.2583136558532715,-2.3964550495147705,5.613869667053223,-1.0943915843963623,4.306039333343506,1.514553427696228,-1.670157790184021,1.4809116125106812,0.5561782717704773,4.451662540435791,-0.6550778150558472,0.18132154643535614,-5.472638130187988,0.46361517906188965,-0.8994514346122742,-0.9315492510795593,0.1601315140724182,0.17213869094848633,6.382723331451416,0.014353852719068527,0.021347247064113617,6.096181392669678,-1.8223564624786377,-1.192182183265686,0.4388578534126282,6.240363597869873,0.0437658466398716,3.032973527908325,6.231115818023682,-3.6101913452148438,-0.7981483340263367,-3.5589284896850586,0.13006387650966644,-7.181191444396973,1.9796708822250366,5.790411949157715,4.36708402633667,7.971778392791748,0.08481695502996445,0.11131367832422256,0.07173590362071991,1.5141576528549194,2.216350555419922,-0.6309584379196167,-4.180022239685059,11.74099349975586,7.5930070877075195,-6.24353551864624,1.2316267490386963,7.46320104598999,12.061553001403809,-4.177086353302002,-0.9471198916435242,1.2234278917312622,7.143989086151123,-1.2870863676071167,5.9754228591918945,12.903424263000488,7.0866875648498535,3.5809097290039062,-3.5624797344207764,-0.597644567489624,6.120868682861328,7.447092056274414,-2.9565722942352295,2.70706844329834,12.903432846069336,12.903464317321777,7.054970741271973,5.190338611602783,1.2351908683776855,-2.4679532051086426,8.392024040222168,6.033109188079834,11.741214752197266,11.741218566894531,-0.1824590265750885,-1.406253695487976,4.650556564331055,-0.7880985140800476,-1.8152244091033936,-2.087388038635254,2.714066743850708,4.85778284072876,2.7063474655151367,2.7419190406799316,2.73982572555542,0.6273218989372253,-8.85069751739502,-2.8766849040985107,0.9342187643051147,-1.5786430835723877,2.525574207305908,-2.7941839694976807,5.5043110847473145,5.662086486816406,-1.10758638381958,2.7370681762695312,4.80001163482666,-9.084677696228027,-6.244137287139893,-0.6223227977752686,7.316823482513428,-3.5897417068481445,0.5793712139129639,3.5809319019317627,2.745022773742676,-0.559794008731842,7.463201999664307,-6.8074846267700195,4.367007255554199,5.9857306480407715,2.2315456867218018,2.731001853942871,-0.601547122001648,-0.8416847586631775,-1.8906763792037964,-1.4117454290390015,5.7824506759643555,4.585710525512695,-10.011272430419922,5.649551868438721,2.0546586513519287,10.713448524475098,7.296707630157471,6.062666416168213,1.791238784790039,3.5809504985809326,-0.9021421670913696,-5.258745193481445,7.514645099639893,3.0450263023376465,-8.68657112121582,-0.8200024962425232,0.49616381525993347,7.515359878540039,-4.144191265106201,-1.0724656581878662,-8.686604499816895,7.089178562164307,-1.2827167510986328,-1.280388355255127,-3.290062427520752,-1.3519463539123535,3.639331817626953,-0.4027951955795288,-9.209091186523438,-2.6210179328918457,-5.899256229400635,-1.161624550819397,2.2815680503845215,3.500253915786743,11.740769386291504,0.30156606435775757,-4.613193035125732,-1.9377952814102173,12.074301719665527,5.376985549926758,-6.477594375610352,-1.095834493637085,-1.11695396900177,7.934571266174316,9.480118751525879,-1.9132208824157715,3.9341323375701904,6.607118129730225,1.9727014303207397,0.41500750184059143,0.3499124050140381,7.021682262420654,6.973996639251709,0.007653489243239164,1.3898025751113892,3.0703439712524414,-1.5740694999694824,-9.230047225952148,-3.296293258666992,4.814080715179443,-3.6594176292419434,11.7404203414917,4.674826145172119,7.088199138641357,7.054073333740234,-1.568385362625122,-6.477478504180908,-6.478003978729248,3.6313796043395996,-1.3192447423934937,-1.1471741199493408,4.585691928863525,4.590373992919922,0.24351061880588531,4.212378025054932,2.7484216690063477,-1.9162880182266235,-0.2315596491098404,-9.919502258300781,-8.909307479858398,-2.4559690952301025,-8.836569786071777,0.3179459571838379,-1.4764388799667358,0.08575902134180069,3.651177406311035,-1.4532301425933838,-6.820672035217285,-2.818486213684082,-3.1664652824401855,-1.4979608058929443,5.561483860015869,0.3238927125930786,-9.210503578186035,-1.8510785102844238,-3.306562900543213,-0.8409317135810852,-0.8299404382705688,-1.583053469657898,5.802972793579102,-1.911734700202942,6.74833869934082,-0.5849584341049194,-8.313206672668457,-1.6711840629577637,-1.6705347299575806,-8.313211441040039,7.013105869293213,-8.313234329223633,5.335424423217773,-3.659377336502075,-5.882936954498291,1.6236954927444458,-0.7599303126335144,7.002686500549316,4.500336647033691,-1.52744722366333,-3.281775712966919,7.251231670379639,10.611812591552734,-0.7933222651481628,-3.0817453861236572,-1.1703468561172485,-1.2932193279266357,-9.685829162597656,-1.4445477724075317,3.0762884616851807,-1.574768304824829,3.080310821533203,-9.899202346801758,-0.24585717916488647,-2.8807666301727295,-1.149245023727417,-2.6333425045013428,-5.854238033294678,7.463882923126221,-2.079328775405884,-3.2934391498565674,6.861160755157471,-4.994987487792969,0.1907479465007782,-0.1543729454278946,-0.18075230717658997,4.998753070831299,-8.836538314819336,4.247153282165527,-4.357109069824219,-5.786710262298584,5.085693836212158,5.1948771476745605,-0.2513502538204193,-6.83648157119751,-1.2782174348831177,-1.2012205123901367,-9.011341094970703,-1.5477714538574219,2.3633737564086914,3.0225460529327393,8.89470386505127,8.894835472106934,3.014286994934082,1.8442832231521606,1.8439604043960571,1.9727612733840942,-3.3092286586761475,1.2630105018615723,5.931427955627441,13.109822273254395,13.109814643859863,3.753112316131592,1.2365858554840088,-0.5419723391532898,2.5978939533233643,1.3588666915893555,4.25275182723999,1.9727602005004883,0.17962099611759186,3.138201951980591,7.229483604431152,1.297709584236145,-0.6070635318756104,-0.5954229235649109,-0.5550212264060974,-10.012182235717773,-4.993657112121582,12.497366905212402,1.2365695238113403,1.359025239944458,0.4915620684623718,6.071987628936768,-8.834540367126465,-0.733229398727417,-9.473488807678223,0.21299463510513306,-6.521151065826416,4.726338863372803,0.27363407611846924,6.520446300506592,-6.295063495635986,2.185948133468628,0.1422402560710907,1.7911255359649658,0.10477151721715927,4.826540946960449,0.09368479251861572],"y":[8.113670349121094,9.893635749816895,9.810641288757324,8.114092826843262,9.718452453613281,-6.946948051452637,7.735603332519531,3.544137477874756,5.535734176635742,4.076681613922119,4.070932865142822,-0.6917662620544434,-1.2597777843475342,9.798192024230957,1.376610279083252,-3.670987606048584,1.619265079498291,8.116372108459473,7.906944751739502,-5.6780619621276855,-3.7048676013946533,-6.946958065032959,5.533093452453613,3.9684464931488037,12.045379638671875,-0.8340239524841309,-3.675265312194824,-3.69704532623291,5.5204033851623535,6.78675651550293,9.879960060119629,11.186359405517578,4.060729503631592,3.0406837463378906,0.8595343828201294,12.048713684082031,8.063997268676758,10.292684555053711,3.0598137378692627,4.310322284698486,2.157761573791504,4.236133575439453,4.233948707580566,2.158351421356201,9.599553108215332,7.945862770080566,-3.6821370124816895,-3.9240195751190186,-0.9102680087089539,-0.681236207485199,3.3978421688079834,3.094261884689331,0.5410590171813965,8.118270874023438,3.7032196521759033,-1.4002654552459717,4.886020183563232,1.618992567062378,10.396175384521484,11.005833625793457,-5.041959762573242,-3.924091339111328,3.593968152999878,12.07224178314209,9.802030563354492,-5.678086757659912,-5.6774678230285645,10.594841003417969,8.11429500579834,10.151105880737305,4.250063419342041,5.322165489196777,10.928139686584473,10.928171157836914,-3.6838250160217285,12.100379943847656,14.788280487060547,9.83216381072998,9.81781005859375,0.859264612197876,4.247644424438477,4.266486644744873,-3.680516004562378,12.102651596069336,3.5271003246307373,-1.9486156702041626,7.929585933685303,-6.947326183319092,-6.9471211433410645,8.112616539001465,10.485323905944824,4.007182598114014,4.006289005279541,12.018095970153809,7.926936149597168,-0.6952677369117737,-5.042176246643066,11.082633018493652,8.294386863708496,1.4281072616577148,-1.9488154649734497,8.108363151550293,-3.7812209129333496,-0.9227950572967529,-0.9102231860160828,10.37200927734375,6.778384208679199,6.780393123626709,6.782710075378418,7.8043670654296875,1.624063491821289,3.869548797607422,2.9250950813293457,0.5398669242858887,11.083271026611328,2.429058074951172,3.831141471862793,6.417158126831055,4.292441368103027,-6.947174072265625,-4.85203742980957,5.571714401245117,3.1044366359710693,3.744821310043335,1.4531896114349365,3.976555585861206,10.552040100097656,2.842189073562622,-1.950474739074707,9.753206253051758,1.404188632965088,10.666878700256348,12.02856159210205,6.664002895355225,5.454298973083496,4.929747581481934,-3.9241044521331787,8.378727912902832,5.224916458129883,8.372832298278809,11.016924858093262,10.563451766967773,5.571261405944824,4.0780134201049805,3.0124690532684326,5.306377410888672,10.625718116760254,-0.7294622659683228,11.271036148071289,5.469772815704346,14.749267578125,14.749351501464844,14.749310493469238,8.78458023071289,10.650670051574707,12.022624969482422,11.0054931640625,6.803839683532715,1.1294916868209839,5.298273086547852,5.322137832641602,9.776843070983887,11.252825736999512,11.211381912231445,-4.85204553604126,4.704123497009277,-6.78154993057251,9.779655456542969,9.880249977111816,9.86303997039795,1.5735385417938232,11.055938720703125,11.041857719421387,11.579696655273438,5.454616069793701,-6.781591415405273,1.45271635055542,6.7147674560546875,-0.6207215785980225,12.021686553955078,11.076065063476562,4.046651363372803,5.507997989654541,9.76668930053711,0.8603235483169556,1.6701161861419678,6.778419017791748,0.5394080281257629,7.927375316619873,-6.781561851501465,10.649829864501953,-0.710349440574646,3.883148431777954,-1.9751543998718262,0.9096877574920654,10.658230781555176,4.11980676651001,7.069127559661865,4.910320281982422,5.308498859405518,5.539193153381348,10.924908638000488,11.97519588470459,10.765229225158691,10.04025936126709,11.591412544250488,8.770014762878418,5.556009292602539,5.4543962478637695,3.3238792419433594,11.568170547485352,11.568202018737793,5.62385368347168,2.471379280090332,-0.39842188358306885,0.5398123264312744,2.471383571624756,0.8592925071716309,0.5374450087547302,11.558112144470215,11.517520904541016,5.305972576141357,5.553325176239014,-0.2636895775794983,11.199186325073242,5.3718037605285645,5.557818412780762,11.564652442932129,2.061986207962036,-5.720302104949951,-5.720322608947754,11.034784317016602,11.512760162353516,11.574849128723145,10.965143203735352,11.513056755065918,11.514017105102539,-1.9753596782684326,-1.975368857383728,1.6536370515823364,11.517104148864746,11.64439868927002,10.254509925842285,6.760456085205078,10.386324882507324,-0.9876461625099182,-1.4347810745239258,8.708380699157715,11.718894004821777,-0.23980212211608887,11.040532112121582,6.6594038009643555,6.668013572692871,5.57869291305542,9.089932441711426,-1.0135515928268433,1.6389168500900269,5.454665184020996,3.6340537071228027,10.189987182617188,5.595309257507324,5.584903240203857,11.711441040039062,11.718607902526855,9.106372833251953,11.204533576965332,10.701295852661133,9.880261421203613,-0.6118226647377014,3.453880786895752,6.256800651550293,-0.2754057049751282,3.4981706142425537,5.626950263977051,4.763032913208008,5.5142741203308105,5.549081325531006,10.855263710021973,10.51666259765625,11.244036674499512,13.75383186340332,-1.4352355003356934,4.666665554046631,10.9425687789917,5.577303886413574,-3.788658857345581,5.373776912689209,3.9401211738586426,11.435449600219727,10.60936450958252,-3.9241323471069336,6.7693023681640625,6.7814836502075195,5.348234176635742,10.604486465454102,10.307378768920898,2.805072069168091,5.491544723510742,6.743322849273682,10.089083671569824,-1.0129926204681396,5.378844261169434,1.2771633863449097,0.9920458793640137,8.117518424987793,6.496849536895752,1.2167595624923706,-1.4003839492797852,5.495063304901123,10.784750938415527,-0.7294930815696716,5.021641731262207,10.348377227783203,0.8592734336853027,-7.214982032775879,10.573083877563477,6.4868621826171875,-2.3611884117126465,6.614648818969727,10.78478717803955,-0.4139326214790344,-0.4678839445114136,-1.0083950757980347,9.996184349060059,3.0950112342834473,10.354377746582031,6.478830337524414,6.655031204223633,-1.014747142791748,1.4355460405349731,6.706202030181885,10.201077461242676,6.614831447601318,6.600211143493652,10.377070426940918,4.468404769897461,-7.178654670715332,3.6583375930786133,11.244284629821777,-7.197048187255859,-0.5705610513687134,6.586998462677002,6.619097709655762,3.9089603424072266,11.973620414733887,8.31582260131836,7.7981953620910645,0.995795488357544,11.458639144897461,3.6759331226348877,8.271918296813965,9.726887702941895,5.379974365234375,9.110280990600586,3.6059861183166504,10.375028610229492,2.778949022293091,-1.014468789100647,9.807411193847656,3.8504552841186523,4.885503768920898,12.028129577636719,3.5908749103546143,3.590620994567871,-0.7257083058357239,0.5394599437713623,-7.1615705490112305,10.374059677124023,-7.143109321594238,-7.157307147979736,11.176795959472656,-7.172485828399658,5.583441734313965,10.537104606628418,5.303617477416992,3.5340828895568848,-7.162301063537598,4.326099872589111,6.6477227210998535,13.570589065551758,10.675383567810059,4.982182502746582,3.826996088027954,14.749250411987305,4.8881354331970215,2.5644495487213135,9.5994873046875,4.817317962646484,1.6208500862121582,9.113073348999023,6.556522846221924,3.759960651397705,4.116182327270508,-4.633340358734131,6.532246112823486,11.116630554199219,5.585305213928223,-4.63332986831665,1.159590482711792,-4.633338451385498,-0.5327195525169373,4.304530143737793,8.248187065124512,10.573454856872559,3.7083234786987305,3.6800196170806885,2.0856752395629883,10.4255952835083,10.715668678283691,8.485736846923828,1.1807938814163208,-3.924142837524414,1.9108986854553223,6.427558422088623,4.839415550231934,7.678175449371338,11.777782440185547,5.571332931518555,-0.6090695858001709,-0.7102054357528687,1.5763062238693237,10.427684783935547,1.8832968473434448,10.761972427368164,9.017194747924805,6.448171615600586,3.5451161861419678,1.445025086402893,8.393475532531738,5.585741996765137,-0.6939592361450195,11.973797798156738,2.0668201446533203,11.982643127441406,7.70074462890625,13.570534706115723,2.24454665184021,10.670785903930664,10.684606552124023,5.301017761230469,8.210540771484375,8.318047523498535,6.4820966720581055,5.57083797454834,5.461813926696777,6.256807327270508,11.24355411529541,5.5706400871276855,10.7620210647583,6.689824104309082,3.4055824279785156,0.8014422655105591,7.6755218505859375,9.105171203613281,10.221397399902344,4.620612144470215,3.044389247894287,5.570617198944092,-0.6745676398277283,10.762125015258789,0.47859102487564087,5.569464683532715,2.6986958980560303,-0.22818884253501892,1.697038173675537,0.5394376516342163,4.334570407867432,5.571633815765381,1.206550121307373,1.7522231340408325,4.153439521789551,5.267436981201172,7.675800323486328,10.604168891906738,5.190682411193848,12.038665771484375,8.31568431854248,-5.883447647094727,5.440858364105225,10.653610229492188,8.319416046142578,10.703944206237793,2.548144578933716,3.892629623413086,4.714044570922852,4.552731990814209,9.75330638885498,3.898817539215088,-3.641063690185547,-3.6413097381591797,4.024848461151123,4.988464832305908,10.576773643493652,1.5726531744003296,0.7808894515037537,-0.6638927459716797,-5.883538722991943,4.13586950302124,10.571558952331543,10.338414192199707,4.225528717041016,1.743247628211975,-5.883526802062988,9.107205390930176,13.57060432434082,5.57152795791626,10.94304370880127,0.9066741466522217,4.291659832000732,7.6339311599731445,5.503833770751953,4.114397048950195,3.684810161590576,11.973773956298828,8.769619941711426,3.012164831161499,0.9839369654655457,3.663965940475464,8.356759071350098,1.5745065212249756,4.397056579589844,4.889017105102539,2.6819100379943848,5.454509735107422,10.65371322631836,3.9122421741485596,11.083417892456055,1.5095820426940918,11.08318042755127,11.104588508605957,-0.91033935546875,4.116008758544922,11.10708236694336,5.572740077972412,1.767409324645996,3.059802770614624,12.592516899108887,9.599580764770508,4.116109848022461,0.7673987150192261,0.7669110894203186,3.0112552642822266,2.606900930404663,2.6607630252838135,3.692337989807129,0.8043285012245178,1.7557127475738525,3.127279758453369,1.151827335357666,11.081379890441895,-1.0253748893737793,1.786527156829834,4.89196252822876,4.759403705596924,8.30905532836914,3.580413579940796,-0.9104095101356506,2.0011491775512695,-1.1426079273223877,4.538790225982666,2.7200839519500732,-0.24198593199253082,8.117956161499023,8.117132186889648,8.118587493896484,8.378658294677734,4.00737190246582,3.6836273670196533,0.7619746923446655,7.671999931335449,4.0042219161987305,0.47873079776763916,2.7725167274475098,4.763016700744629,7.659285068511963,11.176694869995117,0.606343686580658,1.2979917526245117,6.66469669342041,2.8577382564544678,12.657028198242188,8.216293334960938,4.79564905166626,9.990859031677246,4.675728797912598,8.490450859069824,-6.196556568145752,12.269330978393555,6.418953895568848,3.961164951324463,9.656417846679688,12.543498039245605,3.6990296840667725,14.426156044006348,4.7285590171813965,15.272631645202637,4.746122360229492,7.068221569061279,-3.228992223739624,8.340287208557129,-4.133017539978027,9.462764739990234,-0.6513314843177795,13.1343412399292,-0.6761071085929871,13.056805610656738,-2.9327845573425293,7.099563121795654,-2.879850387573242,8.28472900390625,3.3204033374786377,9.27314567565918,7.22939395904541,6.65574836730957,14.530729293823242,2.250971794128418,7.251289367675781,2.2604727745056152,9.80848217010498,6.547253608703613,3.5571537017822266,7.892326354980469,6.55359411239624,10.846288681030273,10.846393585205078,4.250546932220459,6.250988960266113,3.3929548263549805,-0.6681632995605469,12.440342903137207,-0.17685487866401672,8.828449249267578,8.828324317932129,8.316938400268555,6.476629257202148,1.1465708017349243,1.1465455293655396,3.6970157623291016,1.8477474451065063,0.9349371194839478,-2.8818721771240234,8.076225280761719,9.453611373901367,13.570402145385742,6.66692590713501,-0.6541318893432617,-2.99516224861145,11.719731330871582,-2.9948742389678955,-6.196030616760254,1.872488021850586,5.188908100128174,7.909217357635498,10.557808876037598,13.230574607849121,5.904176712036133,-0.09742418676614761,-0.09745562076568604,-0.6586461067199707,4.756088733673096,8.825342178344727,2.6492373943328857,0.9350976943969727,1.897329568862915,3.6251354217529297,13.110748291015625,1.2452499866485596,4.759449005126953,16.36826515197754,8.073254585266113,5.681921005249023,8.949190139770508,3.5263922214508057,6.913619518280029,8.312544822692871,6.657397270202637,8.482592582702637,4.417294979095459,3.228675603866577,9.557483673095703,6.900020599365234,3.6907408237457275,1.6189566850662231,3.3182690143585205,-3.2203807830810547,12.542868614196777,6.718141555786133,11.719805717468262,9.656970977783203,-3.3263537883758545,9.45747184753418,3.540762186050415,3.685178756713867,-2.6948931217193604,6.036774635314941,-2.7633461952209473,9.478853225708008,0.242716982960701,9.659530639648438,2.848369836807251,-5.560822486877441,0.84629887342453,15.272868156433105,14.524746894836426,2.260241746902466,-0.6335718035697937,-3.1990818977355957,12.84314250946045,12.766340255737305,2.6626670360565186,8.828834533691406,7.1241888999938965,-0.6403332352638245,3.9947619438171387,4.503036022186279,0.24271179735660553,4.810288429260254,3.5527820587158203,6.9483513832092285,0.5398685932159424,2.1409413814544678,1.2850228548049927,12.631875038146973,3.583369016647339,-3.1631648540496826,1.258225440979004,-0.6741369962692261,9.607294082641602,4.7592949867248535,8.70207405090332,-3.1465160846710205,12.67138671875,-1.5705828666687012,7.89736270904541,7.076173305511475,3.976139545440674,3.9742319583892822,6.870510578155518,11.71979808807373,-0.5673982501029968,-0.5670405030250549,12.923081398010254,3.929710626602173,4.153777599334717,9.652911186218262,-6.195875644683838,-0.6265502572059631,3.804894208908081,16.36804962158203,16.368284225463867,7.051666736602783,7.06556510925293,7.589471817016602,8.106060981750488,8.75016975402832,7.327918529510498,8.92065715789795,-3.6305954456329346,8.757699012756348,8.798543930053711,12.549513816833496,11.018149375915527,3.9791054725646973,6.2951884269714355,1.1411657333374023,8.111326217651367,8.109156608581543,1.0323070287704468,11.474172592163086,11.474230766296387,1.24906325340271,2.039703607559204,7.516364097595215,9.460356712341309,-0.631558358669281,7.060420989990234,13.218618392944336,8.479201316833496,8.5770263671875,2.631498098373413,15.272652626037598,1.1080207824707031,4.0621771812438965,7.203690528869629,-0.6277900338172913,8.560480117797852,4.328566551208496,-0.623577892780304,13.023896217346191,-3.1787469387054443,11.719839096069336,1.8272157907485962,8.12779712677002,10.089122772216797,3.6519410610198975,8.3103666305542,-2.593677282333374,-0.5642096996307373,9.486725807189941,6.872225761413574,8.088580131530762,7.994899749755859,-4.261803150177002,3.1474528312683105,12.590155601501465,7.913503646850586,12.515559196472168,3.977221727371216,16.71493911743164,10.441598892211914,4.121841907501221,4.952372074127197,16.839370727539062,-3.6434614658355713,10.094501495361328,10.090813636779785,10.094731330871582,-5.92888069152832,-5.928920745849609,8.978814125061035,-3.3571271896362305,1.442447543144226,-2.6299312114715576,16.839359283447266,7.064317226409912,16.83939552307129,8.351250648498535,3.368870258331299,7.50947380065918,-0.5206172466278076,2.3792216777801514,6.879755973815918,8.812663078308105,2.281965732574463,8.827108383178711,6.812036037445068,-4.355507850646973,8.091451644897461,-0.0995342805981636,2.621277332305908,4.384744167327881,3.673497200012207,8.527910232543945,-3.365135669708252,8.719794273376465,2.197662353515625,14.472900390625,1.1545602083206177,4.143989086151123,8.157891273498535,2.4314804077148438,2.6540703773498535,13.094900131225586,4.190546035766602,9.241667747497559,3.4673638343811035,7.4337263107299805,4.389759540557861,3.8948872089385986,2.3904449939727783,-6.071803569793701,5.236315727233887,14.420294761657715,14.440146446228027,3.259938955307007,5.231053829193115,6.198845863342285,11.719932556152344,-0.47582027316093445,8.128328323364258,2.617790460586548,5.423800945281982,12.74605941772461,9.406145095825195,4.798194408416748,9.406352996826172,8.04846477508545,5.534485816955566,5.531754970550537,6.790229797363281,3.2639524936676025,2.399963140487671,8.063165664672852,11.670492172241211,2.0313589572906494,11.673479080200195,12.755879402160645,12.75413990020752,11.62205982208252,5.540687084197998,3.2692153453826904,8.13184928894043,11.618828773498535,9.819921493530273,9.456628799438477,4.130302429199219,12.459391593933105,7.4550395011901855,8.479239463806152,11.808878898620605,8.0523099899292,-3.192836284637451,10.099610328674316,7.444491863250732,2.7718007564544678,3.2614972591400146,9.275594711303711,4.065674781799316,12.622919082641602,5.899448871612549,6.301650524139404,7.748905658721924,11.676325798034668,8.587512969970703,12.473638534545898,2.430943250656128,-0.5620639324188232,4.12878942489624,3.690688371658325,-0.8951523900032043,4.02996826171875,-0.4981617331504822,6.3360676765441895,5.393803596496582,-2.703198194503784,11.599620819091797,5.538634300231934,3.266756772994995,4.754021644592285,8.310044288635254,4.655832767486572,2.392519474029541,2.3946900367736816,5.285996437072754,14.473772048950195,-6.3860249519348145,8.153726577758789,-4.355547904968262,12.603984832763672,8.705391883850098,2.035792350769043,-5.928925514221191,11.018142700195312,6.796064853668213,7.379549980163574,6.5778584480285645,3.0034942626953125,-2.7032525539398193,8.30945110321045,8.316702842712402,-0.5228040814399719,8.784523963928223,3.654181718826294,1.114033818244934,12.96574878692627,1.1506431102752686,8.426819801330566,8.943290710449219,3.9805305004119873,12.473915100097656,-0.5162760019302368,3.6070384979248047,12.675200462341309,3.1241657733917236,-0.5523462295532227,8.92071533203125,10.368098258972168,10.351734161376953,0.82735276222229,-0.5399186611175537,9.885449409484863,3.544748067855835,2.685042381286621,12.71902847290039,12.622757911682129,12.623624801635742,3.7422268390655518,11.935426712036133,3.2582361698150635,-0.5378599166870117,11.729743003845215,7.76858377456665,5.842684268951416,9.717275619506836,13.059768676757812,6.636016845703125,3.6043426990509033,2.8589861392974854,12.468337059020996,12.619217872619629,3.855639696121216,7.0690178871154785,10.339923858642578,7.379730701446533,6.580600738525391,8.178544044494629,8.193843841552734,-0.8952348828315735,4.063040733337402,4.057591915130615,0.05499613285064697,8.742666244506836,-2.429316997528076,4.754899501800537,7.106550693511963,8.553642272949219,10.184100151062012,10.16840934753418,11.720108032226562,-0.6200956702232361,3.0200212001800537,6.677847385406494,4.302398681640625,7.768123149871826,9.620712280273438,12.75650405883789,-0.5568458437919617,1.3114336729049683,7.290903568267822,4.355602264404297,7.139543056488037,3.6788454055786133,2.6298110485076904,11.63415241241455,3.682441473007202,2.927293300628662,12.641043663024902,1.437224268913269,3.795686721801758,6.236011981964111,11.843287467956543,-3.6324095726013184,-2.4293386936187744,-3.6321256160736084,7.302011489868164,3.3975296020507812,3.665011167526245,8.813714981079102,2.9275991916656494,8.052115440368652,7.073313236236572,12.627176284790039,8.29868221282959,8.071757316589355,8.04980182647705,-2.7032177448272705,7.085877418518066,3.2658677101135254,3.2838096618652344,7.101564407348633,3.1064260005950928,-2.9948935508728027,7.40425968170166,-2.675887107849121,9.819437026977539,10.086180686950684,-0.6272005438804626,11.71981430053711,0.552505612373352,-2.668884754180908,-6.1992316246032715,10.24295711517334,5.633492469787598,2.606301784515381,2.8895788192749023,9.498608589172363,8.479316711425781,7.940442085266113,2.9347167015075684,1.5286972522735596,6.475285530090332,10.42424488067627,4.118159770965576,3.653629779815674,-2.702885389328003,1.184829831123352,17.77230453491211,17.772348403930664,10.251168251037598,-3.282094717025757,8.740909576416016,4.128401279449463,2.6257221698760986,11.599597930908203,7.096407890319824,8.375264167785645,4.348710060119629,2.5800347328186035,4.7259674072265625,4.371954441070557,0.8444861173629761,12.628093719482422,12.613397598266602,8.747354507446289,8.36667537689209,4.424221515655518,12.603006362915039,-0.7822657823562622,-0.00040893754339776933,8.048028945922852,7.666017532348633,-0.6305333375930786,8.538704872131348,2.649942636489868,10.754983901977539,3.2390220165252686,5.98952579498291,10.225617408752441,10.239001274108887,-6.200127124786377,4.432106018066406,10.408329010009766,14.267757415771484,1.175405740737915,2.578702688217163,8.704195976257324,8.78386116027832,7.293807506561279,7.294591426849365,2.6869993209838867,2.586367130279541,7.085960388183594,7.224924564361572,6.2508978843688965,3.331071138381958,8.092164993286133,8.116222381591797,-3.365110158920288,7.892463207244873,5.944129467010498,9.709614753723145,7.060307502746582,8.62163257598877,1.420633316040039,8.988199234008789,-2.70194411277771,-6.204762935638428,8.280256271362305,5.524095058441162,-1.414119005203247,-0.6311612129211426,3.6285030841827393,7.24981164932251,10.36314582824707,8.897148132324219,2.5400404930114746,5.29771089553833,5.394144535064697,10.42402172088623,3.2610061168670654,-4.550300598144531,14.626092910766602,8.55455493927002,6.201997756958008,9.771440505981445,7.924004554748535,12.657129287719727,3.64113712310791,7.738442420959473,-0.10808213800191879,8.880325317382812,3.1679317951202393,8.089068412780762,6.991735935211182,10.318198204040527,10.277195930480957,4.8011932373046875,-4.675374507904053,7.08418083190918,7.957949638366699,7.968088626861572,9.76684856414795,-1.7340444326400757,8.10069751739502,8.087687492370605,3.9421334266662598,4.754047870635986,1.6675540208816528,7.580354690551758,8.08878231048584,2.4415361881256104,11.644707679748535,1.9282375574111938,5.248053550720215,8.07928466796875,-0.4299089312553406,-2.1591732501983643,4.0126261711120605,4.0195159912109375,6.706858158111572,7.603333473205566,8.045949935913086,10.081541061401367,7.872764587402344,11.836363792419434,7.94254207611084,3.5364441871643066,7.279781341552734,10.356017112731934,10.161901473999023,3.3195836544036865,-0.5965733528137207,3.0095930099487305,8.890241622924805,4.7598981857299805,7.65479040145874,12.591073989868164,5.431663990020752,6.763667106628418,-2.0743677616119385,4.224874973297119,7.0937676429748535,-0.6206766963005066,8.278471946716309,9.876055717468262,10.398399353027344,5.498000144958496,12.977953910827637,-0.5076887607574463,-2.8088104724884033,8.754401206970215,-3.3604607582092285,11.599589347839355,7.9808125495910645,-4.055666923522949,2.5651724338531494,-4.055665493011475,9.478067398071289,11.719681739807129,1.1722314357757568,-4.406213760375977,7.980834484100342,7.595607280731201,7.989858627319336,7.086488246917725,4.032890319824219,2.696108102798462,2.6620166301727295,2.6621253490448,2.6620025634765625,8.125258445739746,0.41898438334465027,3.113985061645508,7.431089878082275,7.430301666259766,9.025778770446777,-0.5178735256195068,-0.5365040898323059,9.54063892364502,10.091293334960938,7.79842472076416,2.7501649856567383,8.019508361816406,-1.4140851497650146,7.607880592346191,0.8979198336601257,5.435244560241699,-6.197465419769287,14.267509460449219,7.1006364822387695,9.429910659790039,8.310455322265625,3.408046245574951,3.3147659301757812,8.049640655517578,-3.7485640048980713,8.279827117919922,6.48175048828125,6.483370780944824,7.148232936859131,8.27706241607666,12.77207088470459,10.409772872924805,-0.4753618836402893,1.114038348197937,3.6613032817840576,1.7939802408218384,2.89581036567688,4.135705947875977,14.26791763305664,2.015939712524414,1.6191978454589844,0.05530645698308945,3.3973071575164795,5.525946617126465,-3.7485973834991455,-0.8951594233512878,10.321394920349121,0.4861298203468323,1.1639397144317627,0.05513744801282883,10.365251541137695,1.5920222997665405,-6.1986846923828125,9.014652252197266,2.8177480697631836,12.911327362060547,1.8011894226074219,1.2727553844451904,6.201320648193359,-1.7340075969696045,8.600350379943848,0.8274611830711365,10.17902660369873,4.7519025802612305,1.808559775352478,14.476600646972656,2.3605551719665527,6.050477981567383,8.115534782409668,8.898407936096191,12.188191413879395,8.523365020751953,13.736640930175781,7.753537178039551,8.386528015136719,8.091607093811035,1.2697495222091675,6.202025413513184,8.08671760559082,4.095089912414551,9.437905311584473,7.637740612030029,3.3390207290649414,14.450521469116211,10.754902839660645,8.985051155090332,8.541150093078613,5.742677688598633,1.3883342742919922,2.3433632850646973,10.164143562316895,10.204700469970703,8.31569766998291,8.130852699279785,10.361919403076172,-0.48676788806915283,-0.4864625632762909,0.6057108044624329,2.025569438934326,10.246371269226074,-1.066699504852295,4.730889797210693,6.026825428009033,5.75996732711792,-3.7486774921417236,10.260476112365723,4.080043315887451,4.06110954284668,12.627189636230469,12.569989204406738,10.354668617248535,15.272665977478027,3.311281204223633,3.8209519386291504,6.201841831207275,8.589943885803223,4.995941638946533,8.078150749206543,3.304621934890747,2.9108972549438477,14.536877632141113,8.759325981140137,4.742648124694824,2.713209867477417,-0.6681273579597473,8.147623062133789,10.081762313842773,8.323969841003418,7.582587718963623,3.5526175498962402,8.074378967285156,4.495903968811035,6.7608819007873535,6.762522220611572,5.195411205291748,8.10763931274414,10.06326675415039,4.220311164855957,10.062251091003418,7.180351734161377,4.015676498413086,8.69229507446289,3.2574305534362793,8.836008071899414,2.6620166301727295,4.759239673614502,5.11734676361084,3.820655345916748,8.757770538330078,-3.7486698627471924,3.3112123012542725,3.92328143119812,6.042701721191406,1.8111459016799927,0.09498295933008194,12.188153266906738,4.569062232971191,-0.5242263674736023,5.391968727111816,8.421258926391602,9.550454139709473,2.8932271003723145,0.03436575457453728,3.847249984741211,10.37570571899414,-2.5538113117218018,0.5535864233970642,3.339264392852783,0.9713735580444336,8.29176139831543,2.8636696338653564,12.911356925964355,4.050870418548584,4.023125171661377,3.2328760623931885,0.8813454508781433,6.775724411010742,7.591674327850342,3.1125974655151367,6.867701530456543,7.993030548095703,6.201842784881592,8.423687934875488,11.114364624023438,8.157888412475586,3.186070680618286,6.956201553344727,7.291268825531006,6.289520740509033,5.07817268371582,7.1219587326049805,3.802846670150757,4.969019889831543,8.592531204223633,3.977226495742798,8.560729026794434,8.317758560180664,4.969077110290527,-2.5285255908966064,2.013899326324463,11.114521026611328,6.024577617645264,9.009286880493164,-3.027482032775879,8.112120628356934,8.123263359069824,5.214995861053467,4.379951477050781,4.359253883361816,13.189208030700684,7.663416862487793,5.987645626068115,7.976066589355469,12.898441314697266,7.4163432121276855,2.738064765930176,12.943347930908203,7.091764450073242,7.7860612869262695,12.620414733886719,8.334579467773438,3.7605769634246826,0.8571099042892456,7.6163506507873535,10.246366500854492,-0.4889329671859741,8.36782455444336,8.323177337646484,8.375992774963379,0.06191013380885124,9.561606407165527,-2.5056705474853516,6.424840450286865,5.70048713684082,10.359345436096191,11.653708457946777,7.965516090393066,11.193633079528809,7.5442795753479,6.427379608154297,5.504048824310303,-3.6325042247772217,2.7112975120544434,9.9876070022583,8.073869705200195,3.4990744590759277,2.8868279457092285,15.828977584838867,12.625663757324219,3.734534740447998,7.4073920249938965,1.8208866119384766,6.0531721115112305,11.967375755310059,3.4990556240081787,3.499086618423462,8.529293060302734,6.709837436676025,-1.1538572311401367,3.9212729930877686,-0.6693667769432068,6.788222789764404,5.700493335723877,5.7004499435424805,5.017001152038574,3.7512528896331787,-2.4156174659729004,7.758842468261719,14.528000831604004,2.735342502593994,11.975662231445312,-4.547307014465332,11.969992637634277,12.006043434143066,12.003183364868164,9.02636432647705,4.018497467041016,6.203537940979004,16.12980842590332,2.423983573913574,11.79766845703125,6.307798862457275,3.629483699798584,7.0625319480896,3.7995119094848633,12.00384521484375,2.337810754776001,4.081862926483154,11.654664039611816,8.548532485961914,2.254331111907959,7.074634552001953,8.125177383422852,15.828996658325195,12.00810432434082,-0.17965063452720642,11.193706512451172,4.077413082122803,10.246164321899414,8.075057983398438,9.558453559875488,12.007280349731445,-2.4736123085021973,2.437828779220581,2.5986716747283936,3.464245080947876,7.591999053955078,-2.2764804363250732,3.9871602058410645,7.076900005340576,0.9028456807136536,-1.606028437614441,2.2380967140197754,6.81704568862915,1.869547724723816,15.829008102416992,-2.9019222259521484,6.331386566162109,10.280389785766602,0.8111563920974731,1.1140400171279907,9.47293758392334,8.085636138916016,10.28121280670166,8.332386016845703,15.824453353881836,1.1140053272247314,2.8839709758758545,10.059534072875977,10.053516387939453,12.292604446411133,7.596028804779053,11.097599983215332,8.298127174377441,4.356408596038818,0.9814342856407166,10.09573745727539,7.706108570098877,9.550254821777344,8.53927993774414,5.7004714012146,3.3097543716430664,6.88455867767334,7.096853256225586,8.701983451843262,0.6367254257202148,-2.2458620071411133,7.767480373382568,7.7441020011901855,4.74001407623291,0.8543043732643127,7.12336540222168,4.684758186340332,4.103294849395752,17.7724666595459,8.407958030700684,8.427804946899414,8.495555877685547,8.440544128417969,7.413162708282471,-0.8635887503623962,1.967066764831543,2.4237067699432373,4.158595561981201,3.3107688426971436,9.152761459350586,-2.4284958839416504,5.700508117675781,-2.5018725395202637,4.7431416511535645,3.0075676441192627,2.4297568798065186,-2.2458243370056152,-2.245567798614502,8.49459171295166,9.694997787475586,3.69050931930542,-2.276747465133667,-2.276611328125,8.242554664611816,8.259049415588379,8.923919677734375,7.758219242095947,5.186086654663086,3.9706735610961914,4.036430835723877,5.35978889465332,4.02074670791626,8.661343574523926,7.498677730560303,8.343403816223145,11.097210884094238,7.517049789428711,3.920670747756958,6.335836887359619,5.411745071411133,7.514188766479492,3.681483507156372,8.763884544372559,4.129229545593262,9.75117015838623,12.315658569335938,9.479574203491211,9.473928451538086,7.450272083282471,7.640748023986816,7.753581523895264,3.7142820358276367,-2.457515001296997,-3.7134435176849365,7.60735559463501,7.609016418457031,-3.7134459018707275,3.057657241821289,-3.7134475708007812,3.487464189529419,-2.42854642868042,11.641969680786133,0.7403272986412048,9.471006393432617,3.0802502632141113,5.522688388824463,7.515832901000977,12.285262107849121,2.2550055980682373,-1.4128460884094238,-2.648822546005249,5.655848026275635,7.697513580322266,2.869497060775757,4.031350612640381,7.5283918380737305,4.747735977172852,5.763532638549805,4.7434234619140625,3.948230504989624,5.168814182281494,-0.6092497110366821,7.717984676361084,1.020227313041687,10.061616897583008,1.5318653583526611,6.971883296966553,12.297810554504395,10.84632396697998,8.499409675598145,3.362717390060425,5.480949878692627,5.263472080230713,-2.7386646270751953,4.016965389251709,6.607745170593262,11.935563087463379,5.2259039878845215,1.9957067966461182,6.633852481842041,5.163751602172852,-2.0152647495269775,7.6591386795043945,7.684659004211426,4.0737738609313965,2.4561479091644287,10.422407150268555,8.68327522277832,-0.6759989857673645,-0.6761417388916016,4.804644584655762,-5.542773723602295,-5.543081283569336,17.772554397583008,3.3105037212371826,8.172561645507812,1.3709925413131714,2.817711591720581,2.8177101612091064,8.454511642456055,-3.618436574935913,-2.4160215854644775,1.5967848300933838,4.157443523406982,-0.8948761820793152,17.7725772857666,2.0124690532684326,3.5375070571899414,2.2707760334014893,8.171834945678711,8.163485527038574,8.171258926391602,-2.4250130653381348,3.979311227798462,8.50024127960205,7.44362735748291,-3.6184885501861572,-0.4853648543357849,9.108490943908691,6.827098369598389,4.01714563369751,7.4279866218566895,4.089634418487549,8.370179176330566,8.888884544372559,3.5216522216796875,8.385685920715332,-4.373534202575684,1.1747273206710815,10.30474853515625,5.6144118309021,14.883085250854492,3.55315899848938,4.705144882202148,8.370946884155273],"z":[1.1015775203704834,4.318093776702881,4.197763919830322,1.100240707397461,3.9303383827209473,-2.216585397720337,0.5578038096427917,6.892659664154053,-5.80401086807251,6.756845474243164,6.7476959228515625,-5.150374889373779,10.003236770629883,4.216006278991699,-4.601091384887695,0.935518205165863,9.698488235473633,1.09967839717865,0.7627423405647278,3.053934335708618,0.9701097011566162,-2.2165935039520264,-5.804739475250244,5.709224700927734,5.549618721008301,1.8512223958969116,0.9397158026695251,0.9621975421905518,5.308732509613037,6.6401872634887695,4.001245975494385,5.208722114562988,6.765163898468018,5.139490127563477,-2.4550094604492188,5.546466827392578,1.0198044776916504,4.904741287231445,5.143208026885986,5.963014125823975,7.9080586433410645,6.422417163848877,6.421783447265625,7.909844398498535,7.830015659332275,0.8225019574165344,0.946556031703949,-1.5293997526168823,-1.714705467224121,-5.164304733276367,5.551845073699951,5.181420803070068,11.769272804260254,1.1178970336914062,6.350636959075928,-2.8539860248565674,6.993119716644287,9.700772285461426,4.796082019805908,1.3706037998199463,5.832893371582031,-1.5293723344802856,6.699541091918945,5.522644996643066,4.210752487182617,3.053945541381836,3.0539956092834473,5.319597244262695,1.1080322265625,4.885807991027832,6.469671726226807,4.9556379318237305,6.130722999572754,6.130098819732666,0.9483627080917358,5.494682312011719,3.335080862045288,4.246594429016113,4.239351749420166,-2.455967664718628,6.4738993644714355,6.459577560424805,0.9448057413101196,5.492227554321289,6.9249982833862305,-5.554708480834961,0.7858064770698547,-2.2174811363220215,-2.216799020767212,1.1068553924560547,5.124796390533447,6.469878673553467,6.465157508850098,5.5775556564331055,0.7843543887138367,-5.146201133728027,5.833115577697754,-1.7489992380142212,1.0774431228637695,-4.627814769744873,-5.554824352264404,1.0963268280029297,1.1853059530258179,1.8627785444259644,-1.714477300643921,4.333414554595947,6.614030838012695,6.602741241455078,6.623003959655762,0.6275490522384644,9.723864555358887,6.251883506774902,5.072169780731201,11.776460647583008,-1.7481557130813599,4.8955254554748535,6.371553897857666,7.22504186630249,6.513289928436279,-2.216787338256836,3.2511942386627197,4.741456031799316,5.180471420288086,6.24116325378418,-4.564547061920166,5.697836399078369,4.003983497619629,5.018462657928467,-5.555671691894531,3.908216714859009,-4.615357398986816,5.108718395233154,5.566336154937744,6.9042887687683105,-3.1365010738372803,5.864215850830078,-1.5294954776763916,4.999291896820068,5.3016252517700195,5.000235557556152,5.7575225830078125,5.019859790802002,4.742884635925293,6.770137786865234,5.100372791290283,4.970261573791504,5.128915786743164,-3.8604588508605957,5.162531852722168,5.578833103179932,4.651489734649658,4.651489734649658,4.651498794555664,1.0848206281661987,5.363890647888184,5.573375701904297,1.3725206851959229,6.633848190307617,6.096585750579834,4.978774070739746,5.0161237716674805,4.078737258911133,4.265718936920166,5.086940765380859,3.2512025833129883,5.946990013122559,3.778886556625366,4.123016357421875,1.0530236959457397,5.088467597961426,-4.747241973876953,5.9485931396484375,5.6055908203125,5.1610026359558105,-3.143202543258667,3.7789101600646973,-4.5802388191223145,4.011271953582764,1.794702410697937,5.574304580688477,5.926682949066162,6.679956912994385,5.41958475112915,4.083998680114746,-2.4529941082000732,-4.8676581382751465,6.656863212585449,11.77834415435791,0.7825607657432556,3.7788734436035156,5.539428234100342,1.827746868133545,6.258541584014893,-8.669431686401367,-4.332294464111328,5.2813005447387695,-5.350375175476074,-4.379711151123047,5.732330799102783,5.007291316986084,4.760790824890137,4.3455491065979,5.620920181274414,4.562411308288574,4.509221076965332,5.309747695922852,4.982944011688232,5.379921913146973,-3.1390981674194336,2.76900315284729,4.0936808586120605,4.093632221221924,5.3333611488342285,-6.253657817840576,1.491709589958191,11.7769193649292,-6.253696918487549,-2.4557573795318604,11.776639938354492,1.259839415550232,1.32060968875885,4.976364612579346,5.355729103088379,1.5804182291030884,5.284735202789307,5.052451133728027,5.3590192794799805,1.2545641660690308,6.729882717132568,6.561930179595947,6.5619587898254395,5.912871360778809,1.322139859199524,1.246997594833374,4.577728748321533,1.3279668092727661,1.3277491331100464,-8.669593811035156,-8.6696195602417,-4.8490166664123535,1.3249038457870483,5.141123294830322,4.249849796295166,6.6801676750183105,4.7511067390441895,1.8785465955734253,4.564703941345215,4.992384433746338,5.150163650512695,1.5990219116210938,5.163768291473389,6.918393611907959,6.891549587249756,5.368638515472412,0.8868341445922852,-4.342313289642334,-4.83045768737793,-3.1411914825439453,6.362850666046143,4.685737609863281,5.727717876434326,5.7160115242004395,5.152588367462158,5.151710510253906,0.8815199136734009,5.240287780761719,3.9098598957061768,1.0513266324996948,1.792131781578064,2.9016740322113037,4.653598785400391,1.5700526237487793,2.949446678161621,5.322978973388672,5.9413228034973145,5.42588472366333,5.669491767883301,4.701750755310059,4.145408630371094,-3.967273712158203,2.352365016937256,4.564570903778076,5.933478832244873,-3.9567370414733887,5.706632137298584,1.17041015625,5.02142858505249,5.761826992034912,4.0896124839782715,4.013485431671143,-1.529358983039856,6.67260217666626,6.657218933105469,5.045588970184326,4.049361228942871,4.883066654205322,5.015803337097168,5.611380577087402,6.714957237243652,4.919371128082275,1.8861736059188843,5.509971618652344,-4.549227237701416,-4.394026279449463,1.0956636667251587,7.153257369995117,-4.519321918487549,-2.853860378265381,4.853457450866699,7.795101165771484,-3.8605878353118896,7.15095853805542,4.848703384399414,-2.456012487411499,-0.016223574057221413,5.2965569496154785,7.163966655731201,3.955186128616333,7.0137810707092285,7.795162677764893,1.6771539449691772,1.7004685401916504,1.8844952583312988,4.080381393432617,5.1885600090026855,4.852604389190674,7.172222137451172,6.932112216949463,-4.343396186828613,-4.635990142822266,6.790256023406982,4.893862724304199,7.013929843902588,7.013408660888672,4.825438976287842,5.775737285614014,-1.0562341213226318,6.488020420074463,-3.9676363468170166,-1.07418692111969,1.7619229555130005,7.0523529052734375,7.0096354484558105,5.933636665344238,9.716383934020996,5.637029647827148,0.6181005835533142,-4.397772789001465,4.07168436050415,5.931698322296143,5.341722011566162,3.911464214324951,5.0261149406433105,0.8885775804519653,6.652228355407715,7.123938083648682,5.016080856323242,-4.343344211578369,3.94228458404541,5.899043560028076,6.98267936706543,5.56640100479126,6.704530239105225,6.704059600830078,1.847538948059082,11.777158737182617,-1.0380939245224,7.1250104904174805,-1.0196805000305176,-1.0342885255813599,-0.7167770266532898,-1.0491505861282349,5.377553462982178,3.96270751953125,5.06102991104126,6.389753341674805,-1.041638970375061,-5.4871721267700195,6.94905948638916,5.929504871368408,5.390158176422119,5.7527875900268555,-6.186765193939209,4.651478290557861,5.865692138671875,6.256192207336426,7.830049514770508,5.684991359710693,9.705183982849121,0.8839384913444519,7.0897650718688965,6.130521297454834,5.962527751922607,0.3442765176296234,7.114891529083252,5.208653450012207,4.733444690704346,0.3443937599658966,-4.4887871742248535,0.3442796468734741,1.737126111984253,6.549729824066162,5.251718521118164,5.335919380187988,6.300746440887451,5.927713394165039,6.299556255340576,4.528517246246338,4.135143756866455,1.0757954120635986,6.101500034332275,-1.5293805599212646,6.170477867126465,7.216205596923828,5.7370734214782715,0.5070061683654785,3.543625831604004,2.7456369400024414,1.792468786239624,1.8303929567337036,5.5280537605285645,4.345456123352051,6.174219131469727,-0.21232439577579498,-5.565782070159912,7.198300361633301,6.383196830749512,-4.554962635040283,5.336068153381348,4.7284746170043945,-5.147292613983154,9.716392517089844,6.719566822052002,3.621490478515625,0.5253323316574097,5.929377555847168,6.111193656921387,5.38048791885376,5.3977508544921875,5.028320789337158,5.284860134124756,5.00789737701416,7.170013904571533,2.7460272312164307,5.2011590003967285,4.653548240661621,-3.966653823852539,2.746359348297119,-0.21238940954208374,6.8285298347473145,5.547280788421631,-4.250116348266602,0.5026262998580933,0.8841571807861328,4.894153594970703,6.192951202392578,6.267716884613037,2.746333360671997,-5.178586483001709,-0.21255551278591156,-0.8138411045074463,2.7477362155914307,5.022258281707764,1.6003156900405884,5.605292320251465,11.778509140014648,6.1983208656311035,2.745082378387451,4.242160797119141,-4.977270126342773,6.34332799911499,5.47647762298584,0.5039361715316772,5.425837516784668,5.460396766662598,3.6393301486968994,5.0075201988220215,1.9653637409210205,5.457820415496826,5.365908622741699,5.006089210510254,5.109226703643799,6.213950157165527,5.840665817260742,6.092005252838135,6.015378952026367,3.9105963706970215,5.808797359466553,0.9060510396957397,0.9062849879264832,-0.4950554668903351,5.746565341949463,4.049717903137207,-4.730292320251465,-4.233619213104248,-5.187743186950684,1.9653897285461426,6.2794647216796875,5.342160701751709,4.8575825691223145,6.429443836212158,-4.965584754943848,1.965397596359253,0.9080353379249573,5.929541110992432,2.7450473308563232,-3.9564208984375,-4.329615116119385,6.539400577545166,0.4832945168018341,4.855425834655762,6.2904767990112305,6.342186450958252,9.716301918029785,4.9805121421813965,8.007661819458008,-4.385376453399658,6.351346969604492,5.00092887878418,4.567012310028076,6.232554912567139,-5.864075660705566,5.0103440284729,-3.144029140472412,5.548007488250732,5.838977813720703,-1.750960111618042,5.139428615570068,-1.7549824714660645,-1.7428293228149414,-1.7151601314544678,5.9627909660339355,-1.7401162385940552,5.370967864990234,5.831643104553223,6.263651371002197,-4.268604278564453,7.830183982849121,5.96258020401001,-4.221079349517822,-4.2199602127075195,8.006112098693848,6.242062568664551,6.248828411102295,6.295102119445801,-4.252121925354004,-4.981388092041016,5.26301908493042,4.2439141273498535,-1.7573400735855103,10.213723182678223,4.594360828399658,6.98633337020874,5.947030544281006,5.007270336151123,6.744930267333984,-1.7152109146118164,6.13032341003418,10.108956336975098,5.866826057434082,6.255763530731201,1.5922423601150513,1.096274733543396,1.0954195261001587,1.095789909362793,1.0772918462753296,-0.4813310205936432,6.335964679718018,-4.215991497039795,0.5011709928512573,6.417577266693115,-0.8139711618423462,6.256792068481445,5.940232276916504,0.4905281960964203,-0.7166950702667236,6.329094409942627,1.3008414506912231,-1.272348403930664,2.7748167514801025,1.3685392141342163,-5.911296367645264,10.144177436828613,-3.225470781326294,3.3320024013519287,-3.9711742401123047,4.225924491882324,-4.666384220123291,-3.4536776542663574,1.7285720109939575,-5.224082946777344,-4.684372425079346,-8.976449966430664,-5.992680072784424,2.1341192722320557,4.67104959487915,10.046586990356445,-6.660678386688232,7.04069709777832,7.07192850112915,-0.21328189969062805,-2.1766114234924316,5.291779518127441,1.2435246706008911,-3.650552272796631,1.2151596546173096,7.173880577087402,-5.026421546936035,7.201223373413086,-5.891016483306885,-4.700733661651611,-3.4526336193084717,-1.2679249048233032,1.2672834396362305,-5.882739067077637,-5.391870498657227,-6.589550971984863,-5.397261142730713,9.603083610534668,1.006393551826477,-1.2574290037155151,-4.012363910675049,1.011985182762146,-6.16355562210083,-6.163875102996826,-5.648587703704834,-4.007702350616455,-5.622115612030029,5.278436183929443,-4.689046859741211,7.317234039306641,7.486615180969238,7.486480712890625,-3.6911919116973877,-3.540644645690918,1.780426025390625,1.7804120779037476,3.4348015785217285,-5.230461597442627,-4.80623197555542,-0.44405993819236755,7.56362771987915,-3.250079393386841,5.9291887283325195,1.2868742942810059,5.295373916625977,5.102445602416992,2.997554302215576,5.1027703285217285,4.2255377769470215,-5.244354724884033,-0.9932724237442017,6.383309364318848,-4.5971221923828125,1.3352470397949219,-1.4206751585006714,-2.1022653579711914,-2.102431297302246,5.286440849304199,10.059185981750488,7.4842352867126465,-0.2923017740249634,-4.807032585144043,-5.260331153869629,-9.054543495178223,1.2236406803131104,3.466160774230957,10.053705215454102,3.216763973236084,0.8935021162033081,-3.7092020511627197,0.3493974506855011,-1.2332807779312134,1.9292700290679932,-1.9210398197174072,1.2734140157699585,-1.423646092414856,-0.4095926284790039,5.745461940765381,-5.003232479095459,-1.11689031124115,6.343985080718994,5.2715020179748535,0.6637866497039795,7.047573566436768,-4.6843581199646,-4.202358722686768,2.997593402862549,-5.225289344787598,-0.3239784240722656,-5.140777587890625,-1.2460122108459473,3.429581642150879,7.153275489807129,-2.9526047706604004,7.253163814544678,-4.953810214996338,10.40196704864502,-5.227232456207275,-0.970257580280304,1.7363624572753906,-0.8164531588554382,4.670872688293457,-5.891608238220215,-5.397312164306641,5.297598838806152,7.063119411468506,-9.606919288635254,1.357229471206665,-3.3722314834594727,7.486813545227051,-4.972994804382324,5.295058727264404,-0.47157007455825806,-0.7072380185127258,10.402021408081055,10.164344787597656,-11.014518737792969,-1.0369455814361572,0.8136324286460876,0.7424381971359253,0.43856537342071533,1.3931970596313477,-0.8735622763633728,7.082404136657715,3.4519426822662354,5.273270606994629,-5.11364221572876,10.057432174682617,-3.0715491771698,7.091645240783691,1.3660842180252075,-0.0555097758769989,-4.008275985717773,-5.023324012756348,1.721930980682373,1.7212129831314087,-4.116916656494141,2.9975574016571045,-1.6771239042282104,-1.6768691539764404,1.277185082435608,3.6467044353485107,-1.623699426651001,-5.211574077606201,4.22548770904541,5.302935600280762,-5.6025004386901855,3.2167844772338867,3.2167739868164062,-4.409696102142334,-4.399948596954346,-6.849112510681152,-6.093008995056152,-4.65720796585083,-0.8164879083633423,-4.632343769073486,4.298022270202637,7.421646595001221,7.460287094116211,-4.681394100189209,12.46976089477539,-0.4558297097682953,1.9417223930358887,-4.8849873542785645,-6.089909076690674,-6.092466831207275,-4.8404035568237305,-2.3675644397735596,-2.3676340579986572,3.4623188972473145,1.5468076467514038,-6.5338664054870605,-5.1246466636657715,5.315710544586182,-6.706803321838379,1.3240655660629272,-9.289164543151855,-1.4423972368240356,2.3679587841033936,4.670951843261719,-3.7737462520599365,-0.38816317915916443,-4.690367221832275,5.320806980133057,-4.6199870109558105,-0.4230786859989166,5.384042263031006,1.2323670387268066,7.087154865264893,2.997607946395874,-5.213406562805176,-6.058056831359863,-6.201590538024902,-9.027606964111328,7.048526287078857,-0.3142869472503662,-10.391326904296875,-5.417593955993652,-4.2650675773620605,-5.748693466186523,-5.023090362548828,-0.16005854308605194,-0.5549989342689514,1.4105534553527832,6.3880767822265625,1.4560861587524414,-0.45193275809288025,-3.8408772945404053,9.755304336547852,-0.6693808436393738,-1.4403831958770752,-1.1982396841049194,-0.28376373648643494,2.73616099357605,2.7281675338745117,2.736145496368408,0.19181247055530548,0.19182221591472626,-4.550367832183838,-2.1207635402679443,-7.45298957824707,7.1230340003967285,-1.1982100009918213,-6.708270072937012,-1.198238730430603,7.040167331695557,0.6598029136657715,-6.5498762130737305,-1.6328389644622803,-0.6728097200393677,-4.267428398132324,-4.734574317932129,-0.7706677317619324,-4.733054161071777,-4.238838195800781,-3.4266693592071533,7.586945533752441,-0.4333306849002838,-0.7264516353607178,3.774348735809326,-1.2900302410125732,7.209177017211914,-2.118488311767578,7.386072158813477,9.947592735290527,-4.992668628692627,-1.0152884721755981,-6.344106674194336,-1.2928649187088013,-0.4320251941680908,-0.28858163952827454,1.2106283903121948,-1.6817060708999634,-4.7467145919799805,0.3770856261253357,-2.511054277420044,-5.6840972900390625,0.46794620156288147,-0.6867966055870056,3.034450054168701,-5.866827964782715,-5.998754024505615,-5.979196071624756,2.69873046875,2.9780004024505615,-0.0581732876598835,2.9976115226745605,-1.5964784622192383,-6.117582321166992,-0.7236227989196777,-1.2737387418746948,-5.739758491516113,-2.3346691131591797,2.4144747257232666,-2.3347373008728027,-5.660120964050293,-3.6477277278900146,-3.647273302078247,1.628117322921753,-5.6247148513793945,-0.66706782579422,-1.3455955982208252,1.0833525657653809,1.537398099899292,1.0627145767211914,-5.749203205108643,-5.747248649597168,-7.450899600982666,-3.650456428527832,-1.636170506477356,-6.122966766357422,-7.454817771911621,-9.622881889343262,-5.101053714752197,-1.0903621912002563,1.4264154434204102,-1.442436695098877,-9.28925895690918,0.9567008018493652,-5.00643253326416,7.071508407592773,-6.209908485412598,-2.5187594890594482,4.887678146362305,-1.6371533870697021,-3.446127414703369,-3.909121513366699,-4.299436092376709,-1.421413779258728,1.9357966184616089,0.45974189043045044,1.064988613128662,-4.107962131500244,1.4445033073425293,2.6172564029693604,-0.5598421096801758,-0.6615393757820129,-8.98664379119873,5.585114002227783,-0.49922680854797363,-1.6181553602218628,1.9089062213897705,1.5667515993118286,-5.491446495056152,3.6924796104431152,-3.650015115737915,-1.6266685724258423,-1.6935374736785889,6.973609447479248,-1.1682530641555786,-0.6899272203445435,-0.6875062584877014,-6.538795471191406,-4.993010997772217,-4.754138469696045,0.6515102386474609,-3.42667293548584,-4.280211448669434,-1.5403804779052734,1.5489099025726318,0.19180752336978912,12.46969223022461,1.6436549425125122,8.850076675415039,1.100790023803711,-5.648068904876709,-5.4915008544921875,7.037579536437988,7.085189342498779,-1.6367053985595703,-4.672283172607422,-1.5758684873580933,-7.983434200286865,1.4671186208724976,-1.0074800252914429,-1.1025729179382324,-4.598357200622559,0.39171159267425537,1.4336110353469849,-1.6327438354492188,0.12281733006238937,-8.138737678527832,6.1993913650512695,-6.337627410888672,-4.646119117736816,-6.322846412658691,-6.319266319274902,4.712131023406982,-1.6507192850112915,12.298213958740234,-1.5874881744384766,2.5008316040039062,-5.7110772132873535,5.647730827331543,5.6478776931762695,-1.546311616897583,-2.0447490215301514,2.701864242553711,-1.647066354751587,1.029301404953003,-0.551256000995636,5.3499755859375,-4.972048282623291,1.2160860300064087,-1.3108607530593872,-1.5816705226898193,2.7610771656036377,1.427972674369812,-4.295624732971191,-1.573877215385437,-4.378040790557861,9.752838134765625,8.85187816619873,1.1054892539978027,6.793729782104492,6.821053981781006,5.5850067138671875,-0.5293796062469482,-0.5241724848747253,-5.602404594421387,-4.288443565368652,-0.41612517833709717,2.448046922683716,-6.668984413146973,-4.605957508087158,9.626126289367676,9.60943603515625,2.9976515769958496,5.467102527618408,-0.35269516706466675,0.5472226142883301,-0.40177249908447266,-0.5515813231468201,-5.6454973220825195,-5.749466419219971,-0.5606105923652649,-4.9382500648498535,-1.308395504951477,-0.9600098729133606,-6.665874004364014,3.427320957183838,0.5524266362190247,-7.435303688049316,3.427882194519043,-3.7879574298858643,-4.649752616882324,-7.4516987800598145,-3.958810567855835,-1.3432531356811523,0.9307172298431396,4.299900531768799,-0.4161291718482971,4.299421787261963,1.8272260427474976,-1.1083531379699707,-9.014120101928711,-4.734272480010986,2.1606550216674805,-5.640977382659912,-4.382541179656982,-4.303353309631348,-1.3203164339065552,-5.872462272644043,6.59417200088501,-5.4914937019348145,-4.394808769226074,-1.6350888013839722,-1.6312874555587769,-6.6840314865112305,-5.578904628753662,5.102754592895508,-4.447378158569336,7.512759685516357,-9.622476577758789,-4.978102207183838,5.517497539520264,2.997612953186035,0.8082879185676575,7.523439884185791,4.227686405181885,9.684700012207031,-1.3893170356750488,-0.4111582636833191,2.731898069381714,-3.203312635421753,-9.2892427444458,-5.036717891693115,-0.20705820620059967,-0.800422728061676,4.784910678863525,9.779353141784668,-0.7953857183456421,-9.026029586791992,-5.491365909576416,-1.0527406930923462,0.5778248310089111,0.5778242349624634,9.671964645385742,-0.31657785177230835,6.6474456787109375,-0.6901717782020569,0.5430532693862915,3.692484140396118,-4.402996063232422,7.287565231323242,-0.4162251949310303,-5.384768962860107,2.237396717071533,3.425607681274414,-3.282367467880249,-4.663856506347656,-4.665751934051514,-1.4832420349121094,-1.2488912343978882,2.5671751499176025,-4.2790846824646,-0.5180726647377014,-0.524622917175293,-6.148818492889404,-1.4067754745483398,5.505434989929199,-1.6225314140319824,0.5278484225273132,6.527956485748291,-0.6848605871200562,-5.495173454284668,9.665449142456055,9.674273490905762,4.228290557861328,2.5838942527770996,-6.370992183685303,4.575493335723877,-1.0497353076934814,-0.6566183567047119,-1.483698844909668,-4.668046951293945,-1.3083070516586304,-1.3065712451934814,-3.711089849472046,-3.7161221504211426,-4.393972396850586,-1.2691911458969116,-4.007551193237305,-1.4561493396759033,-6.062543869018555,-6.101834774017334,-2.118483304977417,-4.011792182922363,-5.511599540710449,-5.740496635437012,-4.403249740600586,-1.4999785423278809,-4.9933762550354,-5.582812309265137,-5.491023540496826,4.230404376983643,-1.3791636228561401,-0.5914227366447449,-2.627795457839966,5.503887176513672,-9.049755096435547,-4.5761213302612305,-6.344480514526367,-6.171789646148682,-1.123887538909912,-6.399484157562256,-2.507046699523926,9.70987606048584,2.6982903480529785,-0.018703212961554527,-1.871278166770935,-4.6090168952941895,1.37356436252594,-5.84372615814209,-5.410678863525391,1.3725872039794922,-9.037393569946289,-6.265700340270996,-0.4285335838794708,-4.666254997253418,2.6408743858337402,-6.04866886138916,-4.396348476409912,-6.30963659286499,-6.298887729644775,2.4138424396514893,-4.873381614685059,-4.392650127410889,6.456099510192871,6.4735798835754395,-5.609875202178955,3.0087568759918213,-6.068533420562744,-6.042380332946777,3.651254415512085,2.4491453170776367,-5.121857643127441,-6.827428340911865,-6.040585994720459,-0.4251876175403595,-7.422171115875244,-5.272737503051758,-3.6846275329589844,-1.3315974473953247,-0.5362968444824219,-0.07358154654502869,-0.4819353520870209,-0.4891453683376312,-3.821875810623169,-6.89161491394043,-5.687523365020752,2.7072319984436035,-4.03495454788208,0.9365504384040833,6.432574272155762,6.921207427978516,-6.630093097686768,9.677518844604492,-6.243706703186035,2.3689301013946533,5.566280364990234,5.8872857093811035,0.36340340971946716,2.4348087310791016,-1.456254005432129,-4.267198085784912,-1.6514695882797241,-4.221943378448486,-0.03684418648481369,-0.3923913538455963,-4.399753570556641,5.4416680335998535,-1.499167561531067,-5.968083381652832,-6.365645885467529,-9.89564037322998,1.4721876382827759,-1.6319547891616821,-0.5471674799919128,-1.4783861637115479,-0.32095998525619507,3.692455530166626,-6.265848159790039,1.9554709196090698,-0.5260757803916931,1.9554288387298584,-4.9563822746276855,2.9975743293762207,-1.0438451766967773,-2.2956454753875732,-6.266070365905762,-6.869684219360352,-6.253746032714844,-5.019289016723633,-0.5013812780380249,2.4729249477386475,-3.3730618953704834,-3.372921943664551,-3.3730413913726807,-5.801276683807373,-4.652264595031738,-5.60634183883667,-2.5092012882232666,-2.5088770389556885,-5.573240280151367,-1.6292413473129272,-1.6444380283355713,-4.97390604019165,-6.204771995544434,0.6178871989250183,2.7026302814483643,-6.214253902435303,-2.6278655529022217,-6.9027628898620605,-0.651767373085022,-1.3320978879928589,4.226991653442383,4.575541973114014,-4.391407012939453,-5.1046953201293945,7.165223598480225,-5.615540981292725,2.3074629306793213,-5.660205841064453,-0.437829852104187,-1.3801418542861938,4.7887067794799805,4.789321422576904,-5.029552936553955,-5.900411605834961,1.35114586353302,9.715519905090332,-1.625610589981079,-7.983463287353516,-9.018104553222656,-5.185885429382324,-0.9656150937080383,-0.6616218090057373,4.57549524307251,-3.5170507431030273,5.271340847015381,-5.602566719055176,-5.619929790496826,-0.5535823106765747,-0.4379042983055115,5.5850605964660645,-6.321771621704102,-4.673220157623291,-1.0296566486358643,-5.6025567054748535,9.691965103149414,-0.8469439744949341,4.227606773376465,-5.568519115447998,-0.6514805555343628,-2.3609182834625244,-5.196210861206055,3.4360451698303223,1.3733723163604736,3.0086300373077393,-4.617738723754883,-0.7694284915924072,9.795868873596191,10.063514709472656,-5.197309970855713,-5.940273761749268,-3.6932716369628906,-5.475113868713379,7.541675090789795,-4.674094200134277,1.3636102676391602,-1.605231761932373,-6.648116588592529,-10.615880012512207,-1.5003468990325928,7.586413860321045,3.4385271072387695,1.3734396696090698,-6.043787956237793,-5.592854022979736,-5.113495826721191,-4.24842643737793,0.595958411693573,-5.968441486358643,6.527675151824951,-5.5821356773376465,-1.6250994205474854,3.3901844024658203,-1.0008635520935059,-5.349470138549805,9.806985855102539,9.79872989654541,7.234668731689453,7.528839111328125,9.692415237426758,-1.626621961593628,-1.6211076974868774,0.8080571293830872,0.46305984258651733,9.999222755432129,-0.5219988822937012,2.2795379161834717,-6.684640407562256,11.133829116821289,-0.4378225803375244,9.739537239074707,-1.3257752656936646,-1.4820688962936401,-4.661998271942139,-4.246107578277588,-3.201418161392212,4.6708855628967285,2.308896780014038,1.8298497200012207,1.3733888864517212,6.999828815460205,-1.3518580198287964,0.9026276469230652,0.6770569086074829,-0.8874284625053406,-5.879744052886963,-1.4791687726974487,2.334545373916626,0.5657493472099304,5.267530918121338,-5.858070373535156,-6.219016075134277,7.253607273101807,-6.536309242248535,-11.015077590942383,0.8754569888114929,3.436596393585205,1.5304120779037476,1.531256079673767,-1.057296633720398,7.546258449554443,2.687751293182373,-0.4548868238925934,2.687535285949707,-1.2417691946029663,-0.3600979149341583,-1.5189324617385864,2.3078672885894775,-4.673799991607666,-3.373100519180298,10.088955879211426,3.083163261413574,-1.1091594696044922,-1.479364037513733,-0.43792492151260376,2.303063154220581,3.6614532470703125,-2.96370792388916,1.3018715381622314,-3.1080193519592285,1.3636116981506348,-0.4165017604827881,-1.6351842880249023,3.028132200241089,-4.7963080406188965,-5.533130645751953,1.3717905282974243,-3.1593809127807617,4.006701946258545,-6.324934959411621,7.0695672035217285,0.8105541467666626,0.5991188883781433,4.642282962799072,-1.8132511377334595,2.777878761291504,-2.3608877658843994,-1.4918370246887207,-1.5183225870132446,-4.815849781036377,-3.9256339073181152,1.6072241067886353,2.161341428756714,-0.76772141456604,-4.109409332275391,-1.4828784465789795,1.373496651649475,-5.834375858306885,2.269641876220703,0.6470792889595032,5.516635417938232,-1.026968240737915,-1.3089696168899536,1.9429600238800049,4.128035068511963,-4.250453472137451,1.845549464225769,-1.3422892093658447,7.268920421600342,0.39529913663864136,7.238152980804443,7.250786781311035,-1.4362521171569824,7.068717956542969,-3.5236008167266846,2.269754409790039,-3.2032718658447266,-5.570466995239258,7.1347737312316895,7.5574564933776855,7.542844295501709,-6.128664016723633,-1.3215291500091553,-1.3190557956695557,1.2943111658096313,-10.53107738494873,3.61958646774292,-5.535374641418457,1.2900118827819824,1.6269981861114502,-0.6204874515533447,1.267767310142517,-4.392578125,-0.5314587950706482,-4.664381980895996,7.3241448402404785,3.4831576347351074,-0.8281789422035217,-4.284741401672363,9.999298095703125,5.669618129730225,7.294940948486328,7.340592861175537,7.287683963775635,-0.5794786810874939,-5.038330554962158,7.075983047485352,1.8514325618743896,7.367335796356201,9.671893119812012,2.782799482345581,6.476317882537842,1.4218387603759766,-1.126294493675232,1.8513975143432617,-1.8656708002090454,4.299892902374268,-5.410780429840088,-6.100257873535156,0.8792839050292969,2.9951648712158203,-5.439828395843506,-3.9555413722991943,-4.663811683654785,3.80017352104187,4.0345778465271,-5.201763153076172,-3.722930431365967,-0.06467358767986298,2.995182514190674,2.9951682090759277,-1.6112374067306519,-3.823178768157959,-0.5207009315490723,3.036862373352051,5.2714762687683105,1.6372783184051514,7.367461681365967,7.367485046386719,-1.3483506441116333,2.1049158573150635,-0.21288752555847168,-0.5500401258468628,-5.886397361755371,6.146888732910156,-0.0750400498509407,-0.023488394916057587,-0.06242742761969566,-0.11392702907323837,-0.11726153641939163,-3.7459557056427,-0.3560434877872467,-3.8587729930877686,-1.8636665344238281,-0.4309123158454895,13.37165641784668,-3.9680166244506836,-9.048371315002441,-5.03407621383667,1.8229655027389526,-0.10906405001878738,0.32647472620010376,-0.32872191071510315,2.782653331756592,-5.777344226837158,-5.369592189788818,-4.386366844177246,-6.117110729217529,-3.9555559158325195,-0.11783293634653091,-9.966684341430664,1.4218645095825195,3.5845112800598145,9.99886703491211,0.8878337144851685,-5.024503231048584,-0.10289238393306732,7.078841209411621,-0.654914379119873,0.5522618293762207,1.3700995445251465,-4.299363613128662,-0.1255638748407364,-0.46270036697387695,-5.039040565490723,-0.8847730755805969,-0.026582151651382446,-5.342494010925293,1.672094702720642,1.5899103879928589,-3.9555459022521973,7.185770034790039,1.9916576147079468,9.587626457214355,0.8438929319381714,-7.983444690704346,-5.149114608764648,-6.006453037261963,9.580177307128906,2.245992660522461,-5.5537614822387695,-7.9834723472595215,-5.4374542236328125,-6.22981071472168,-6.226950168609619,-4.6964545249938965,-6.44491720199585,-4.820972919464111,-5.878291606903076,-0.42852872610092163,4.626021862030029,2.7379229068756104,-6.29615592956543,-4.994715690612793,-4.626682281494141,7.3671464920043945,-1.494327187538147,1.914287805557251,-6.6452956199646,-3.07147216796875,-0.5166676640510559,-4.3288092613220215,-6.240735054016113,-6.25835657119751,2.326342821121216,-7.522779941558838,-6.669555187225342,-1.3904286623001099,-5.601163864135742,0.577727198600769,7.183596611022949,7.173842906951904,-1.576774001121521,-1.5191991329193115,1.6503057479858398,-0.5181239247322083,-10.294677734375,-0.43458205461502075,-0.3540478050708771,2.311159610748291,-4.541199684143066,-5.694586753845215,7.366990089416504,-0.25299137830734253,1.9452707767486572,-5.647654056549072,-0.43143215775489807,-4.328804016113281,-4.328947067260742,-4.65043830871582,-5.736057281494141,2.5851333141326904,-0.12641149759292603,-0.12747259438037872,-1.7418464422225952,-5.1212615966796875,-4.662963390350342,-10.620708465576172,-1.003481388092041,-0.4548409581184387,-0.3451696038246155,4.270304203033447,-0.36419203877449036,-1.9515256881713867,-6.475834846496582,7.322307586669922,-4.822089672088623,-6.4926276206970215,3.6726160049438477,-3.9833641052246094,-0.33448895812034607,-6.558535099029541,-8.99724292755127,6.732375621795654,-0.3479677140712738,-5.595606803894043,-4.691979885101318,-5.273364067077637,-5.234653949737549,-6.60691499710083,-4.264893054962158,-10.615952491760254,-5.588778972625732,7.084981441497803,0.30087682604789734,-6.8848700523376465,-6.885077476501465,0.3008790612220764,-5.599924087524414,0.3008604347705841,-9.19325065612793,-5.694581031799316,-7.425580978393555,-0.6579931974411011,-5.213912010192871,-5.607054710388184,-0.5621312260627747,-6.6089582443237305,-4.686095714569092,-5.322688102722168,-0.18684430420398712,7.111711502075195,-0.5972157120704651,-6.302812099456787,2.7704639434814453,-0.3987838327884674,-6.482556343078613,10.049919128417969,11.12944221496582,10.045433044433594,-0.44677963852882385,-0.9710174798965454,7.187328338623047,-6.28022575378418,4.581707954406738,2.6875128746032715,-5.050856590270996,-6.788433074951172,-4.69217586517334,-6.16401481628418,-2.2935569286346436,-1.414796233177185,-1.3473567962646484,-1.1794663667678833,-0.5484491586685181,-0.3605082929134369,-1.3846371173858643,-2.0449378490448,2.9753079414367676,0.464469850063324,0.34621402621269226,-0.9646158218383789,7.662516117095947,-6.424734592437744,-6.355594635009766,-0.3330252766609192,-0.41515836119651794,-4.768406391143799,-4.615843772888184,-3.6509130001068115,-3.650916337966919,10.15701675415039,4.237085342407227,4.237034797668457,0.5776275992393494,2.3079018592834473,6.78490686416626,-0.963355541229248,-0.6514626145362854,-0.6514919400215149,-4.677247524261475,4.283831596374512,7.101665019989014,1.5071476697921753,-1.6236510276794434,5.584665298461914,0.5776355862617493,-3.532909393310547,6.924319267272949,-5.315095901489258,6.7815260887146,-6.008603096008301,-5.9969024658203125,7.08426570892334,-0.4527391493320465,-2.299997091293335,3.460090160369873,4.283884048461914,-0.5484214425086975,-2.249669313430786,1.6835246086120605,-0.36142227053642273,-0.7075842022895813,-0.3819192051887512,7.265514373779297,-6.163995265960693,-0.7285470962524414,7.231442928314209,7.428728103637695,-1.0400429964065552,-5.010748386383057,1.1367413997650146,-0.6951596736907959,-11.01345443725586,-1.5735286474227905,7.290572643280029],"type":"scatter3d"},{"hovertemplate":"label=quality_6\u003cbr\u003eproj0=%{x}\u003cbr\u003eproj1=%{y}\u003cbr\u003eproj2=%{z}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"quality_6","marker":{"color":"#EF553B","symbol":"circle"},"mode":"markers","name":"quality_6","scene":"scene","showlegend":true,"x":[-6.78790807723999,10.963234901428223,-4.184564113616943,2.2268078327178955,-1.1599386930465698,1.9322189092636108,5.873270511627197,-1.0961946249008179,5.108872413635254,9.513063430786133,0.9967580437660217,0.9967982769012451,-0.9288379549980164,2.5551199913024902,3.3255913257598877,-1.574608564376831,1.8866115808486938,7.666466236114502,11.488555908203125,6.222554683685303,5.077199459075928,5.813308238983154,5.074446678161621,1.9932475090026855,4.885765075683594,4.812869548797607,5.7795610427856445,-1.0515080690383911,-2.428595781326294,-5.687584400177002,0.9968666434288025,-1.3343334197998047,6.070701599121094,2.6149520874023438,5.727575778961182,2.5038578510284424,0.785834014415741,3.4402244091033936,2.6126174926757812,5.077949047088623,2.43605637550354,5.711578845977783,0.7858185768127441,2.414062023162842,5.594267845153809,2.554826259613037,3.3758492469787598,-6.673862934112549,3.9610648155212402,-1.3554778099060059,2.3742990493774414,5.264389514923096,2.2489852905273438,9.480231285095215,3.524198055267334,-9.716479301452637,-1.6840635538101196,0.8059312105178833,0.805966317653656,4.075037002563477,-0.9288109540939331,7.54119873046875,3.517941951751709,4.9419145584106445,2.9515726566314697,7.541184902191162,4.784807205200195,2.6317477226257324,4.753646373748779,4.440993309020996,2.2294955253601074,11.655829429626465,2.595496654510498,4.1276535987854,2.7596781253814697,3.8210480213165283,4.573762893676758,-1.502692461013794,-1.5027308464050293,2.527663469314575,0.3799028992652893,8.347358703613281,2.297241687774658,-1.4926780462265015,11.63422966003418,4.763652801513672,4.00963020324707,4.162628173828125,0.9911815524101257,1.8660073280334473,5.231420516967773,5.538641929626465,4.737541198730469,4.0811285972595215,3.9289608001708984,4.1734418869018555,4.8754472732543945,6.95734977722168,4.875378131866455,6.957271099090576,5.129428386688232,1.7132514715194702,4.09383487701416,3.2596161365509033,4.518656253814697,8.180115699768066,2.7848012447357178,4.135554313659668,5.238361358642578,3.493779182434082,2.9672160148620605,8.180129051208496,4.165029525756836,-5.041144371032715,-9.686038970947266,-1.553773045539856,-9.708683013916016,3.918076276779175,4.907683372497559,-3.701720952987671,3.894666910171509,4.386332035064697,4.7767109870910645,4.206075191497803,8.348814010620117,4.135857105255127,4.064001083374023,1.8339025974273682,5.041342735290527,3.832726240158081,3.783677577972412,4.768136978149414,5.8016886711120605,3.863933563232422,3.9291350841522217,4.975133419036865,5.063961505889893,3.848097324371338,10.963878631591797,2.9159226417541504,2.3092598915100098,4.476057529449463,4.9918622970581055,3.3034708499908447,4.178460597991943,5.026162624359131,4.448296546936035,4.77941370010376,4.77329158782959,4.786260604858398,3.655784845352173,6.472789764404297,4.950057029724121,-1.4635628461837769,-1.4553636312484741,-1.6974942684173584,0.6114115715026855,3.908447742462158,11.638935089111328,4.733597755432129,3.8622891902923584,3.534841299057007,5.068095684051514,10.148873329162598,1.9924254417419434,5.168975830078125,-4.276148319244385,11.636080741882324,-4.247378826141357,4.735080718994141,11.659599304199219,4.215117931365967,9.546014785766602,5.0031418800354,4.453711986541748,-1.5026310682296753,0.9967256188392639,4.877710342407227,4.878976821899414,3.199087142944336,3.8765149116516113,1.993991494178772,-1.5025280714035034,-1.5027680397033691,2.16827130317688,5.171757221221924,4.8900628089904785,4.454616069793701,-6.788125038146973,0.6113362908363342,-2.7587170600891113,-3.635359048843384,3.831092357635498,5.07347297668457,-3.634575366973877,-3.6343116760253906,-6.651800632476807,5.076142311096191,5.846438407897949,5.971492767333984,2.0846011638641357,2.773556709289551,0.48998865485191345,5.615985870361328,3.297250747680664,5.148490905761719,2.5998446941375732,2.7561559677124023,4.811215877532959,-4.081122398376465,-5.05056095123291,-5.051485061645508,4.610912799835205,-1.5026626586914062,4.544036865234375,4.571311950683594,5.0722551345825195,6.243420124053955,3.028554916381836,4.032290935516357,6.5760602951049805,3.0264225006103516,2.761125326156616,4.887109756469727,7.259586811065674,-5.0149054527282715,1.5313284397125244,-5.040861129760742,-4.246596336364746,9.546281814575195,-4.192440986633301,9.546308517456055,10.964312553405762,-1.1997690200805664,1.3493695259094238,-1.0884517431259155,-4.982671737670898,-4.982643127441406,2.922861099243164,4.623265743255615,4.63071870803833,5.1193084716796875,5.119275093078613,-1.5241369009017944,11.638540267944336,1.9937548637390137,2.4453234672546387,-4.108386516571045,5.687164783477783,-6.625116348266602,5.112067222595215,5.073062896728516,3.9171457290649414,4.228776454925537,11.638171195983887,3.5513370037078857,2.2043356895446777,5.060293674468994,-3.0557806491851807,-5.053957462310791,7.547621726989746,3.366875648498535,5.7550368309021,3.381831407546997,3.348220109939575,2.447706699371338,2.451314926147461,5.97861909866333,2.4643032550811768,2.1006693840026855,1.7205029726028442,2.455535411834717,4.881668567657471,2.448300838470459,2.4452409744262695,1.741694688796997,2.2288551330566406,2.555044412612915,-3.267810106277466,-4.0290703773498535,3.538740634918213,1.7797348499298096,5.853826522827148,2.9603519439697266,1.9635072946548462,1.347575306892395,2.3980069160461426,1.674423098564148,2.460922956466675,2.4264445304870605,2.357588768005371,-3.2198150157928467,-3.113011121749878,2.8001298904418945,-4.316566467285156,5.008038520812988,2.011188268661499,6.1477131843566895,2.238666534423828,10.393340110778809,3.9136569499969482,2.7972092628479004,6.394755840301514,1.7320154905319214,10.393380165100098,2.7729642391204834,-2.0903799533843994,4.692516803741455,-2.446197509765625,4.037800312042236,-3.101801872253418,1.713394045829773,2.4741063117980957,5.932586669921875,2.7840638160705566,6.385223865509033,3.2320687770843506,2.6882290840148926,-6.736557483673096,2.4499690532684326,2.5325725078582764,2.036740303039551,2.0362863540649414,-6.673149585723877,3.8188247680664062,2.48254132270813,3.794700860977173,2.2455174922943115,5.09929895401001,2.4133408069610596,5.849662780761719,3.7125954627990723,-4.134646892547607,2.401491641998291,1.647547721862793,2.4749302864074707,2.488434314727783,2.753840208053589,1.8530163764953613,-4.315828800201416,2.5973494052886963,1.7246662378311157,2.61138916015625,-3.1083364486694336,5.062903881072998,4.526995658874512,2.4685158729553223,8.177968978881836,3.0401172637939453,2.2991414070129395,2.044945240020752,1.8962509632110596,1.8986331224441528,4.870761394500732,2.184816837310791,1.4961299896240234,5.029215335845947,4.672886371612549,2.499821901321411,4.629920959472656,-9.72066593170166,2.4217581748962402,2.943934440612793,1.858210802078247,2.5316295623779297,1.3590608835220337,4.623988628387451,2.475126028060913,-4.1630964279174805,1.988040804862976,4.8293280601501465,1.434374213218689,2.6054046154022217,4.464787006378174,2.438927173614502,2.5517818927764893,4.6752400398254395,2.4320850372314453,3.53642201423645,2.3718972206115723,1.4520695209503174,2.8961708545684814,7.547092437744141,2.4398202896118164,-4.061084270477295,-6.477216720581055,2.4355695247650146,0.9909939169883728,5.064204216003418,-1.6779910326004028,2.583230972290039,4.5808796882629395,10.393394470214844,2.3821263313293457,-9.725183486938477,2.640206813812256,2.8809475898742676,-3.0666651725769043,4.235475540161133,5.717835426330566,3.5499634742736816,4.124508857727051,1.4559205770492554,1.7344764471054077,4.527667045593262,-4.276595592498779,9.486618995666504,2.2976901531219482,2.443465232849121,8.10011100769043,1.982190728187561,1.6430063247680664,2.7786688804626465,4.714608669281006,2.4103524684906006,6.085062503814697,1.9643549919128418,-2.0903897285461426,-6.648471355438232,-4.899484634399414,2.239987373352051,2.2862887382507324,5.964846611022949,2.5425047874450684,2.4468746185302734,1.9957749843597412,1.9599820375442505,1.7158924341201782,1.8437601327896118,1.4628362655639648,-3.36372447013855,2.0282366275787354,5.996964454650879,5.99819278717041,2.679030656814575,6.245484352111816,-3.311126708984375,9.515835762023926,6.259949207305908,0.9124978184700012,2.6820292472839355,-6.883724689483643,1.8452996015548706,3.4419641494750977,3.440453290939331,5.056063652038574,1.9167841672897339,2.181837320327759,4.413744926452637,2.7504117488861084,2.596472978591919,-6.678013801574707,4.560291290283203,2.0449228286743164,7.259557723999023,2.7669622898101807,1.9394948482513428,5.047963619232178,2.3662455081939697,1.5619847774505615,5.732703685760498,2.8834116458892822,2.1388299465179443,1.344928503036499,5.636519432067871,2.0048487186431885,8.180153846740723,1.6793147325515747,0.36400580406188965,1.805034875869751,10.99806022644043,2.499601364135742,-5.332277774810791,2.4949328899383545,-5.33241081237793,3.9452292919158936,10.51331901550293,1.0904390811920166,2.1764187812805176,2.6049582958221436,2.5342016220092773,-2.3812005519866943,1.8145270347595215,2.555053949356079,2.099527359008789,1.738863468170166,10.513470649719238,5.579487323760986,2.5084455013275146,10.513574600219727,-2.381181001663208,2.5351390838623047,10.513595581054688,1.7657015323638916,-5.067933082580566,-5.064563274383545,2.116840362548828,2.560551166534424,5.4414262771606445,2.7533257007598877,1.9973585605621338,2.426017999649048,1.8369508981704712,5.443201541900635,2.700896978378296,2.1940202713012695,2.1991958618164062,1.8568189144134521,2.5754034519195557,4.714050769805908,0.9111214876174927,1.4580801725387573,10.513565063476562,2.0471644401550293,-2.381103277206421,1.7264246940612793,2.107708692550659,1.8385035991668701,1.966499924659729,12.48444652557373,2.634342908859253,-2.3811964988708496,1.874086618423462,2.253209352493286,2.438735246658325,2.620056629180908,2.6312978267669678,2.1983609199523926,6.147620677947998,3.374544858932495,1.8558781147003174,2.1524202823638916,-6.1662516593933105,2.1607351303100586,7.666304588317871,-3.422410249710083,-1.8806463479995728,0.8059952259063721,0.34083446860313416,-0.5453695058822632,0.39543238282203674,2.2213778495788574,-2.1085309982299805,6.734333515167236,-0.4373910427093506,7.60120964050293,4.746456623077393,10.060285568237305,-1.0998404026031494,3.856804132461548,-1.1311007738113403,4.488337993621826,-1.2250940799713135,13.334789276123047,13.334810256958008,-2.943333387374878,-1.833808183670044,-1.7916315793991089,6.213472366333008,3.4210739135742188,1.4035030603408813,0.8731493353843689,5.2423858642578125,8.391400337219238,2.1565957069396973,-4.17314338684082,-9.222335815429688,3.0763168334960938,2.2058942317962646,-2.744077682495117,4.535663604736328,7.3583478927612305,6.25894832611084,5.402466297149658,5.403486728668213,-6.646879196166992,-7.290894985198975,-3.2303378582000732,0.5303427577018738,7.358565330505371,0.34171363711357117,6.430150508880615,3.1844165325164795,0.738559365272522,-1.784972071647644,0.11201778799295425,1.8522850275039673,0.2754899561405182,-0.19268977642059326,4.566291332244873,0.8319665789604187,-2.325388193130493,-0.9504784345626831,1.6263189315795898,0.6431471705436707,-3.9072868824005127,-6.761488914489746,4.870474338531494,5.841573715209961,-1.1135622262954712,-2.7081351280212402,-4.85535192489624,-0.5373130440711975,-0.5342124104499817,-4.404808044433594,-3.9756782054901123,-6.948667049407959,-2.9051835536956787,14.578981399536133,5.993248462677002,5.2916364669799805,0.24776259064674377,2.809506893157959,-4.098771572113037,6.294911861419678,5.1444807052612305,-7.348274230957031,6.061309337615967,6.122218132019043,4.865041732788086,4.878121376037598,4.1710710525512695,-0.10094963759183884,5.301285743713379,-1.9878153800964355,5.962829113006592,5.017578125,-4.404781818389893,0.8738008141517639,5.518604755401611,2.4795193672180176,2.479433536529541,-3.9576523303985596,-3.9569435119628906,9.482719421386719,0.17534656822681427,1.2544666528701782,-3.8874869346618652,0.6200312376022339,1.0901992321014404,-2.6777987480163574,0.0902344286441803,0.29290908575057983,0.2864862382411957,2.448626756668091,-1.887582540512085,2.053281545639038,1.402502179145813,-4.295406818389893,6.805709362030029,0.8614986538887024,-7.368648529052734,-2.8175182342529297,9.480541229248047,5.227046489715576,5.2075886726379395,8.397000312805176,9.361870765686035,0.467439740896225,-3.0679304599761963,0.6154972314834595,7.97758674621582,-3.985395669937134,-2.3498895168304443,7.54074239730835,4.87089729309082,5.290056228637695,-3.1612284183502197,7.046484470367432,9.485587120056152,0.25908195972442627,4.282700061798096,-1.2838714122772217,-5.599429607391357,-4.103370666503906,5.239956378936768,7.458853721618652,6.6136040687561035,4.927591800689697,2.466174602508545,2.6736819744110107,6.288463115692139,5.657642841339111,3.2503414154052734,4.9910430908203125,-3.4268646240234375,-1.799121618270874,6.08421516418457,8.27237319946289,6.463542938232422,6.461625576019287,-2.812072277069092,4.887574672698975,-1.5740433931350708,4.078746795654297,4.120394229888916,5.315099239349365,2.586179494857788,0.4969216585159302,10.295303344726562,-0.6554542183876038,10.65942668914795,3.062328338623047,3.011042356491089,-0.015919160097837448,2.009101152420044,4.48119592666626,4.473685264587402,-2.555208921432495,7.371433734893799,5.478641986846924,-1.6130942106246948,0.5358303785324097,4.302290439605713,5.375028133392334,-0.8993068337440491,5.350308418273926,6.228374004364014,-0.7875161170959473,-6.294941425323486,6.738571643829346,-2.051975727081299,-3.530693292617798,-2.0945727825164795,5.530352592468262,-1.4096012115478516,-3.0842535495758057,0.09294639527797699,4.836390495300293,-1.7137632369995117,-1.3664093017578125,-9.261220932006836,-2.49102783203125,-0.7898790240287781,6.270337104797363,-0.5115159749984741,4.867509841918945,-1.7267651557922363,5.253424167633057,4.710771083831787,7.422335624694824,4.901247024536133,-2.3250629901885986,-0.19091613590717316,6.520397186279297,-0.407542884349823,6.6104207038879395,-1.1370586156845093,0.8055934309959412,12.074313163757324,-0.5302006006240845,4.221042633056641,4.194939613342285,0.27860063314437866,-0.036651402711868286,5.476579666137695,-1.7385141849517822,4.392313480377197,-4.282336235046387,-1.4695583581924438,-2.7707738876342773,-5.599062442779541,-0.8373810052871704,-1.6292299032211304,4.140518665313721,4.153103351593018,-4.100076198577881,-1.5980511903762817,6.467937469482422,-1.2962828874588013,5.382262706756592,0.1306888312101364,0.13934564590454102,9.486668586730957,0.5240229368209839,8.38500690460205,-2.7775678634643555,0.07608899474143982,6.315063953399658,5.293593406677246,-3.987074613571167,6.615446090698242,5.891471862792969,4.895905494689941,-1.6822760105133057,7.455972671508789,0.4088025689125061,-7.372904300689697,-0.6719599366188049,2.7763071060180664,5.209078311920166,-2.493873357772827,-2.6371474266052246,5.908819198608398,-4.013833045959473,-2.452824354171753,-3.092512607574463,6.611067295074463,6.094928741455078,4.354434013366699,8.991747856140137,4.53879451751709,-1.7051074504852295,4.818786144256592,4.9039411544799805,-2.2894794940948486,6.454814434051514,-4.935671806335449,6.338413238525391,-5.004933834075928,-3.9114937782287598,7.983150959014893,-1.8749669790267944,6.339003562927246,5.0561723709106445,-0.5264929533004761,-2.7622411251068115,5.185667991638184,6.635915756225586,-2.3239693641662598,6.60690975189209,9.847403526306152,9.847415924072266,6.632944583892822,-1.7987556457519531,10.705609321594238,1.2715373039245605,0.2552543878555298,1.3230096101760864,1.323005199432373,1.3230135440826416,1.0900799036026,12.619097709655762,4.563806056976318,2.809389352798462,0.26542720198631287,6.359080791473389,4.731661319732666,1.2108426094055176,-0.7910233736038208,5.626440048217773,0.3990291655063629,5.335155010223389,-2.4928009510040283,4.745290756225586,-1.0984892845153809,1.5797392129898071,1.4049224853515625,0.6199633479118347,5.543660640716553,13.640459060668945,6.355831146240234,0.39912790060043335,5.116429328918457,-0.4381726384162903,-1.5734913349151611,-1.5047610998153687,-0.4381493031978607,9.846793174743652,-2.0063321590423584,-2.801732063293457,-1.7129946947097778,1.2488298416137695,-6.64115047454834,-6.643482685089111,6.392019271850586,-7.503418922424316,-1.224602222442627,5.024632453918457,4.48612117767334,5.790163993835449,0.40856435894966125,-4.227323532104492,-4.078201770782471,4.222581386566162,0.1971973478794098,1.6457958221435547,-2.588489294052124,5.953578948974609,6.281534194946289,0.244890958070755,0.2663405239582062,-0.5169099569320679,9.362814903259277,-3.1684465408325195,5.543281078338623,4.8008880615234375,1.3913859128952026,8.579611778259277,4.868218898773193,0.20000062882900238,-2.7008700370788574,1.001589298248291,-0.2198508381843567,-0.6398382782936096,-3.6895506381988525,7.999490261077881,4.433468818664551,4.4075798988342285,6.060464382171631,6.389462471008301,7.831523418426514,4.408242225646973,-1.8176120519638062,1.2667479515075684,-4.466604709625244,4.374960899353027,0.37025317549705505,4.378089427947998,5.7957763671875,8.050535202026367,6.467529773712158,10.09737491607666,4.466548442840576,6.5651116371154785,4.520359039306641,7.193132400512695,5.558724880218506,5.549871921539307,-1.0893641710281372,2.7486400604248047,4.27302360534668,5.186201572418213,-2.448883533477783,5.794991970062256,-4.0735931396484375,-1.7619813680648804,4.114841461181641,-2.6738975048065186,5.400978088378906,6.14686393737793,9.846427917480469,3.4559056758880615,-0.5742912888526917,-2.707059144973755,4.865854740142822,-1.9310344457626343,2.788883686065674,13.6405611038208,6.237063884735107,0.6349360942840576,2.525735855102539,-4.210147857666016,0.4059385061264038,6.389514446258545,4.859894275665283,3.1678497791290283,6.074294090270996,4.859189987182617,6.420776844024658,-2.4502694606781006,0.2643209397792816,6.3684492111206055,9.68358039855957,11.741132736206055,-1.53000009059906,2.6000518798828125,1.5048038959503174,4.828460693359375,10.165721893310547,7.68619441986084,0.31495431065559387,3.1070923805236816,-2.1460695266723633,1.2220728397369385,8.015020370483398,7.862348556518555,-5.858728408813477,-0.20199985802173615,-0.20172376930713654,7.997057914733887,-0.35301604866981506,6.212594985961914,-1.85964035987854,6.885316848754883,4.919316291809082,2.9100241661071777,3.244685173034668,0.0939316526055336,-7.392072677612305,6.8069329261779785,10.165547370910645,8.638096809387207,-0.4736725687980652,0.33851930499076843,-5.0811591148376465,10.420554161071777,-4.402724742889404,5.13969087600708,2.425861120223999,11.590907096862793,-3.7610323429107666,-4.269380569458008,6.807953357696533,2.7676327228546143,-0.8659600019454956,9.674309730529785,4.961048603057861,1.2864432334899902,0.1428273618221283,4.473355770111084,0.9693440198898315,1.5036494731903076,7.491339683532715,-5.764508247375488,-4.198004245758057,4.272951602935791,7.299125671386719,11.590919494628906,0.17732593417167664,3.1994729042053223,-2.0962297916412354,4.634648323059082,8.329994201660156,8.330397605895996,2.9691271781921387,2.200284242630005,0.5538101196289062,8.330255508422852,-0.8056016564369202,-0.2651458978652954,2.035655975341797,5.598432540893555,10.711600303649902,-1.8268641233444214,4.874022006988525,-3.071155309677124,0.2269972562789917,-1.85430109500885,12.520020484924316,0.18480372428894043,4.744955062866211,5.6321282386779785,10.06081771850586,1.4141227006912231,1.403110146522522,-2.380708932876587,0.9126793742179871,12.41430377960205,5.601663112640381,10.060893058776855,-4.831571102142334,12.403316497802734,-0.986546516418457,12.407222747802734,4.854724407196045,-6.72189474105835,5.494010925292969,-2.059945821762085,2.9726383686065674,-4.190517425537109,-2.05631422996521,5.147519111633301,7.506103992462158,-2.701554536819458,5.069872856140137,2.772758960723877,1.2870604991912842,6.267889976501465,1.4743238687515259,-0.8474587202072144,-3.3010716438293457,1.2875003814697266,2.7658627033233643,10.407181739807129,-5.057869911193848,-2.729462146759033,-5.071324825286865,-5.067824363708496,-4.893213748931885,0.053590454161167145,6.060242176055908,-2.6951663494110107,-3.1618494987487793,-3.305678129196167,-5.599781513214111,5.4512553215026855,2.7749900817871094,-6.720941543579102,0.053398218005895615,0.6077021956443787,2.784306764602661,-2.7184548377990723,-0.9816175699234009,-5.774982929229736,-5.774997234344482,-0.9800782203674316,-0.9701119661331177,1.2719072103500366,5.841556072235107,12.393084526062012,0.8666213154792786,4.859740734100342,4.881375789642334,7.497680187225342,5.578436374664307,-4.894264221191406,1.2872228622436523,7.967239856719971,2.885056257247925,4.843164443969727,8.118009567260742,-3.431724786758423,7.508650302886963,8.118490219116211,1.6642036437988281,1.5252588987350464,-3.301659345626831,-1.0762662887573242,12.528499603271484,7.632570266723633,1.2495073080062866,5.596867084503174,5.61954402923584,-4.472334384918213,5.116086006164551,4.840095520019531,6.09194803237915,0.04978897422552109,-0.0780148133635521,6.602712631225586,8.05389404296875,0.05573909729719162,4.833230972290039,1.2870748043060303,-3.4437921047210693,-0.8770986199378967,1.0911808013916016,-3.3980743885040283,-0.4999622106552124,-2.191521406173706,-1.0761041641235352,7.84733247756958,-0.22974573075771332,0.2563713788986206,4.8678693771362305,4.384000301361084,4.103946208953857,0.3149406611919403,-4.277405738830566,4.621480941772461,4.100284099578857,6.072968482971191,5.239346504211426,6.068644046783447,4.900639533996582,9.36274528503418,5.530059337615967,5.409449100494385,-2.2026121616363525,5.978240966796875,3.309141159057617,0.2276960015296936,10.097061157226562,-1.7928394079208374,6.084140777587891,5.58665132522583,-1.8898310661315918,6.087178707122803,-2.943373441696167,0.9081912636756897,8.637859344482422,-4.472381591796875,-0.4345746636390686,-0.5604482889175415,6.016033172607422,1.6217200756072998,-7.360795497894287,-6.2974724769592285,3.5850608348846436,10.097077369689941,0.1958974152803421,3.9762818813323975,-5.781495094299316,-0.5078813433647156,-5.781521320343018,-1.1017476320266724,3.3091256618499756,-5.145411014556885,-3.37728214263916,9.362433433532715,-4.472415924072266,4.841668605804443,-2.32499623298645,0.5844427943229675,-3.340808153152466,-4.226131439208984,-2.1094603538513184,3.1823573112487793,-2.1089377403259277,-2.3552489280700684,0.0584343820810318,6.412673473358154,6.412356376647949,4.899293422698975,-9.219096183776855,5.612026214599609,5.936161041259766,-0.8673134446144104,-0.4028934836387634,4.106626987457275,5.947092533111572,7.261438846588135,5.947299003601074,5.327069282531738,-9.857059478759766,13.640605926513672,-1.5766615867614746,-3.940702199935913,0.22648070752620697,-0.7424025535583496,-2.6751790046691895,4.798890113830566,7.686807155609131,-7.335369110107422,-5.128515243530273,4.647637367248535,4.611741542816162,4.606441020965576,-1.5713297128677368,-0.4926920235157013,0.388998806476593,-1.4191566705703735,7.390762805938721,7.297581672668457,1.6666486263275146,1.6969820261001587,-3.1465306282043457,-7.31206750869751,-2.9316883087158203,-1.3455018997192383,-7.333542823791504,4.272988319396973,4.375685691833496,0.9032801389694214,6.556879043579102,4.841835021972656,0.0888674184679985,7.671763896942139,10.162776947021484,7.856057643890381,-1.0823204517364502,4.476759910583496,-0.9112964868545532,2.7756845951080322,9.362810134887695,-7.377140998840332,-0.28351402282714844,7.308790683746338,-3.340813398361206,7.317809104919434,4.383299350738525,7.374590873718262,4.84248161315918,10.061090469360352,4.210008144378662,-0.15401750802993774,5.555699348449707,1.608537197113037,-1.4160475730895996,-1.3096789121627808,-1.7400672435760498,3.2456507682800293,4.9533491134643555,-7.487483978271484,10.72414493560791,-6.2699408531188965,5.578426837921143,0.2083575278520584,3.2433769702911377,-0.44887739419937134,6.07987642288208,1.22670578956604,-0.5127595663070679,5.230175495147705,3.924868583679199,0.6295232772827148,-0.5742020606994629,-0.7178851962089539,0.1742028445005417,8.110557556152344,2.602491617202759,-3.6864919662475586,-7.392947196960449,5.981729984283447,2.4996535778045654,12.619423866271973,3.1295876502990723,5.296124458312988,-1.8125102519989014,-3.7299978733062744,0.9223698377609253,0.24615730345249176,-1.8207223415374756,-0.806242048740387,1.3502252101898193,4.903405666351318,-3.919039011001587,1.2725739479064941,5.493630886077881,-2.931689977645874,4.54143762588501,5.921080112457275,-2.5971200466156006,5.529766082763672,1.2224462032318115,6.768251895904541,5.4956135749816895,-1.8882348537445068,-2.233981132507324,-2.23073410987854,8.991744995117188,-1.5330049991607666,-0.812644898891449,-0.1840330809354782,1.1950701475143433,1.256868839263916,4.621375560760498,0.04971587657928467,10.215608596801758,-0.9507716298103333,0.8160140514373779,-2.4862542152404785,-2.7467620372772217,5.485751628875732,4.909884929656982,3.4067623615264893,6.596765995025635,0.18295225501060486,7.66609525680542,1.4280598163604736,-1.3316943645477295,-2.324082374572754,8.121261596679688,5.25902795791626,2.775970220565796,0.7583580017089844,5.472473621368408,5.143978595733643,-4.465961933135986,6.808470726013184,6.808629512786865,4.319583415985107,-4.221587657928467,-1.9651106595993042,-7.518655300140381,2.8203721046447754,-2.1917929649353027,-2.2109386920928955,5.486202239990234,5.633825302124023,1.0900062322616577,4.382943630218506,2.9691672325134277,5.6505889892578125,5.6056132316589355,1.2467678785324097,10.21824836730957,5.248865604400635,-6.267152309417725,3.026956558227539,-7.424688339233398,-0.8349147439002991,-4.713459491729736,8.272383689880371,-1.1357719898223877,4.875039100646973,5.469865322113037,-6.654245853424072,4.9168829917907715,4.100649356842041,-2.026384115219116,1.2421538829803467,4.843606472015381,5.405162334442139,-2.8195533752441406,-1.7893285751342773,-1.3807381391525269,1.793275237083435,6.807076454162598,-7.222087860107422,2.9532394409179688,-1.3652751445770264,10.180356979370117,6.4011759757995605,4.781083583831787,6.401330947875977,5.343612194061279,6.302947998046875,-7.23284912109375,7.504416465759277,4.7888264656066895,0.19405756890773773,8.112454414367676,4.221616268157959,4.638143539428711,4.863420009613037,7.376269817352295,4.89471435546875,-7.4018754959106445,-0.8981456160545349,-1.5335242748260498,5.037500381469727,-0.02380003221333027,4.718995094299316,1.3833060264587402,-2.205946683883667,4.113575458526611,7.540724277496338,-2.208179235458374,-7.237797260284424,1.7749806642532349,7.232774257659912,-3.6031041145324707,-3.17031192779541,-0.8003712892532349,-0.8809806108474731,-2.246396541595459,8.36142635345459,8.117864608764648,-3.1020431518554688,5.011623382568359,-0.522660493850708,4.79328727722168,4.890392303466797,4.745431423187256,4.6767377853393555,-1.4162302017211914,6.996179103851318,5.576086521148682,3.054530620574951,0.9339461326599121,-2.2086575031280518,-1.6662331819534302,-1.034342646598816,-2.2750020027160645,4.636927604675293,5.411852836608887,7.996270656585693,1.4034708738327026,6.011441707611084,0.7191990613937378,5.528299808502197,7.315871238708496,-2.1569454669952393,5.732561111450195,-7.298725605010986,-7.267110347747803,-2.943323850631714,6.392482280731201,-6.836312770843506,-0.9437113404273987,4.3763251304626465,4.183470726013184,2.715890884399414,-5.060623645782471,4.211167335510254,5.409497261047363,-2.943035364151001,6.3635382652282715,0.1001412495970726,2.9689083099365234,3.9536163806915283,-1.8690080642700195,-0.9799956679344177,-1.0795940160751343,-2.336085557937622,6.741117477416992,-0.5533305406570435,5.965445518493652,-4.100792407989502,-1.0901823043823242,-2.1795496940612793,5.089076995849609,-1.1245579719543457,5.8907976150512695,3.4559381008148193,3.4559848308563232,-7.457047462463379,0.29282402992248535,-2.2910706996917725,-7.302793502807617,-5.190483093261719,5.038245677947998,-4.90483283996582,5.323424339294434,9.677109718322754,-0.9433919191360474,-6.641532897949219,-6.036553859710693,6.123481750488281,-0.5745999217033386,-1.609147310256958,-3.026533842086792,-3.5583183765411377,4.0975565910339355,-0.8659935593605042,5.250762462615967,10.722103118896484,-1.8445401191711426,-1.8252992630004883,-2.1039185523986816,-5.027844429016113,-1.0614434480667114,-2.392449378967285,3.923159599304199,4.461126327514648,-3.4335551261901855,6.468019485473633,6.46812629699707,0.8893590569496155,5.7884087562561035,4.413030624389648,-5.89413595199585,4.856868267059326,0.2925097346305847,1.793140172958374,0.12403197586536407,0.2049022614955902,0.13540634512901306,5.201117515563965,0.05490082874894142,-1.0194751024246216,-1.3670254945755005,-6.836434841156006,0.6268636584281921,0.630510151386261,4.28912878036499,0.4945247173309326,4.904501914978027,0.035284169018268585,-2.6355857849121094,5.4011006355285645,4.375962257385254,-1.3965778350830078,-0.8672949075698853,-2.3361198902130127,-1.222007155418396,-0.35872137546539307,-3.931215524673462,-4.237930774688721,4.3488383293151855,4.705620765686035,5.179502487182617,5.163599967956543,-0.8665811419487,-2.748258590698242,-1.1272002458572388,6.188037872314453,5.8133673667907715,0.42327266931533813,-6.575504302978516,-2.4076731204986572,-1.3510234355926514,-5.8941264152526855,-4.472364902496338,-2.446181297302246,-0.8398454785346985,5.401598930358887,0.4095996022224426,-1.4046813249588013,0.8437677621841431,0.09279048442840576,-0.08007385581731796,-2.15826153755188,-3.4439103603363037,6.47651481628418,-1.1333446502685547,9.13115406036377,0.18027235567569733,-1.213586449623108,-0.6217530965805054,12.366720199584961,-1.2205467224121094,-2.639672040939331,-1.8364462852478027,-0.8492898941040039,-0.07091039419174194,-3.426290988922119,-0.47838690876960754,-2.7482874393463135,0.8558090925216675,-0.23177412152290344,-1.8269051313400269,0.4092220067977905,0.3574775755405426,4.796518325805664,7.351501941680908,-0.8537525534629822,1.3229973316192627,7.349263668060303,1.274796962738037,4.7190022468566895,10.54025650024414,-2.3614094257354736,-0.8577922582626343,-1.3858301639556885,-0.7788590788841248,9.683577537536621,0.5563371181488037,-0.6274499297142029,5.843959808349609,6.187480449676514,4.014492511749268,-0.018546462059020996,-1.6396725177764893,1.625312089920044,-5.59706449508667,9.683954238891602,0.49590060114860535,0.5499307513237,-5.597443580627441,-5.59913969039917,2.6406710147857666,-2.0123178958892822,-2.500422477722168,2.2057571411132812,-1.6826478242874146,3.6182093620300293,-0.6721535325050354,-1.0745517015457153,-0.7916680574417114,-2.463212490081787,5.614856719970703,3.4188597202301025,5.600512504577637,0.33928054571151733,1.413924217224121,-4.078213691711426,-0.9639424681663513,-0.8073056936264038,-2.2898595333099365,-2.0304031372070312,5.409268856048584,5.481388568878174,2.668297529220581,3.173968553543091,-0.5958792567253113,-1.2937378883361816,-1.4562104940414429,10.532153129577637,-0.21297378838062286,-1.8212248086929321,-5.075845718383789,5.997067451477051,5.433165550231934,-4.8875532150268555,-2.3492023944854736,-0.2668987810611725,5.524623870849609,-0.3039857745170593,-2.7665364742279053,-5.053652286529541,-0.21854375302791595,-2.626178026199341,6.635906219482422,3.5325276851654053,-0.5264765620231628,0.2387254685163498,1.9282026290893555,0.3380141854286194,1.49834144115448,0.3541637361049652,-3.0715315341949463,-1.070472240447998,6.864232540130615,-2.5367469787597656,12.373031616210938,3.015547275543213,-2.1605162620544434,-0.2938098907470703,5.888886451721191,-6.369531631469727,4.541687965393066,-3.292229652404785,-0.923482358455658,0.14375461637973785,5.494591236114502,-2.9320192337036133,-0.9775444865226746,-1.469173789024353,-3.7539262771606445,4.699448585510254,5.713208198547363,-1.3616907596588135,-3.470273017883301,-1.8335567712783813,-0.6426860094070435,-3.0413424968719482,-0.6328153610229492,-4.325101375579834,-4.325125217437744,-0.9398496150970459,0.2654631435871124,0.17750129103660583,10.564872741699219,-1.364989161491394,4.565531253814697,-3.4269471168518066,5.995880603790283,-2.2765889167785645,-6.369241237640381,-6.369271755218506,-1.1187714338302612,1.2361998558044434,5.706703186035156,-0.32436445355415344,0.9376981258392334,0.9547764658927917,-0.8759116530418396,-5.230254650115967,-8.156495094299316,-1.1529768705368042,-2.331787586212158,0.15452252328395844,0.15123900771141052,-1.0708870887756348,7.085816860198975,0.08567415177822113,7.077509880065918,-1.1851087808609009,-0.9474665522575378,-4.899110794067383,0.9391673803329468,-4.885830402374268,-5.997835159301758,-3.855973958969116,-3.033395767211914,3.052102565765381,5.1907572746276855,5.030784606933594,-6.709099292755127,10.570439338684082,-6.212307453155518,-2.6395089626312256,5.277278900146484,4.407558441162109,-1.8190934658050537,-3.1728947162628174,-1.818541169166565,-0.9446006417274475,5.403122425079346,-3.07169508934021,-0.041041187942028046,3.473158597946167,0.4206586182117462,-3.033430337905884,-2.380119562149048,-5.045642852783203,-0.0519595742225647,5.654197692871094,5.655236721038818,4.955515384674072,-4.183136463165283,5.61234188079834,-1.7269070148468018,-1.8509068489074707,-2.70939302444458,5.2383246421813965,3.3090381622314453,6.731853008270264,10.192822456359863,-6.616933345794678,-3.3303067684173584,4.191371440887451,0.9147431254386902,0.9301507472991943,-1.4930925369262695,-2.432344675064087,-5.050111770629883,-3.0714879035949707,-0.5609149932861328,-2.6137466430664062,-2.099543333053589,-2.209700107574463,-4.1882829666137695,0.4713396430015564,4.6768927574157715,4.761511325836182,4.780231475830078,-0.8831011056900024,0.12036815285682678,0.38149294257164,-2.9290225505828857,10.560123443603516,-1.006905198097229,-2.2652037143707275,-0.35511574149131775,-1.644835114479065,-2.3318052291870117,1.79302978515625,3.083397150039673,-0.6379517316818237,3.456094741821289,5.930812358856201,-5.198306560516357,5.615067481994629,0.5220454335212708,4.664797306060791,-2.3061482906341553,4.383791923522949,6.9568190574646,4.383997440338135,-1.9192042350769043,-3.4263691902160645,12.061545372009277,12.061537742614746,-2.906041383743286,-2.695117950439453,-0.7879216074943542,0.9795957207679749,5.804342746734619,-3.0702524185180664,-2.455883502960205,5.419989109039307,9.100835800170898,5.015995025634766,5.481543064117432,7.5859880447387695,5.563368320465088,6.120995044708252,0.13662934303283691,7.20643949508667,-1.828931450843811,5.280770778656006,-0.4624345898628235,11.370818138122559,-1.8287070989608765,-1.864240050315857,4.565373420715332,-0.8572537302970886,5.484054088592529,5.507211208343506,-1.8408312797546387,4.397489070892334,-1.0702325105667114,9.100837707519531,-5.3856940269470215,4.076415061950684,1.5515258312225342,6.0753631591796875,0.3708246350288391,-1.5223445892333984,-4.892751693725586,-6.272939205169678,-5.188834190368652,-5.326712131500244,0.04148881882429123,0.2508024275302887,0.3824874758720398,4.211787223815918,-3.3968091011047363,-3.261422872543335,-6.6796088218688965,0.5173953175544739,-1.447718620300293,-0.1376417875289917,2.5255658626556396,-0.8273505568504333,-1.4978828430175781,-3.306528329849243,3.2279813289642334,-1.0777146816253662,0.5561485886573792,-2.444465160369873,-1.4160274267196655,-3.298220634460449,-2.4437336921691895,-1.3603326082229614,1.6219549179077148,5.660505294799805,4.405350208282471,0.28891879320144653,9.676608085632324,0.04183013737201691,8.017509460449219,7.892595291137695,-1.8281317949295044,4.196393966674805,7.99204158782959,-5.311491966247559,0.38599467277526855,-2.086353302001953,2.6682960987091064,-5.235410213470459,-2.9438304901123047,-0.8583747744560242,-0.6479902863502502,7.0857038497924805,-3.3269636631011963,1.533446192741394,-1.2196402549743652,-2.268730640411377,1.2548707723617554,-1.206588864326477,-2.2854652404785156,-2.843513250350952,0.5121806263923645,-1.202134609222412,-2.3725314140319824,-1.3842965364456177,-1.6792653799057007,-2.438904285430908,-3.1238784790039062,-1.1503334045410156,10.552128791809082,-3.141916036605835,-2.3281428813934326,-0.7648833990097046,10.6266450881958,-6.660994529724121,10.614114761352539,4.782821178436279,-0.9403644800186157,1.5631892681121826,-1.3579624891281128,1.852307677268982,0.5848006010055542,0.5029206871986389,3.070234775543213,0.02509157732129097,-2.8928396701812744,-2.0122358798980713,-1.8695787191390991,-0.8331525325775146,7.009761810302734,0.12401853501796722,-6.653866291046143,-2.639789581298828,4.907032012939453,-2.63973069190979,-1.8206509351730347,0.05528785288333893,-1.8273165225982666,-0.44195669889450073,-3.042858600616455,5.529663562774658,7.201292991638184,-3.1892569065093994,-0.8591462969779968,-0.48580604791641235,-0.849761962890625,3.04671311378479,-5.376589298248291,-5.240942478179932,-2.458564519882202,-5.427165508270264,4.728918075561523,4.7684407234191895,0.5055481791496277,-5.412276744842529,-5.425759315490723,7.075611591339111,-2.326669454574585,-4.669156551361084,-4.669151782989502,-2.3288040161132812,-9.223444938659668,0.34372013807296753,0.5607508420944214,2.2093565464019775,-8.836587905883789,-3.423774003982544,-1.5747507810592651,-0.9910828471183777,3.92459774017334,4.738246440887451,4.8230204582214355,4.907479763031006,-1.1838836669921875,-1.837354063987732,7.31477689743042,4.218591213226318,7.359694004058838,-0.8462405204772949,3.060215950012207,-0.9997336864471436,-1.3571677207946777,-1.0214682817459106,-2.1556296348571777,-3.292544364929199,7.077654838562012,-5.21292781829834,-3.288384199142456,6.996507167816162,3.1960244178771973,-1.682244896888733,0.9349949955940247,6.1148176193237305,5.617997646331787,4.90889310836792,-3.230435609817505,-0.7873780727386475,-3.642404556274414,-0.587694525718689,4.544256210327148,-2.1473581790924072,0.5394940972328186,-2.442747116088867,5.345205783843994,-2.4335618019104004,-1.4465349912643433,5.654393672943115,-0.02170650102198124,0.40805909037590027,5.199135780334473,5.80703592300415,10.551921844482422,5.283897876739502,3.170903205871582,4.975529193878174,0.04173777997493744,5.397508144378662,10.544286727905273,10.527765274047852,-1.427609920501709,9.131309509277344,0.5630940198898315,2.686070442199707,-5.230878829956055,7.085875034332275,-0.7279884815216064,-0.8427466750144958,1.4760881662368774,5.622999668121338,0.1500551849603653,0.5477412343025208,6.090612888336182,-1.1462109088897705,-0.8062322735786438,-0.8517141342163086,7.206376075744629,-0.878722608089447,-0.19356654584407806,-0.9769500494003296,-6.37034273147583,0.3816205561161041,-0.559108316898346,0.4995587170124054,-0.7640487551689148,6.74288272857666,-1.9796167612075806,5.403504371643066,-0.8129191398620605,-9.91926383972168,5.2935261726379395,-2.425940990447998,4.793727397918701,-1.621085286140442,-5.237394332885742,-6.711789131164551,4.4083123207092285,-5.293705940246582,-6.667732238769531,-3.0216047763824463,-2.8993160724639893,5.829285621643066,-0.730949878692627,4.340153694152832,4.816230773925781,1.3723416328430176,-1.9791665077209473,4.859597206115723,-1.9314686059951782,0.05063658580183983,-2.954272985458374,-3.169402599334717,0.035635679960250854,5.1452555656433105,4.100458145141602,-2.4522788524627686,-2.994668483734131,-0.6341069340705872,-3.0079987049102783,-2.521235466003418,-2.4323413372039795,-3.1626482009887695,4.013590335845947,-3.0529656410217285,0.9354764819145203,3.0813448429107666,0.03431353345513344,-1.8689038753509521,0.5338958501815796,-0.5594928860664368,-5.2457966804504395,-2.5930113792419434,5.318643093109131,-0.7749427556991577,2.596813678741455,0.40507206320762634,4.820955276489258,4.8208723068237305,5.334456920623779,0.03086598590016365,-0.7651636004447937,-1.360361099243164,0.9339004755020142,4.7963786125183105,-6.722316265106201,4.792363166809082,-2.488010883331299,-1.4667057991027832,-0.7911914587020874,-1.6583446264266968,-5.275931358337402,-2.8273141384124756,7.069332122802734,8.272269248962402,-1.1432318687438965,-2.500168800354004,4.891491413116455,0.09222005307674408,3.5467472076416016,-2.351311445236206,-5.313546657562256,-0.7202157378196716,-0.6093099117279053,-1.075779676437378,-6.621366024017334,0.5314508080482483,-4.713319301605225,-0.6771389245986938,5.339015483856201,-2.125108242034912,3.532527446746826,4.476757526397705,-1.8573416471481323,3.070230722427368,-0.8813458681106567,6.36589241027832,4.995842456817627,0.8704063296318054,4.028454303741455,-0.8193690180778503,-2.420790672302246,-1.3178306818008423,0.07545094937086105,-0.7858585715293884,0.592016339302063,-0.42484426498413086,0.10974399000406265,-5.176818370819092,0.1095537319779396,2.1968672275543213,4.857393741607666,4.857570171356201,-0.008403748273849487,-2.4630212783813477,4.857334136962891,4.857138156890869,-0.13774944841861725,-0.6467210650444031,-2.4615917205810547,-0.8659738302230835,-2.372201681137085,4.509976863861084,-1.2696514129638672,-1.2381751537322998,-2.235126256942749,-0.649039089679718,-3.096243143081665,-1.6493927240371704,-0.6014840006828308,-0.8611869215965271,0.9361366629600525,-1.8403303623199463,4.736205577850342,-2.4573981761932373,0.506395697593689,0.4105755090713501,0.4079269766807556,6.056885719299316,-2.9735069274902344,0.31279900670051575,-1.1643389463424683,-1.665448546409607,-2.3760385513305664,-0.7994827628135681,-3.156461000442505,4.981502532958984,5.33028507232666,-3.4743425846099854,-1.9093531370162964,-0.6906508207321167,-0.8467462658882141,2.4644742012023926,-0.9114549160003662,7.571829319000244,0.4269961416721344,-0.8619145750999451,-5.546503067016602,3.7380483150482178,-1.574623465538025,-6.836496829986572,-1.2215629816055298,4.408275604248047,-0.7792528867721558,-2.2885165214538574,12.367219924926758,-3.1391818523406982,-8.352910995483398,5.56931209564209,-0.5109233260154724,4.935354232788086,-0.5213362574577332,-0.5090242028236389,-3.8559820652008057,-2.4619998931884766,-1.169172763824463,-0.7688767313957214,-6.608004093170166,-2.407758951187134,4.847863674163818,0.00792759284377098,0.4747416079044342,3.700242280960083,-1.925171136856079,-1.2081880569458008,6.456054210662842,6.959600448608398,-6.521336078643799,1.366041898727417,2.4534153938293457,2.717730760574341,5.229279518127441,5.221133708953857,-1.3961247205734253,5.3550286293029785,5.203953742980957,5.186919689178467,0.5898963809013367,-1.2001396417617798,2.055072069168091,10.601004600524902,5.522546291351318,-2.9419729709625244,2.699230909347534,-2.8396780490875244,3.0386404991149902,0.8197979927062988,0.3689504861831665,4.460022449493408,4.585562229156494,5.164119243621826,5.161984443664551,-0.8441542983055115,4.624602794647217,2.4585354328155518,2.468562602996826,0.3078499138355255,4.8213300704956055,7.234507083892822,-3.253079414367676,4.401542663574219,-0.03342651575803757,8.125663757324219,-0.6982735991477966,0.5023740530014038,0.4322667121887207,-2.54689359664917,-6.661271095275879,2.060415267944336,1.6113563776016235,-0.6662684082984924,-3.4236960411071777,-3.2306201457977295,-0.7107678055763245,-1.51390540599823,-1.8071281909942627,-10.012944221496582,-2.28574538230896,0.7106713056564331,10.71363639831543,-3.4278314113616943,7.571837425231934,-0.03156935051083565,6.635923862457275,-0.3852384686470032,-3.3982603549957275,8.577157020568848,-6.521472930908203,10.060745239257812,-5.600225448608398,-1.5542514324188232,-1.8664461374282837,6.087887763977051,6.520412921905518,4.485630035400391,4.473991870880127,-2.62801456451416,-1.6028293371200562,-4.3419294357299805,-0.9964779019355774,-0.8400875329971313,-2.3780672550201416,7.08587121963501,2.5860397815704346,6.92938756942749,-0.6889658570289612,-2.9416897296905518,-2.9417338371276855,5.198616981506348,5.758317470550537,-1.8830913305282593,8.578001976013184,-0.6144979596138,5.0566792488098145,1.2456021308898926,5.857806205749512,5.820063591003418,9.674971580505371,6.06935453414917,1.7911776304244995,0.40667736530303955,-0.7709337472915649,1.791184425354004,0.4066215455532074,-1.226046085357666,1.9742404222488403,5.182041168212891,-0.9320507049560547,5.416264057159424],"y":[13.753667831420898,6.664803504943848,-1.003271222114563,1.201336145401001,10.211286544799805,4.754048824310303,4.016635417938232,10.09512996673584,5.682965278625488,-0.6667593717575073,13.888311386108398,13.88834285736084,12.973174095153809,2.433364152908325,5.516970157623291,9.771357536315918,4.822462558746338,-1.6844761371612549,-1.4002349376678467,14.785909652709961,5.552956581115723,4.670009136199951,5.773464202880859,-5.299578666687012,10.613273620605469,8.451229095458984,4.535852432250977,10.006839752197266,-4.85197639465332,7.809547424316406,13.888129234313965,10.518500328063965,14.92785930633545,3.6858272552490234,4.271355152130127,2.0664196014404297,-3.7658567428588867,5.161342620849609,4.278663158416748,5.560665607452393,4.786872386932373,4.2232441902160645,-3.7658402919769287,-1.2070047855377197,9.808857917785645,2.4265964031219482,10.856653213500977,1.5470142364501953,11.056640625,9.799482345581055,-1.2472312450408936,5.518050670623779,1.1668825149536133,-0.7175595760345459,5.233771324157715,6.721252918243408,10.633505821228027,7.3137102127075195,7.313938617706299,10.999510765075684,12.973133087158203,-0.6180543303489685,5.235750198364258,10.221656799316406,4.8452887535095215,-0.6180920600891113,10.74606704711914,4.730753421783447,10.87948226928711,10.648829460144043,1.179117202758789,7.3452324867248535,2.683987617492676,11.005730628967285,4.665349960327148,10.975970268249512,11.312639236450195,16.329105377197266,16.329132080078125,2.259568691253662,8.203412055969238,3.8879809379577637,1.1265121698379517,10.750076293945312,7.363933086395264,3.4001312255859375,11.622421264648438,11.17115306854248,12.247315406799316,4.371859073638916,9.868796348571777,9.824037551879883,9.555225372314453,10.978721618652344,11.030853271484375,11.033476829528809,8.303619384765625,11.670919418334961,8.303311347961426,11.670982360839844,5.195733070373535,1.9867130517959595,11.62845230102539,5.4410200119018555,11.293543815612793,14.570978164672852,5.132541656494141,11.011749267578125,9.860767364501953,10.96005630493164,5.263192176818848,14.571001052856445,11.032524108886719,8.846628189086914,6.692963600158691,10.129902839660645,6.714967250823975,11.054559707641602,4.68811559677124,-0.31393107771873474,11.048739433288574,8.728133201599121,11.025739669799805,11.182498931884766,3.886941909790039,11.148505210876465,10.989842414855957,3.0373289585113525,5.478043556213379,11.057119369506836,11.028310775756836,10.643988609313965,4.414640426635742,11.092780113220215,11.049742698669434,5.461546421051025,5.7211103439331055,11.583511352539062,6.66605281829834,4.769582271575928,2.0084877014160156,11.210041046142578,8.296274185180664,10.757884979248047,11.625384330749512,8.26683521270752,10.661271095275879,10.750447273254395,9.36736011505127,5.430392742156982,10.980644226074219,5.618117332458496,10.841390609741211,10.764549255371094,10.768088340759277,6.101239204406738,-7.2149248123168945,11.044650077819824,7.3595290184021,11.012971878051758,11.008018493652344,10.54111385345459,10.802820205688477,2.471452236175537,-5.299089431762695,10.72460651397705,12.136185646057129,7.36157751083374,-0.931601881980896,11.088254928588867,7.3394975662231445,11.059916496276855,13.913354873657227,9.90315055847168,10.65627670288086,16.329055786132812,13.88841724395752,8.311955451965332,8.310349464416504,10.740673065185547,11.005868911743164,-5.299925327301025,16.328872680664062,16.329132080078125,1.4469249248504639,9.8866548538208,10.44360065460205,10.656874656677246,13.753847122192383,-7.215097904205322,6.3307905197143555,-0.4440752863883972,11.053202629089355,5.6472697257995605,-0.44989490509033203,-0.4495876431465149,1.5614663362503052,5.532995700836182,4.736141204833984,5.305940628051758,1.486106514930725,4.615488052368164,11.973726272583008,9.805599212646484,5.454500198364258,5.591413974761963,2.787797212600708,5.106431484222412,8.484283447265625,11.842617988586426,8.839791297912598,8.836073875427246,8.873865127563477,16.329132080078125,11.206217765808105,11.190398216247559,5.745800018310547,14.766392707824707,4.93007230758667,11.04015064239502,5.4815850257873535,4.929006576538086,4.618741035461426,10.730097770690918,-1.9466854333877563,8.826265335083008,2.067385673522949,8.831625938415527,-0.9308497309684753,13.913631439208984,-1.0130590200424194,13.913630485534668,6.667568206787109,10.694035530090332,2.19533109664917,10.146451950073242,-2.3612613677978516,-2.3612606525421143,-7.183393955230713,11.458171844482422,11.463618278503418,-5.350110054016113,-5.349649429321289,10.645816802978516,7.3605122566223145,-5.299822807312012,-7.493699073791504,11.882210731506348,4.959238529205322,1.592331051826477,5.667348861694336,5.7700934410095215,11.023609161376953,10.844671249389648,7.360380172729492,10.526468276977539,0.5509828925132751,8.21496868133545,3.8132429122924805,8.820741653442383,12.422637939453125,5.489030361175537,4.119317054748535,5.504106521606445,5.483436107635498,4.75856351852417,4.775139808654785,5.285706043243408,4.755521297454834,4.063225269317627,5.0672478675842285,4.786856174468994,8.311236381530762,-7.491688251495361,4.767542362213135,2.5268075466156006,1.1805979013442993,1.2798596620559692,4.320068836212158,11.756793975830078,10.544739723205566,4.340083122253418,4.716721534729004,5.2594685554504395,1.0181926488876343,4.169213771820068,-1.2238043546676636,4.332825183868408,1.672075867652893,1.8844064474105835,1.959742546081543,3.8276729583740234,3.978076219558716,5.147090911865234,12.213643074035645,9.913273811340332,4.162096977233887,-0.2001805156469345,3.701833486557007,2.263503313064575,8.315526962280273,4.6547064781188965,5.51202392578125,1.7444733381271362,2.26373028755188,4.628754615783691,-4.612391948699951,9.327662467956543,3.747283697128296,11.082534790039062,4.158197402954102,5.076900482177734,1.3319849967956543,3.582913637161255,4.640766143798828,5.481062412261963,5.422920227050781,3.00203537940979,9.198604583740234,4.800472259521484,4.426238059997559,4.137641429901123,4.13446044921875,1.5279717445373535,10.971144676208496,1.8235400915145874,10.865513801574707,4.249758720397949,5.659945487976074,-1.2068359851837158,4.84230375289917,10.761746406555176,11.920353889465332,-1.2198920249938965,4.357458591461182,1.1696538925170898,1.81417977809906,4.620946407318115,4.878897666931152,12.20794677734375,1.5013829469680786,2.5962283611297607,1.5026057958602905,4.17652702331543,8.229400634765625,5.396043300628662,1.1545616388320923,14.570965766906738,9.454178810119629,4.287848949432373,1.5664243698120117,1.4094151258468628,1.4032835960388184,8.400200843811035,1.5348330736160278,2.0380606651306152,8.242546081542969,5.4233808517456055,1.2402822971343994,9.547975540161133,6.72482442855835,-1.1979002952575684,5.253012657165527,1.4732532501220703,-1.074573278427124,4.170142650604248,11.458768844604492,1.7788234949111938,11.957606315612793,1.6071745157241821,8.438429832458496,1.9625122547149658,-0.9930202960968018,8.759339332580566,1.5115686655044556,1.5107715129852295,8.674221992492676,4.778591632843018,10.54919719696045,-1.2506687641143799,1.9768223762512207,5.246964454650879,12.422286987304688,1.5119224786758423,11.812955856323242,1.7054331302642822,-1.1854443550109863,12.247593879699707,5.6456298828125,2.3171677589416504,4.403102397918701,8.83138370513916,2.263399124145508,1.9366923570632935,6.726237773895264,5.030467987060547,5.2561540603637695,3.8210275173187256,11.19704532623291,4.2658305168151855,10.552471160888672,11.02176570892334,4.208614826202393,1.7382463216781616,5.396367073059082,12.137422561645508,0.8688684701919556,1.126743197441101,4.778151988983154,13.57065486907959,4.684172630310059,4.346652984619141,4.611032962799072,9.383459091186523,-1.2115310430526733,14.914356231689453,1.5629843473434448,-4.612351417541504,1.5722483396530151,9.123973846435547,1.1951689720153809,4.38590145111084,5.0091753005981445,1.269120693206787,4.514971733093262,1.5288491249084473,1.5525232553482056,1.7612749338150024,1.9633516073226929,1.994756817817688,3.8407063484191895,1.5963889360427856,3.491062641143799,3.4898271560668945,3.1660971641540527,14.764206886291504,4.4297099113464355,-0.6758551001548767,14.749829292297363,1.6826262474060059,3.1531286239624023,1.1003233194351196,1.3995110988616943,5.1693525314331055,5.169032573699951,5.783195495605469,1.5942378044128418,1.3291528224945068,11.135793685913086,4.621748924255371,-1.0022279024124146,1.5382078886032104,11.37185001373291,1.5886067152023315,-1.9466506242752075,4.612210750579834,1.4805492162704468,5.522522449493408,4.6636576652526855,4.268517971038818,4.306621074676514,5.23618745803833,0.7524745464324951,2.1955676078796387,4.17137336730957,0.9071375131607056,14.571085929870605,4.329647541046143,8.185243606567383,1.417358160018921,12.590710639953613,4.382014274597168,1.888399362564087,4.389299392700195,1.8883968591690063,11.054333686828613,-2.2092063426971436,13.924880981445312,0.8719916343688965,4.3898234367370605,4.3433427810668945,-0.8893031477928162,1.4278342723846436,1.2679696083068848,0.75396329164505,2.429577350616455,-2.2093541622161865,4.123691082000732,1.787782907485962,-2.2094855308532715,-0.889388382434845,-1.0716562271118164,-2.2094995975494385,3.0806655883789062,8.80632209777832,8.809652328491211,0.785708487033844,-1.0436581373214722,5.584100246429443,4.566430568695068,-5.301837921142578,4.7849016189575195,1.440151333808899,5.581185817718506,3.129124641418457,0.5660770535469055,0.5552969574928284,2.0084073543548584,4.392289161682129,11.115286827087402,1.682191014289856,1.9877198934555054,-2.209549903869629,0.8263005018234253,-0.8893221616744995,2.5724799633026123,4.055617809295654,1.4440382719039917,1.6615608930587769,7.430707931518555,1.5002899169921875,-0.8892952799797058,1.9978891611099243,4.342519283294678,1.5150362253189087,5.0150065422058105,5.022756099700928,0.5590559244155884,-0.20044006407260895,5.492413520812988,1.9896436929702759,0.8213284611701965,11.088577270507812,0.722978949546814,-1.6844978332519531,-0.19078145921230316,9.761741638183594,6.705489635467529,8.751441955566406,8.208548545837402,3.722508668899536,10.213491439819336,5.818788528442383,8.333638191223145,16.42123794555664,8.364154815673828,8.442974090576172,10.701766014099121,4.732120513916016,4.101798057556152,3.455095052719116,4.9504313468933105,4.048311710357666,1.285019040107727,1.2850421667099,2.363459348678589,2.4363760948181152,2.425668716430664,2.989870548248291,3.3904218673706055,-0.46173661947250366,9.609522819519043,3.4120895862579346,-0.6579616069793701,9.453729629516602,7.744061470031738,4.339964389801025,2.6889662742614746,8.374475479125977,7.435606002807617,7.2291131019592285,0.6259204745292664,5.289588928222656,9.808096885681152,9.808530807495117,4.39882755279541,3.791861057281494,12.843341827392578,9.366579055786133,0.627500593662262,8.66741943359375,2.4912614822387695,3.7304272651672363,7.802896976470947,2.4058868885040283,1.9552388191223145,11.57226848602295,3.7244832515716553,7.328121662139893,5.974269390106201,6.690029144287109,3.710797071456909,0.3700631558895111,0.7352920174598694,5.837700366973877,3.2133617401123047,-1.7339860200881958,-4.144360065460205,12.67516040802002,4.720609188079834,7.461658000946045,5.395934104919434,2.6812682151794434,2.6772079467773438,11.583946228027344,3.2755649089813232,3.8064863681793213,-0.32478800415992737,1.1465344429016113,-5.686752796173096,4.670040130615234,0.883108913898468,-5.5607686042785645,7.906547546386719,0.9661615490913391,8.334120750427246,3.8383116722106934,5.6366987228393555,5.532214164733887,-4.127776145935059,-2.8312108516693115,8.234869003295898,2.061270236968994,4.9349565505981445,2.4330246448516846,2.538377046585083,6.4629693031311035,11.583943367004395,9.610021591186523,6.983996391296387,10.577618598937988,10.5812406539917,3.257646322250366,3.2570550441741943,0.8599199056625366,1.0326541662216187,-3.6006462574005127,3.1931228637695312,9.048282623291016,13.925714492797852,7.424099922180176,5.723636150360107,8.133504867553711,8.1341552734375,8.3683443069458,12.58922290802002,16.368223190307617,-0.4619239568710327,7.356313228607178,4.316281318664551,6.6727752685546875,3.947692394256592,2.545745849609375,0.8548670411109924,2.8798866271972656,2.868539333343506,-0.6688877940177917,12.685935974121094,9.047526359558105,4.4126667976379395,9.108104705810547,-0.5538781881332397,3.3008434772491455,2.5518605709075928,6.824185848236084,-4.146798133850098,4.646315574645996,-1.36260986328125,4.8480401039123535,0.8660836815834045,0.8910971879959106,4.7551398277282715,10.06855583190918,7.379997253417969,8.250320434570312,4.539252281188965,1.8096600770950317,4.095775604248047,6.35255765914917,10.547935485839844,1.4007720947265625,2.5774667263031006,1.5394333600997925,0.7044823169708252,4.79024600982666,-0.1583612561225891,7.639578342437744,6.017501354217529,5.5372209548950195,5.262958526611328,5.264693260192871,2.5434744358062744,-2.8469672203063965,5.762955665588379,8.167055130004883,8.204660415649414,4.818146228790283,8.960455894470215,9.12983226776123,1.2679827213287354,4.182423114776611,-1.5060346126556396,2.69634747505188,4.8078999519348145,1.8971740007400513,0.919470489025116,4.943378925323486,4.955345630645752,2.7659099102020264,4.778174877166748,6.932318687438965,3.2660274505615234,8.088804244995117,6.556732654571533,6.321596145629883,4.5257039070129395,5.167259216308594,12.679787635803223,-2.6526951789855957,0.2427087277173996,3.3200552463531494,6.881405830383301,12.587632179260254,5.842348098754883,7.932308197021484,3.674268960952759,5.659283638000488,-0.5333103537559509,3.6365437507629395,3.1981823444366455,3.80961012840271,4.3383941650390625,4.979639530181885,7.781861782073975,1.9205719232559204,2.673724889755249,-4.131180286407471,14.441756248474121,4.565057754516602,8.618497848510742,1.8961020708084106,7.888862609863281,11.02599811553955,7.329555511474609,-4.373486518859863,-0.17529930174350739,4.1017913818359375,4.7090606689453125,6.981772422790527,8.702213287353516,2.6769371032714844,6.420210361480713,6.436262607574463,8.13929271697998,5.5923566818237305,6.92879581451416,9.740039825439453,5.2850565910339355,7.375123500823975,12.253012657165527,3.139561414718628,7.379707336425781,-6.947071552276611,4.028597354888916,6.465821266174316,6.460880756378174,8.24845027923584,6.0534491539001465,5.2562456130981445,2.9430062770843506,6.331620216369629,1.2673141956329346,1.2425525188446045,0.8685940504074097,9.227677345275879,-0.6491025686264038,-0.20242707431316376,4.205651760101318,5.195239067077637,5.078880786895752,8.044196128845215,4.096746921539307,2.650822162628174,4.040238857269287,7.607023239135742,1.207472324371338,10.018149375915527,3.8447465896606445,8.108957290649414,0.3878418505191803,3.678481101989746,2.675497055053711,0.947333037853241,8.443808555603027,3.3277101516723633,3.603101968765259,5.655066967010498,4.100139617919922,2.638640880584717,8.285212516784668,9.900068283081055,2.774474859237671,9.744009971618652,3.520615577697754,4.7606072425842285,7.383800983428955,5.271689414978027,5.4156060218811035,5.103021144866943,5.440036773681641,3.216949462890625,-0.5559954643249512,7.717106819152832,5.101963996887207,-2.6647932529449463,2.669935941696167,3.1388118267059326,3.6725122928619385,14.478215217590332,3.7090394496917725,4.109306335449219,11.131741523742676,11.131692886352539,4.079361915588379,7.6385178565979,-1.5920604467391968,7.92425537109375,8.013078689575195,-0.5643367767333984,-0.5642657279968262,-0.5644059777259827,13.926008224487305,11.719854354858398,-1.9155431985855103,-5.560892581939697,0.903595507144928,5.004859924316406,8.43660831451416,4.118094444274902,4.627621173858643,1.7034307718276978,16.715024948120117,0.9905622601509094,5.274652481079102,6.250498294830322,3.805746555328369,0.23691968619823456,-0.45831769704818726,3.477940559387207,7.905506134033203,5.431629657745361,3.0772860050201416,16.71500015258789,3.7330527305603027,16.4217472076416,3.313117265701294,3.3357889652252197,16.421802520751953,11.13019847869873,7.041196346282959,3.1236495971679688,3.218616008758545,7.955290794372559,4.403489112854004,4.38922643661499,5.20853853225708,3.889711380004883,4.831067085266113,4.2841715812683105,2.7004642486572266,2.9002461433410645,10.008540153503418,8.481829643249512,13.736631393432617,6.65021276473999,0.9790188670158386,-0.10745874792337418,0.9629961848258972,1.4294098615646362,0.9753729701042175,8.142341613769531,8.141368865966797,2.1791024208068848,12.68586540222168,3.3188998699188232,7.043752193450928,-3.3332443237304688,8.487144470214844,-1.6635702848434448,3.8540289402008057,3.621448040008545,3.9610371589660645,3.995410442352295,1.765092134475708,3.2684102058410645,2.994112014770508,8.382697105407715,4.997733116149902,5.12066650390625,8.766300201416016,5.209216117858887,4.7266106605529785,6.280828952789307,7.076685428619385,-1.1103283166885376,2.27927303314209,5.330153465270996,8.481646537780762,5.3661909103393555,7.639372825622559,4.7693257331848145,5.256217956542969,7.303874492645264,2.678861379623413,12.964082717895508,2.760667562484741,8.319377899169922,-1.9697378873825073,-1.9795684814453125,5.489872932434082,1.2365069389343262,2.1976685523986816,6.690828323364258,2.6143412590026855,7.639181137084961,8.13815689086914,9.745538711547852,8.207074165344238,7.424319744110107,6.653392314910889,1.078979730606079,11.129020690917969,14.274479866027832,3.091907262802124,7.4470977783203125,4.850975513458252,12.626755714416504,1.1700407266616821,5.43168830871582,8.723075866699219,6.807880878448486,8.341618537902832,7.636516571044922,10.413082122802734,5.205737590789795,-4.471816539764404,3.5370826721191406,1.1359732151031494,-4.528565883636475,2.6028683185577393,3.6068668365478516,9.885340690612793,3.102886199951172,2.4330077171325684,5.700389385223389,2.650360584259033,1.6698739528656006,2.207136631011963,-3.7241549491882324,11.135200500488281,8.294633865356445,-6.385956287384033,4.104518890380859,2.3936991691589355,4.124452590942383,8.45773696899414,-0.34713318943977356,11.618363380432129,14.004250526428223,14.002889633178711,8.439630508422852,1.9339375495910645,12.604264259338379,9.75086784362793,8.348109245300293,4.07574462890625,0.9935122132301331,4.052516937255859,3.48230242729187,3.934765338897705,4.316646575927734,11.135347366333008,5.962011814117432,2.093791961669922,8.747908592224121,5.296205043792725,1.170384407043457,12.40231704711914,-2.5793488025665283,8.303500175476074,2.7717814445495605,3.0669476985931396,6.311244964599609,4.317276477813721,1.2045236825942993,1.518027663230896,-0.5660728216171265,4.756741046905518,5.503994464874268,1.528396725654602,2.1794521808624268,7.683493614196777,2.208301544189453,4.752774238586426,5.233245849609375,7.703436374664307,2.197674036026001,0.37286606431007385,2.7717788219451904,1.0203688144683838,3.7699382305145264,12.749470710754395,3.044426202774048,0.8279761672019958,0.8277171850204468,14.468153953552246,9.523780822753906,3.732550621032715,0.8281025886535645,3.720717191696167,1.8188620805740356,9.819999694824219,1.5844711065292358,-1.6028352975845337,6.308254718780518,-2.7978315353393555,13.004638671875,3.700735330581665,6.271007061004639,7.46694803237915,5.926529884338379,4.218763828277588,1.5721317529678345,10.702735900878906,2.2251009941101074,2.2410995960235596,6.269162178039551,6.63805627822876,7.359856605529785,1.1790106296539307,10.702706336975098,5.391234874725342,7.348556041717529,3.946354627609253,7.352886199951172,9.22880744934082,4.92092227935791,7.641262531280518,12.734030723571777,14.473880767822266,6.419399261474609,12.730874061584473,5.855963230133057,4.743375778198242,3.175549030303955,4.070493698120117,0.3902737498283386,5.497287750244141,1.923177719116211,2.2340407371520996,3.7623703479766846,4.686135768890381,5.498665809631348,0.39516839385032654,1.1813626289367676,5.475013732910156,3.986525297164917,5.385573387145996,5.419665336608887,5.639106273651123,-0.5518732070922852,5.644217491149902,-3.447225570678711,-1.3634370565414429,4.691429615020752,7.38032341003418,7.439977169036865,0.38870516419410706,4.919856548309326,-0.5517223477363586,9.061192512512207,3.3614649772644043,3.9728598594665527,3.9542438983917236,6.895614147186279,6.895712375640869,6.859975337982178,3.952340841293335,-1.0966929197311401,12.675154685974121,7.339074611663818,6.662835597991943,9.237273216247559,9.26025104522705,4.738646507263184,6.563467025756836,5.639383792877197,5.498905658721924,8.423685073852539,1.0267038345336914,9.2107572555542,8.49698257446289,6.933939456939697,10.274351119995117,-0.7761797308921814,-0.10939367860555649,0.02881193719804287,4.686522960662842,4.8585405349731445,7.475907802581787,8.367325782775879,8.187265396118164,6.581059455871582,7.126306533813477,-2.1183032989501953,6.567596912384033,9.197173118591309,5.592936038970947,11.145041465759277,5.610941410064697,12.983887672424316,8.458115577697754,-0.5511374473571777,9.188715934753418,5.4975433349609375,-0.2764034569263458,4.046880722045898,8.299250602722168,11.142378807067871,5.691323280334473,2.3490211963653564,4.482396602630615,-0.325645387172699,1.763870120048523,0.9562180638313293,8.651432037353516,5.411860942840576,6.496175289154053,-6.385944843292236,7.377659797668457,3.0603737831115723,6.50454568862915,2.9101877212524414,6.033374309539795,2.773594856262207,7.892843723297119,12.68589973449707,-2.0051279067993164,6.571173667907715,2.3465330600738525,6.793457508087158,16.489337921142578,0.9100803732872009,7.303962230682373,6.283022880554199,5.6098833084106445,8.095819473266602,2.598116874694824,5.6046142578125,3.3196070194244385,9.459024429321289,5.962156772613525,-2.1185293197631836,3.4499733448028564,-0.17944316565990448,5.710134983062744,0.7367303967475891,4.049670219421387,1.1694371700286865,3.3443315029144287,7.303905963897705,7.945987224578857,8.180562019348145,5.2313971519470215,2.6719255447387695,5.232069969177246,3.4654810428619385,16.489389419555664,5.344808101654053,11.502595901489258,12.686140060424805,-2.118440628051758,-3.8078298568725586,11.026060104370117,8.117756843566895,-1.6213337182998657,8.481932640075684,5.839806079864502,3.7402968406677246,5.82936954498291,1.082351803779602,5.659636497497559,5.3091912269592285,5.307718276977539,8.611384391784668,4.258547782897949,7.118494987487793,9.005329132080078,4.039278507232666,7.193060874938965,6.488726615905762,9.016772270202637,4.792813301086426,9.01839828491211,6.197359085083008,3.99556827545166,5.4316020011901855,5.764944076538086,8.023822784423828,1.0295076370239258,3.5318496227264404,7.4257893562316895,-3.432981491088867,10.452554702758789,3.828026533126831,-0.478844553232193,8.192864418029785,2.9326272010803223,2.9462363719940186,2.4248833656311035,5.716916561126709,3.2873878479003906,3.772764205932617,0.8141902089118958,0.3658578395843506,-0.11564505845308304,-0.11962735652923584,5.671293258666992,4.4916887283325195,12.288342475891113,2.8508548736572266,4.178261756896973,2.197606086730957,5.329584121704102,7.630529880523682,12.971240997314453,-3.8086133003234863,5.91251802444458,8.288174629211426,11.136299133300781,-0.33743345737457275,4.493120193481445,5.510365962982178,-2.9138143062591553,0.3884759247303009,12.68601131439209,3.8662264347076416,7.020208835601807,0.4066712558269501,-1.6214203834533691,-2.3934192657470703,5.3355889320373535,4.777369499206543,3.6052677631378174,10.70294189453125,8.259513854980469,5.144166469573975,8.02422046661377,0.7213940620422363,10.408122062683105,4.94993257522583,5.513437747955322,2.767906427383423,8.475112915039062,3.9007647037506104,-1.6254706382751465,10.556028366088867,-1.9192918539047241,7.162869930267334,3.9523141384124756,2.720689535140991,1.1381268501281738,-3.6288764476776123,2.6748404502868652,8.399874687194824,4.677345275878906,6.802083492279053,3.090486526489258,3.4361069202423096,3.587824583053589,-0.624744176864624,1.6291097402572632,2.992642641067505,3.889220714569092,1.3903247117996216,10.61396598815918,11.719914436340332,0.7555164694786072,6.854727268218994,12.523690223693848,3.0307822227478027,9.47551155090332,0.8580517172813416,6.2960309982299805,5.005972385406494,4.160580635070801,4.688943386077881,6.449133396148682,4.13693904876709,-2.107642650604248,12.288383483886719,5.87694787979126,5.980763912200928,2.916271924972534,-2.008084297180176,7.702997207641602,3.2815892696380615,0.8170367479324341,12.59047794342041,2.9750237464904785,3.0108389854431152,9.9000883102417,2.6555728912353516,6.271254539489746,7.330843448638916,4.1106085777282715,4.127627372741699,3.061366558074951,11.1453857421875,11.081433296203613,0.3702097237110138,9.424382209777832,6.164470672607422,7.421271324157715,7.167369365692139,8.598220825195312,4.325260162353516,12.98776626586914,5.528580665588379,8.239240646362305,-0.3457632064819336,9.71713638305664,2.946995973587036,4.756048202514648,6.700717449188232,0.38922548294067383,9.405680656433105,-2.153721332550049,13.256037712097168,2.278958320617676,4.3178582191467285,4.318001747131348,6.3465657234191895,8.47721004486084,7.069681167602539,3.8618526458740234,8.80936336517334,2.3471875190734863,6.268856048583984,-2.12276029586792,8.054777145385742,13.925949096679688,5.33860445022583,14.468213081359863,7.21511697769165,7.12060546875,7.686731815338135,11.07845687866211,3.4202945232391357,10.553404808044434,0.8442627191543579,3.9023189544677734,8.955573081970215,9.619874954223633,5.5372138023376465,4.708869934082031,7.8106160163879395,6.926491737365723,4.416581630706787,8.559774398803711,6.5000481605529785,6.874568462371826,7.688422203063965,9.200560569763184,6.5511956214904785,2.5393285751342773,2.4094364643096924,3.508603811264038,1.8425685167312622,4.317698955535889,4.480342388153076,0.9222416281700134,3.2693915367126465,11.120001792907715,5.326303482055664,-3.3029868602752686,5.326913833618164,6.2461090087890625,3.041301965713501,4.473928928375244,1.771298885345459,-3.312734365463257,7.998401641845703,4.75523567199707,6.6509785652160645,8.345574378967285,3.9946281909942627,4.776537895202637,7.858278274536133,3.912252187728882,4.054325580596924,2.652279853820801,3.58313250541687,5.4207258224487305,14.625995635986328,-0.45994997024536133,6.846419811248779,8.201709747314453,6.824207305908203,6.846426010131836,4.4593281745910645,2.023390769958496,2.315957546234131,7.086874008178711,5.503602981567383,3.655627489089966,-2.6588709354400635,6.810435771942139,-0.6257838010787964,-0.7750189304351807,4.495068550109863,3.57063889503479,2.6644837856292725,-2.739057779312134,4.689472198486328,4.770145416259766,-1.2257577180862427,5.092031002044678,-0.8641224503517151,-1.9209169149398804,2.7145776748657227,16.12929344177246,6.846295356750488,3.2567403316497803,3.8457932472229004,6.7810516357421875,8.348132133483887,1.709453821182251,-0.5591066479682922,-0.4612945318222046,6.784666538238525,9.405365943908691,8.464908599853516,0.4335119426250458,3.0116968154907227,8.808788299560547,4.489102840423584,4.492604732513428,2.3634777069091797,5.2615556716918945,-2.0150320529937744,3.2391560077667236,5.292922496795654,-3.285034418106079,1.3061579465866089,6.764914035797119,8.254067420959473,1.7097524404525757,2.364138126373291,3.0827341079711914,-0.4861122667789459,0.9005186557769775,1.966382384300232,14.003920555114746,7.1510396003723145,4.48911714553833,2.7487707138061523,3.3159353733062744,6.278201580047607,2.8860819339752197,8.252555847167969,5.591020584106445,2.357666492462158,5.866749286651611,5.6681671142578125,7.811737537384033,14.274444580078125,14.27432632446289,3.89668869972229,8.170132637023926,6.767612457275391,4.293236255645752,6.6120524406433105,3.5718162059783936,5.672250270843506,6.2083234786987305,-0.5689746141433716,0.36266976594924927,4.391616344451904,5.063127517700195,7.40586519241333,3.088104248046875,3.3216376304626465,3.3206262588500977,2.8928427696228027,6.7395195960998535,1.5181093215942383,3.4217801094055176,-1.6217741966247559,12.552543640136719,12.534735679626465,5.831551551818848,5.1075592041015625,5.553020000457764,3.506051778793335,4.676388740539551,5.48762321472168,-0.25628921389579773,5.258524417877197,5.258172035217285,7.0162272453308105,6.682799339294434,5.299649238586426,-0.3393961489200592,4.673036098480225,2.0937552452087402,1.9404340982437134,8.286540985107422,5.478790760040283,8.095746994018555,3.6772170066833496,5.891074180603027,4.3459367752075195,3.643392562866211,-2.015171766281128,9.02582836151123,9.017011642456055,4.944690704345703,6.005021095275879,6.408115863800049,1.5972951650619507,0.862757682800293,1.7174327373504639,4.976916790008545,2.865105628967285,0.27417147159576416,1.0677911043167114,3.782811403274536,13.387167930603027,9.672229766845703,7.541501045227051,4.964987277984619,3.2391881942749023,3.085238456726074,3.089512348175049,0.26641958951950073,1.4653397798538208,3.3786325454711914,8.760431289672852,8.107466697692871,10.364603996276855,4.699963569641113,6.149604320526123,5.782856464385986,-0.33947813510894775,-2.1185667514801025,3.6492087841033936,0.23377852141857147,7.256511688232422,10.405156135559082,3.3810901641845703,5.023744583129883,8.18922233581543,1.6683175563812256,6.645003318786621,2.817108392715454,5.250523090362549,7.0544328689575195,8.809531211853027,2.013779401779175,4.02644157409668,0.008112098090350628,6.774561405181885,4.049668312072754,10.500789642333984,6.316641330718994,4.773196697235107,8.3467435836792,-0.3618199825286865,5.58063268661499,1.4654314517974854,7.004108905792236,5.58795166015625,12.536240577697754,10.02856159210205,8.777953147888184,9.190193176269531,0.6012914180755615,0.2519705891609192,-0.5643693804740906,0.5916755795478821,-1.09237539768219,14.626038551330566,-1.2454971075057983,6.2801899909973145,0.2546762526035309,3.5040698051452637,5.396900177001953,2.433046340942383,9.330366134643555,5.740139484405518,8.79674243927002,1.7357008457183838,6.818711280822754,5.355188369750977,3.9573285579681396,8.689215660095215,7.3777546882629395,2.432800769805908,6.8589019775390625,5.755858898162842,7.378201484680176,7.3796586990356445,1.5006589889526367,-1.7141120433807373,6.013428688049316,8.374419212341309,6.3176703453063965,4.4047160148620605,5.6429524421691895,7.115401744842529,5.197562217712402,6.0563483238220215,7.114129543304443,3.388051748275757,7.861250877380371,8.060216903686523,2.2238717079162598,13.736676216125488,4.182833671569824,3.7163257598876953,7.383567810058594,5.8536152839660645,0.9203569293022156,-2.1385772228240967,1.6857253313064575,0.726620078086853,3.8422293663024902,6.905361175537109,2.9402313232421875,-1.2328720092773438,1.7619516849517822,3.116703987121582,5.341520309448242,6.042977333068848,1.6939979791641235,5.662817478179932,6.143972396850586,1.772900938987732,7.785212993621826,1.7854079008102417,4.028135776519775,5.467748641967773,1.772360920906067,1.0008440017700195,14.478203773498535,12.058591842651367,-0.09719505161046982,5.9437737464904785,0.8536957502365112,8.727303504943848,8.578027725219727,8.625,13.005819320678711,5.547267913818359,8.349699020385742,6.312328815460205,6.7577056884765625,9.429261207580566,6.218011856079102,1.7814674377441406,8.161853790283203,4.840010166168213,2.780832290649414,12.29702091217041,6.2642364501953125,5.610891819000244,0.7962910532951355,6.110844135284424,4.431352138519287,12.253545761108398,6.3606767654418945,4.222149848937988,1.4204939603805542,2.821650743484497,-0.4190250337123871,14.54544448852539,5.709725379943848,5.6212663650512695,4.193599224090576,-2.105346441268921,-2.1052703857421875,0.358720600605011,8.510428428649902,8.429854393005371,-1.3223825693130493,3.734748125076294,-2.032503128051758,-0.389221727848053,6.041894912719727,1.0936038494110107,4.840129375457764,4.840404510498047,7.745324611663818,7.699615478515625,7.999247074127197,5.836088180541992,7.038651943206787,7.055081367492676,4.583512306213379,6.382259845733643,8.65255069732666,3.697965621948242,4.904565811157227,1.5235352516174316,1.1501976251602173,4.483175277709961,-0.12400943040847778,1.4742103815078735,4.827502250671387,4.016299247741699,3.187032699584961,5.646903991699219,7.040046215057373,5.662296772003174,5.08833646774292,9.095808982849121,5.624066352844238,8.693524360656738,3.0839643478393555,4.2716898918151855,4.656551361083984,-1.2954063415527344,4.950237274169922,10.500470161437988,3.440402030944824,-0.008169718086719513,7.660196781158447,5.549027919769287,7.65933084487915,3.2007009983062744,1.71866774559021,13.006115913391113,5.895803928375244,8.54926872253418,10.02453327178955,5.62428617477417,4.014340400695801,5.143365859985352,8.33829116821289,5.315887928009033,5.3177361488342285,4.762581825256348,8.406838417053223,7.147304058074951,3.2365221977233887,2.9326443672180176,7.467588901519775,6.831921100616455,16.489477157592773,8.336073875427246,11.106884002685547,4.578253269195557,2.72636079788208,4.892168045043945,7.026733875274658,7.033943176269531,5.863874912261963,3.7638657093048096,5.161693572998047,13.00609302520752,-0.17907185852527618,2.8866050243377686,4.304320812225342,4.256138801574707,6.6775407791137695,9.05367660522461,-1.2257590293884277,-3.346302032470703,-3.3302412033081055,4.563576698303223,-0.5169801115989685,10.501045227050781,3.2789971828460693,-1.272514820098877,4.043461799621582,2.7941551208496094,5.597815036773682,3.9619851112365723,2.945741653442383,1.8413968086242676,2.6872031688690186,4.191615104675293,4.398205757141113,7.878556251525879,6.47739315032959,7.11993408203125,9.401226997375488,8.370006561279297,4.128471851348877,5.407595634460449,3.131073236465454,5.409815788269043,4.15606689453125,-0.25815075635910034,7.5442891120910645,7.544279098510742,-0.5867673754692078,-3.4472174644470215,7.781418323516846,4.70166015625,8.090843200683594,4.4276814460754395,6.070018768310547,0.9107906818389893,8.170109748840332,4.292767524719238,-2.138577938079834,8.363486289978027,5.276595115661621,7.406733512878418,5.62737512588501,11.989683151245117,7.666830062866211,5.133403301239014,5.623541355133057,0.013212530873715878,7.6664276123046875,13.996602058410645,2.8417270183563232,0.2499762326478958,0.8567826151847839,6.994760513305664,9.737818717956543,4.30493688583374,5.55117654800415,8.170095443725586,5.345638751983643,6.761714935302734,0.12395535409450531,1.141900897026062,8.761994361877441,6.066818714141846,5.674283504486084,10.559038162231445,6.148224830627441,6.877751350402832,-3.7003698348999023,7.1088666915893555,6.979766368865967,8.259472846984863,11.1425199508667,2.712721586227417,4.409723281860352,9.388166427612305,2.6640357971191406,5.623304843902588,11.797664642333984,7.195459842681885,5.867712497711182,2.736013889312744,0.7119788527488708,4.377033710479736,9.331478118896484,3.8060238361358643,3.7320170402526855,4.685023307800293,3.9143307209014893,2.8024208545684814,-0.10560460388660431,1.6543656587600708,4.236408710479736,5.9602179527282715,-0.5689904689788818,-3.7022249698638916,-0.5601049661636353,-0.3859521448612213,7.129581451416016,-3.3178012371063232,-0.5253887176513672,6.868739604949951,10.485213279724121,2.7349750995635986,1.4136658906936646,6.324517726898193,-0.5573003888130188,4.577902317047119,4.190495014190674,4.783043384552002,2.72131085395813,0.01764376275241375,3.7845664024353027,5.681765556335449,-1.1276813745498657,3.429694890975952,4.77627420425415,6.237659931182861,9.468939781188965,3.732724189758301,5.010568618774414,2.69423246383667,6.011205196380615,3.5952672958374023,5.0868940353393555,3.44761061668396,-1.2601269483566284,5.118489742279053,1.082777976989746,5.319002151489258,-1.4418706893920898,4.445755481719971,-1.4171507358551025,9.217073440551758,0.3598375916481018,-0.06056807562708855,2.8022818565368652,11.572225570678711,5.371415615081787,9.592658996582031,1.9671465158462524,5.835639953613281,4.174759387969971,-1.7140940427780151,14.005825996398926,3.5498902797698975,4.865640163421631,-0.44144386053085327,4.445009231567383,10.500845909118652,3.2081918716430664,10.500763893127441,7.6596360206604,1.742660641670227,7.129657745361328,5.825340747833252,5.613587856292725,7.928457260131836,4.791475772857666,2.705559492111206,0.254660427570343,5.594958305358887,0.2472398728132248,2.728943109512329,5.352224826812744,6.334256649017334,3.7668850421905518,6.956045150756836,3.6982738971710205,3.601369857788086,5.378581523895264,6.945404529571533,6.954277038574219,4.662320613861084,2.946331262588501,-0.7090936303138733,-0.7094133496284485,2.9399731159210205,4.339258670806885,8.763386726379395,6.3386640548706055,9.53505802154541,4.015523910522461,-0.40562623739242554,5.763521194458008,5.421286582946777,4.679487228393555,3.415597915649414,3.278181314468384,3.210318088531494,5.9900078773498535,3.1326589584350586,2.2074997425079346,8.2545166015625,2.0718770027160645,3.760633945465088,0.7271538376808167,5.664581775665283,2.809455156326294,4.307934761047363,3.0136539936065674,2.7210147380828857,4.671641826629639,6.370008945465088,2.705113649368286,-0.8644720315933228,0.7191302180290222,6.3213090896606445,16.130420684814453,7.408609390258789,8.65024185180664,6.4108567237854,12.843141555786133,5.3690924644470215,7.111083507537842,5.799025535583496,4.024549961090088,3.013705253601074,5.883533477783203,3.7567813396453857,1.7700179815292358,3.7626700401306152,3.7925283908843994,5.316702842712402,8.320960998535156,9.995890617370605,3.0773544311523438,2.7317285537719727,-1.2610360383987427,3.4459285736083984,0.7254217267036438,3.1687023639678955,-3.701878070831299,3.5401463508605957,-1.2455021142959595,-1.2309253215789795,4.633932113647461,8.810138702392578,5.607920169830322,8.352690696716309,6.301249980926514,-0.12454571574926376,4.485962867736816,3.558892011642456,-0.10182853788137436,1.7062382698059082,5.5980353355407715,9.346349716186523,6.015310287475586,3.6891536712646484,4.486823558807373,7.179409503936768,11.989622116088867,4.566972732543945,5.864943504333496,7.165084362030029,4.8421196937561035,10.502201080322266,-0.1797780990600586,6.076792240142822,5.349931240081787,3.2915196418762207,9.930015563964844,7.264499187469482,4.516512870788574,3.9702255725860596,3.4538633823394775,5.5259599685668945,3.726331949234009,6.016708850860596,6.336455821990967,9.165959358215332,-0.008767206221818924,6.849985599517822,9.11555290222168,-0.49655619263648987,3.232314109802246,1.3658297061920166,-2.59948992729187,4.549779891967773,3.6547014713287354,-0.8851091861724854,9.930785179138184,-4.507270812988281,3.2185661792755127,5.648167610168457,6.060940265655518,5.545817852020264,1.7605705261230469,13.257068634033203,6.729122638702393,5.214994430541992,-0.5213088989257812,4.192699909210205,-0.5084972977638245,2.9383933544158936,5.143850803375244,5.354893207550049,6.819857597351074,5.624236106872559,16.130870819091797,2.717972755432129,1.7697173357009888,14.004653930664062,5.891984939575195,-0.179565891623497,6.32997465133667,2.5989387035369873,0.5114438533782959,5.341224193572998,8.346733093261719,10.424015998840332,9.167926788330078,9.16849136352539,0.5309685468673706,1.8788155317306519,3.646430730819702,2.8085567951202393,16.12950897216797,8.470149040222168,9.179420471191406,8.463600158691406,5.2781853675842285,5.856493949890137,7.785544395446777,3.9648797512054443,6.343776226043701,2.60040283203125,2.938264846801758,5.537117004394531,4.276198863983154,5.270750045776367,3.219855785369873,-0.4704627990722656,4.336893081665039,1.0813398361206055,6.868161201477051,6.1590423583984375,5.762908935546875,7.111170768737793,9.040799140930176,6.218155384063721,9.61988639831543,4.0294342041015625,0.9856254458427429,3.3523476123809814,12.058586120605469,8.302522659301758,3.039560317993164,1.96718430519104,4.588533401489258,3.0887973308563232,-2.728989362716675,9.607911109924316,6.804165363311768,4.414113521575928,1.0252357721328735,3.699457883834839,-0.42287635803222656,3.5866239070892334,5.844936847686768,3.342454195022583,-3.215886116027832,6.73604679107666,-3.2158467769622803,9.573429107666016,-4.539924144744873,-4.538745880126953,5.871355056762695,5.297029495239258,-4.5342631340026855,-4.539261817932129,5.627721786499023,6.263805389404297,0.9964205622673035,5.413199424743652,3.482970952987671,2.193535566329956,3.4127986431121826,3.4199938774108887,2.536498785018921,6.260257244110107,4.480488300323486,6.008410453796387,6.371265411376953,3.5778658390045166,16.132112503051758,9.748367309570312,2.2660319805145264,3.965339183807373,9.389312744140625,10.398005485534668,10.406908988952637,2.8127667903900146,4.2814412117004395,8.766465187072754,3.5449602603912354,5.990662574768066,1.0466804504394531,6.27124547958374,5.474813938140869,7.411517143249512,0.5203259587287903,-0.4224843680858612,5.142891883850098,6.276396751403809,3.753366708755493,10.545930862426758,2.46734619140625,10.875911712646484,10.287956237792969,7.157680988311768,5.298579216003418,-3.457977771759033,5.76350212097168,-2.0152201652526855,3.739375114440918,-0.006877655163407326,3.7155137062072754,7.384431838989258,6.773743629455566,5.036954402923584,13.284055709838867,-1.930975317955017,-0.10910826921463013,-2.7838079929351807,-0.10123228281736374,-0.10437839478254318,9.0957612991333,3.9586024284362793,4.201492786407471,5.325014114379883,9.021679878234863,6.240737438201904,-4.583099365234375,1.8299988508224487,9.056949615478516,1.9293221235275269,3.1193771362304688,4.031471252441406,2.5806567668914795,3.130643844604492,8.861942291259766,-0.8828967213630676,10.540613174438477,12.00284481048584,-2.484795331954956,-2.491708755493164,2.7036821842193604,5.168939590454102,-2.5078012943267822,3.0867834091186523,6.447824478149414,3.1706724166870117,0.9039825201034546,-1.3921600580215454,-2.0340020656585693,2.366692304611206,11.957324981689453,2.615018844604492,9.44802188873291,6.69525146484375,0.6779693961143494,4.970856189727783,3.234687566757202,3.6785900592803955,3.6591405868530273,6.317544460296631,3.2288262844085693,8.331966400146484,8.33779239654541,8.762361526489258,9.169588088989258,2.2561628818511963,4.656454086303711,4.242743492126465,8.31474781036377,8.50614070892334,0.08778383582830429,9.594029426574707,10.03588581085205,5.954284191131592,4.426793575286865,0.9148631691932678,0.6161473989486694,5.951980113983154,-0.39072561264038086,12.842697143554688,4.086121559143066,3.3326165676116943,2.080843925476074,3.98699951171875,1.1075438261032104,5.202662944793701,-1.6064367294311523,2.7957370281219482,10.87588882446289,1.9006807804107666,14.478262901306152,7.210017204284668,2.7914764881134033,-1.6678993701934814,8.860854148864746,10.702827453613281,7.3808674812316895,6.052992343902588,14.001442909240723,6.844242572784424,-4.373536109924316,4.911505222320557,4.927911758422852,6.328238487243652,5.984919548034668,1.9152247905731201,5.416451930999756,7.322874546051025,5.173277378082275,-0.12483338266611099,1.6009389162063599,3.1424944400787354,6.276898384094238,-0.5587362051010132,-0.5591639876365662,3.0791561603546143,1.439916968345642,4.4700188636779785,-1.6670057773590088,3.740842819213867,6.536789894104004,7.686847686767578,1.5689517259597778,1.562807559967041,-0.5673656463623047,6.022629261016846,14.883015632629395,10.035743713378906,7.68533182144165,14.883180618286133,10.0358247756958,3.7716400623321533,0.8505668640136719,3.0836181640625,9.371045112609863,1.7063168287277222],"z":[2.3523590564727783,-4.635682106018066,1.8824985027313232,6.0989990234375,4.894308567047119,6.861724376678467,6.541004180908203,4.876069068908691,4.464578628540039,-5.1774373054504395,7.529850006103516,7.5298662185668945,-2.162778854370117,4.903619766235352,-5.815722942352295,3.9722750186920166,6.933207988739014,3.8309199810028076,-2.8539931774139404,3.353569269180298,4.779223918914795,5.7655134201049805,4.3646697998046875,-8.358372688293457,4.872564792633057,5.247234344482422,5.780235290527344,4.881479740142822,3.251190185546875,0.6381073594093323,7.529772758483887,5.210483074188232,3.3985822200775146,5.885591983795166,5.853856563568115,4.713418006896973,1.2178467512130737,5.359476566314697,5.957284927368164,4.7702507972717285,6.10610818862915,5.865230560302734,1.2178962230682373,10.05156135559082,5.3331146240234375,4.895184516906738,5.004374027252197,-4.696730136871338,5.300642013549805,4.198570251464844,10.012173652648926,4.811145305633545,6.097463130950928,-5.123950958251953,5.298505783081055,4.013271331787109,4.086260795593262,-8.081271171569824,-8.08143138885498,5.349905490875244,-2.162687301635742,-5.8775458335876465,5.297502517700195,4.989527225494385,5.859792709350586,-5.877345085144043,5.089630126953125,5.9590678215026855,5.11692476272583,5.285843849182129,6.100533962249756,-0.1289713829755783,5.066348075866699,5.627960205078125,5.857539176940918,4.576076507568359,5.1374192237854,4.638611793518066,4.638613224029541,4.812153339385986,-1.2432382106781006,4.366302967071533,6.092936992645264,3.8718626499176025,-0.10807915031909943,2.844072103500366,5.272828102111816,4.200159072875977,-5.423769950866699,-5.6821088790893555,5.105863571166992,5.302110195159912,4.964427471160889,5.324921607971191,5.824072360992432,5.650442600250244,5.722906112670898,-2.720721483230591,5.728050708770752,-2.720978021621704,-1.0775089263916016,6.195528984069824,5.254692077636719,-5.868721961975098,4.230963706970215,2.555678129196167,-5.8867902755737305,5.6830034255981445,5.127432346343994,5.165534973144531,-5.888232231140137,2.555678606033325,5.65588903427124,1.0506772994995117,4.00441837310791,4.1730265617370605,4.01124906539917,5.596904754638672,-1.6243011951446533,1.5726680755615234,5.793933868408203,4.995269298553467,4.7251996994018555,4.21591329574585,4.364456653594971,4.209197998046875,5.326282024383545,6.261023044586182,4.885592460632324,5.938136577606201,5.230201721191406,5.069272041320801,6.118946552276611,5.280365943908691,5.270572185516357,4.913327217102051,4.50498104095459,5.313181400299072,-4.638008117675781,5.841261386871338,4.421542644500732,5.2966132164001465,5.256125450134277,4.72050142288208,5.23521614074707,5.2512054443359375,5.276162147521973,5.0955810546875,4.997615814208984,4.982487678527832,5.166110515594482,5.338594913482666,4.6217875480651855,3.861504554748535,3.8590047359466553,-1.2248095273971558,-0.016124149784445763,5.773257732391357,-0.11203180998563766,5.175674915313721,5.9884819984436035,4.270731449127197,4.596199989318848,-6.25370979309082,-8.357288360595703,4.5225114822387695,3.5751001834869385,-0.10921704024076462,-2.43266224861145,4.574388027191162,-0.1326565146446228,5.619237899780273,3.2609503269195557,5.02194881439209,5.282896995544434,4.63856315612793,7.5299296379089355,5.7110514640808105,5.695101737976074,5.106597423553467,5.986785411834717,-8.359329223632812,4.638508319854736,4.638621807098389,5.916204929351807,5.087702751159668,4.9920654296875,5.280208587646484,2.3522891998291016,-0.01637554168701172,-3.9968976974487305,1.453382968902588,5.942033290863037,4.536689758300781,1.4515966176986694,1.451426386833191,-4.7212090492248535,4.799828052520752,5.8553900718688965,5.12107515335083,5.952157974243164,5.851206302642822,9.716340065002441,5.346251010894775,-5.8812103271484375,4.659102439880371,5.011868476867676,-5.883589267730713,-4.687151908874512,3.5794477462768555,1.0542417764663696,1.0573893785476685,5.086542129516602,4.638614177703857,5.257328033447266,5.1933064460754395,4.415365695953369,3.380025625228882,5.861252307891846,5.244811534881592,5.616235256195068,5.865341663360596,5.799161434173584,4.783007621765137,-0.9176501631736755,1.0929762125015259,6.290948390960693,1.0612972974777222,-2.4328901767730713,3.260762929916382,1.8861255645751953,3.2608940601348877,-4.639788627624512,5.6474175453186035,6.223474025726318,4.92567253112793,3.9552133083343506,3.9552159309387207,-1.0605525970458984,4.070712566375732,4.062106132507324,-0.6660216450691223,-0.665956437587738,5.356327056884766,-0.11154140532016754,-8.359062194824219,-1.4883805513381958,3.5963356494903564,5.758172035217285,-4.750100135803223,4.568268775939941,4.371297836303711,5.892175197601318,5.307647228240967,-0.11140317469835281,4.2571330070495605,6.358724117279053,5.273196220397949,-6.181751251220703,1.0732765197753906,-1.7286237478256226,-5.892195224761963,6.02303409576416,-5.90718412399292,-5.8854169845581055,6.101442813873291,6.0941972732543945,4.9936203956604,6.084739685058594,-5.297420978546143,7.197317123413086,6.0758771896362305,5.71265983581543,-1.4856775999069214,6.091014862060547,6.232603073120117,6.100027084350586,4.340142250061035,-6.647830009460449,3.53336501121521,4.249739646911621,-5.7305169105529785,5.949002265930176,-5.887277126312256,6.167524337768555,-6.303838729858398,10.036140441894531,-5.410210609436035,4.566552639007568,4.51997709274292,4.4570841789245605,-6.190506458282471,-6.329505443572998,-5.886023998260498,3.515259027481079,5.0199079513549805,-5.405437469482422,-6.966951847076416,5.955124378204346,2.3700549602508545,5.007551193237305,5.8173828125,5.420742511749268,5.759006500244141,2.3700788021087646,5.8557586669921875,3.1486282348632812,4.988004207611084,3.3195395469665527,4.246947288513184,-6.482933521270752,7.2068562507629395,4.4040937423706055,6.379415512084961,5.84464168548584,5.436030864715576,-5.873263359069824,5.197581768035889,-6.455236911773682,6.088255882263184,6.137353420257568,-5.3658976554870605,-5.365551948547363,-4.687376022338867,4.537315368652344,4.575497150421143,4.309565544128418,6.112882614135742,4.5197553634643555,10.05104923248291,5.7351155281066895,4.277876377105713,3.6017532348632812,10.0396146774292,-5.405660629272461,4.252959728240967,4.595450401306152,5.7865376472473145,6.97000789642334,3.5194103717803955,4.547203063964844,6.2735915184021,4.5574116706848145,-6.500857830047607,5.2573370933532715,5.05210018157959,4.243581771850586,2.555510997772217,0.061329007148742676,6.247589588165283,5.079367637634277,6.001175403594971,6.0045318603515625,5.254804611206055,4.902594566345215,6.345541954040527,5.294613838195801,5.003108024597168,4.301677703857422,4.914826393127441,4.015507221221924,10.059600830078125,-5.8850321769714355,5.1204657554626465,10.166993141174316,-6.287160396575928,4.07152795791626,4.572935581207275,3.6132748126983643,5.706568241119385,5.250363826751709,6.432068824768066,10.24136734008789,5.0278096199035645,4.584854602813721,4.5397491455078125,5.141657829284668,6.106660842895508,4.262691020965576,10.010159492492676,6.425756931304932,-5.886122703552246,-1.7286139726638794,4.582988739013672,3.560127019882202,-4.916900634765625,10.073205947875977,-5.423933982849121,4.530996799468994,-0.4748707115650177,5.931068420410156,5.065074920654297,2.3704569339752197,4.479145526885986,4.013769149780273,-5.878623962402344,-5.849003791809082,-6.189406871795654,4.213166236877441,6.451548099517822,4.296954154968262,5.615063190460205,-6.178188323974609,5.752968788146973,5.052527904510498,3.571215867996216,-7.538143634796143,6.090695381164551,6.097456455230713,5.9297003746032715,6.783499240875244,-5.397075176239014,5.8428826332092285,4.983353614807129,10.048212051391602,3.3923494815826416,5.124885082244873,3.148609161376953,-4.7283525466918945,0.8793062567710876,6.09246301651001,6.187669277191162,5.607208728790283,4.331878662109375,6.214441299438477,5.957956790924072,5.142272472381592,5.788666248321533,6.168463230133057,6.420844554901123,-6.198585033416748,5.180575847625732,6.397429466247559,6.398230075836182,5.277839660644531,3.3819737434387207,-6.758845806121826,-5.177644729614258,3.3966665267944336,4.368110656738281,5.272768497467041,-4.4562296867370605,5.984723091125488,5.347834587097168,5.350436687469482,4.361119270324707,5.237241268157959,6.007608413696289,5.191464900970459,5.781639575958252,10.230987548828125,-4.693763732910156,4.149575710296631,5.904132843017578,-0.9176076054573059,5.8168840408325195,5.98596715927124,4.824483394622803,6.201103210449219,-6.057528018951416,5.838028430938721,-5.907435894012451,6.281564712524414,6.2216572761535645,5.798431396484375,6.225327491760254,2.5554473400115967,-5.398334980010986,-1.2723020315170288,5.064957141876221,-4.266913890838623,6.2693023681640625,-5.169921875,6.27202033996582,-5.169919013977051,5.330666542053223,4.608139514923096,3.907480001449585,6.233329772949219,5.966964244842529,6.020904064178467,10.809042930603027,5.075297832489014,4.334923267364502,6.3001708984375,6.030910015106201,4.6082305908203125,5.7578206062316895,4.597406387329102,4.6081390380859375,10.809178352355957,10.171855926513672,4.608218193054199,6.26273775100708,1.0850162506103516,1.0863280296325684,6.271848201751709,10.19753646850586,3.660280466079712,5.819545745849609,-8.363754272460938,6.103522777557373,5.084787845611572,3.6588056087493896,5.272778511047363,6.349679946899414,6.355262279510498,6.135426998138428,6.060057640075684,4.885422229766846,4.366541385650635,6.417544841766357,4.608245372772217,6.259933948516846,10.809020042419434,6.264080047607422,-5.28956937789917,5.092066764831543,5.9159255027771,3.447202682495117,4.5700364112854,10.809097290039062,6.12375545501709,6.338259220123291,4.578928470611572,-5.877355575561523,-5.877589702606201,6.353461265563965,-6.966821193695068,-5.894497871398926,6.1343159675598145,6.244227409362793,-1.751494288444519,6.289767742156982,3.831022024154663,7.325181484222412,-5.6046833992004395,-1.2479774951934814,-2.002263069152832,-5.947662830352783,-1.4121859073638916,-4.97681999206543,-3.772388219833374,-1.3953090906143188,0.8044220805168152,-1.0929198265075684,-4.755316257476807,-3.0096914768218994,-3.7336173057556152,-0.6472240686416626,0.8518903255462646,-1.4133268594741821,2.8736448287963867,0.43852972984313965,0.43857985734939575,-8.986985206604004,2.744166851043701,2.7578163146972656,-1.0482155084609985,-0.9716486930847168,-0.5277919769287109,-8.561800956726074,-9.270596504211426,5.286931037902832,-4.958218574523926,2.2009332180023193,-0.4147289991378784,-0.5786131024360657,4.759552955627441,-2.51166033744812,-1.4455640316009521,-4.7169952392578125,-5.8330254554748535,9.602311134338379,9.60299301147461,3.7230818271636963,3.4878103733062744,-9.606925010681152,-2.570490598678589,-4.717559814453125,6.897838592529297,-1.1653498411178589,-1.0273685455322266,-5.095308303833008,2.777369976043701,-3.5862510204315186,2.115997314453125,-1.3512189388275146,1.8032089471817017,-0.22408129274845123,-1.252326488494873,0.02493535913527012,-5.475925922393799,-0.6576444506645203,-3.0920183658599854,5.758096694946289,3.008604049682617,-0.2161969095468521,-8.138670921325684,3.3410706520080566,-2.5291638374328613,1.6221705675125122,-3.6716058254241943,-3.6726009845733643,-0.6785303950309753,5.765295505523682,3.503739595413208,6.890562057495117,1.780417799949646,-4.211919784545898,-0.7812095284461975,-3.307054042816162,1.7362364530563354,2.225093364715576,0.20444531738758087,-1.4985425472259521,3.48022723197937,-5.628422737121582,-5.686301231384277,-0.20711451768875122,-0.4897480010986328,-5.128654479980469,-3.680248737335205,-0.9186489582061768,2.6757171154022217,-0.9539940357208252,-3.520057439804077,-0.6788223385810852,-8.562491416931152,-4.416752815246582,-4.603463172912598,-4.60407018661499,5.771825790405273,5.771753311157227,-7.5288496017456055,-3.4902334213256836,4.266213893890381,5.771378517150879,-3.716419219970703,3.908947229385376,-2.50685453414917,1.1897058486938477,-1.7038569450378418,-1.7083535194396973,-3.786494493484497,-5.582163333892822,3.2167699337005615,-0.5303645730018616,2.053717851638794,9.66439151763916,-1.2854279279708862,3.4423110485076904,6.114572048187256,-7.523568630218506,-0.6477514505386353,-0.6018504500389099,5.275341033935547,-0.8971074819564819,-2.2066049575805664,-0.410201758146286,-3.628784656524658,5.659165382385254,5.689035415649414,2.4594080448150635,-6.712733745574951,-0.2160574197769165,-0.7589349746704102,2.549321174621582,1.8366289138793945,-7.535622596740723,-3.2998368740081787,-1.2174891233444214,-6.230016231536865,8.852044105529785,2.2531378269195557,-0.7168907523155212,-5.203421115875244,-5.606461524963379,-3.362372398376465,-4.626680374145508,1.3742945194244385,-0.8186559081077576,0.0058386619202792645,0.5000118613243103,-1.7198785543441772,7.309310436248779,-10.507918357849121,-2.9238884449005127,8.205697059631348,-6.6338372230529785,-6.62336540222168,6.123071193695068,-0.4908703565597534,11.130234718322754,-5.164893627166748,-5.147643566131592,-0.8578348159790039,-4.607005596160889,-2.272066593170166,3.4411330223083496,3.052633285522461,-0.10902627557516098,-0.5865193605422974,10.16127872467041,-3.676025629043579,-0.8449681997299194,-1.4097342491149902,-1.4019606113433838,2.2543492317199707,1.9261739253997803,-4.430557727813721,1.6814477443695068,-6.061924934387207,-1.3730261325836182,0.6021118760108948,5.071460723876953,-1.0325493812561035,1.3668231964111328,7.115460395812988,10.402009010314941,-4.700948715209961,0.7967238426208496,-4.676673889160156,-3.8036482334136963,0.4563218653202057,2.0007715225219727,-0.6057231426239014,-6.188993453979492,-0.6655643582344055,1.5505197048187256,2.432204484939575,-0.43725547194480896,5.001309871673584,-0.5389878749847412,-0.7612826228141785,-3.7096643447875977,-0.21277529001235962,-5.977575302124023,-0.7277520895004272,-4.7804975509643555,-5.25658655166626,-1.4780418872833252,-1.6878021955490112,1.8021365404129028,7.428661823272705,-5.688903331756592,-5.589657783508301,-3.760215997695923,-0.9779167771339417,-3.071648597717285,-3.675938844680786,0.05455707386136055,0.06103944033384323,-1.702201247215271,-1.4183436632156372,-4.43369197845459,-5.63421630859375,-1.0893113613128662,2.069507122039795,7.444083213806152,2.0397939682006836,8.851845741271973,-2.216777801513672,0.3441322147846222,0.07307206094264984,0.07144691050052643,2.2525522708892822,-1.5265322923660278,-6.648345470428467,2.7345104217529297,0.6057634949684143,-3.6876144409179688,-3.697470188140869,-7.538076400756836,-2.3684494495391846,5.300079345703125,6.734936237335205,-1.3232426643371582,-5.904019832611084,-0.9904717206954956,2.237276554107666,-5.588088512420654,-0.9437048435211182,-0.5794777274131775,-6.900416851043701,-4.888481140136719,-3.229663848876953,3.4703142642974854,-6.092798233032227,-7.13470458984375,-1.3362298011779785,2.3249735832214355,4.666613578796387,0.6059602499008179,5.662639617919922,3.9472219944000244,-0.6084936857223511,-5.592066287994385,-0.9824625253677368,-5.030336380004883,-2.7862911224365234,2.508676528930664,-5.650331020355225,-0.6884304285049438,-1.6127855777740479,9.446698188781738,-6.612148284912109,1.7648754119873047,-5.874897003173828,1.9051430225372314,5.76832389831543,5.654543399810791,-10.581093788146973,-5.874544620513916,-0.5737422108650208,-3.672945499420166,2.0200932025909424,-1.2950329780578613,-4.502274513244629,0.023985106498003006,-5.58777379989624,-4.612663745880127,-4.612676620483398,-5.568984031677246,-10.50745964050293,-0.03767777979373932,6.403235912322998,-1.4052529335021973,-10.391716003417969,-10.391534805297852,-10.3917818069458,3.909212350845337,2.997621536254883,0.04323612526059151,1.7363321781158447,-3.2974510192871094,-5.8311357498168945,-4.772042751312256,-6.481069087982178,2.912741184234619,-0.4558870494365692,-3.8412222862243652,-0.555281400680542,4.2353596687316895,-0.009929493069648743,1.8292087316513062,-0.5876739025115967,-0.5271611213684082,-1.5963033437728882,0.4682634770870209,-1.6514489650726318,-1.1034666299819946,-3.840918779373169,-1.2481272220611572,0.8057558536529541,0.988736629486084,0.994482696056366,0.8056954145431519,-4.613251686096191,-6.731099605560303,2.147820234298706,1.567018747329712,6.452686786651611,3.7647171020507812,3.7724497318267822,-6.144076347351074,3.421274423599243,-3.7469675540924072,-1.5167118310928345,2.4819135665893555,-0.917923629283905,-3.2281501293182373,2.2203304767608643,-6.648097991943359,-1.486090064048767,-3.4335014820098877,-0.40742775797843933,4.660945892333984,-0.9444695711135864,0.19505810737609863,-1.678928017616272,-1.6928693056106567,-0.7717942595481873,-0.8964624404907227,2.3535666465759277,-4.409107208251953,-0.33723798394203186,7.171003341674805,0.7008063197135925,-0.6055678725242615,-1.3114104270935059,-0.21910282969474792,-1.616798758506775,-0.7992643713951111,-3.8654978275299072,5.890535831451416,-1.146234154701233,-1.3522274494171143,-1.2202306985855103,-1.4807143211364746,-6.135638236999512,2.2511708736419678,-0.012605567462742329,1.166205644607544,-0.5221893787384033,-1.90092134475708,-1.2103742361068726,7.114080905914307,-1.145632028579712,-4.246690273284912,2.4119882583618164,-6.644274711608887,-2.309685707092285,2.4889397621154785,1.4636554718017578,2.493920087814331,-1.326535701751709,-0.8781932592391968,-0.8698802590370178,3.1431989669799805,1.2584681510925293,9.94777774810791,-3.810995101928711,2.3830885887145996,-4.247936725616455,2.2485389709472656,-5.628208160400391,-5.143313884735107,-2.505246639251709,0.5764609575271606,0.1037008985877037,-4.613036155700684,5.963326930999756,-3.8318159580230713,-2.520796775817871,-1.4950162172317505,-5.620298385620117,1.1989582777023315,-1.6514508724212646,-1.471908450126648,-1.1915769577026367,-3.720245122909546,2.1760523319244385,-3.205331563949585,-6.125490188598633,-0.05963907390832901,-1.033117413520813,0.04541477933526039,-0.03445027396082878,-0.7586252093315125,3.941836357116699,12.298401832580566,-1.1078495979309082,6.649196147918701,7.367476940155029,2.718526601791382,1.489649772644043,1.8366113901138306,-0.26091188192367554,1.6973949670791626,-1.1093884706497192,-4.754123210906982,-1.0651220083236694,2.609875202178955,-6.469711780548096,-1.0708281993865967,5.784689426422119,-7.454950332641602,-1.4838696718215942,-1.484875202178955,-1.0892727375030518,-0.7849221229553223,1.4000710248947144,-5.593803882598877,-1.4297797679901123,-0.5915156602859497,1.0268326997756958,-1.1261075735092163,-1.311954379081726,3.451580047607422,9.665897369384766,1.697660207748413,-5.678055286407471,-0.7644649147987366,-1.9975119829177856,2.2568981647491455,3.551614284515381,3.3583109378814697,-0.626925528049469,-3.6866955757141113,4.887732028961182,5.835413932800293,1.9243870973587036,9.667241096496582,1.2307705879211426,-9.35780143737793,3.942850351333618,-1.690402626991272,-9.90616226196289,-0.8005267977714539,0.6601438522338867,-4.983272552490234,1.8381505012512207,1.9936100244522095,2.957653284072876,2.1792118549346924,9.947495460510254,-4.634016513824463,4.887740135192871,-3.4770631790161133,-1.0463756322860718,-5.742291450500488,2.598442554473877,4.712149620056152,4.711989402770996,-4.991137504577637,-4.992204189300537,-1.4710959196090698,4.711689472198486,3.7403676509857178,-0.7970608472824097,-9.62290096282959,0.11431004852056503,-0.029128940775990486,-0.8386967182159424,-0.5563392043113708,6.466740131378174,-1.3376469612121582,-0.8265582323074341,3.4834821224212646,1.1680988073349,-0.4426288604736328,0.06252487003803253,-3.0105972290039062,1.8782281875610352,1.9020692110061646,-0.013819277286529541,-1.306917428970337,3.376418352127075,-0.2408139556646347,-3.010634660720825,1.5808329582214355,3.3649799823760986,-4.028531074523926,3.3693535327911377,-4.550543785095215,1.5606285333633423,0.4615350663661957,-5.726042747497559,-4.992976188659668,1.8471975326538086,-5.722686767578125,0.47784677147865295,2.0058135986328125,1.9197076559066772,-1.4173002243041992,-7.135520935058594,-9.894559860229492,-0.758955180644989,1.8703522682189941,3.8059897422790527,-0.5463504195213318,-9.895140647888184,-7.136815071105957,3.5392098426818848,2.0179460048675537,-0.2380397468805313,2.134519338607788,2.0879337787628174,2.4840548038482666,-6.336120128631592,-5.632350921630859,2.332284927368164,2.548658847808838,-0.5511875748634338,8.851945877075195,0.4682258665561676,-7.135054588317871,1.5615445375442505,-6.3384623527526855,-3.703679084777832,-1.0658972263336182,-0.2321803718805313,-4.021874904632568,-5.797244548797607,-5.79723596572876,1.9092825651168823,-4.010631561279297,-0.5226391553878784,-8.138688087463379,3.3552565574645996,-1.2711471319198608,-4.5516462326049805,-4.56439208984375,2.0013275146484375,1.0639337301254272,2.482893943786621,-9.896848678588867,-1.1081074476242065,1.059865951538086,-4.5473151206970215,-1.0586429834365845,-4.295987129211426,9.60663890838623,1.6364996433258057,-0.3908572494983673,-0.5535377860069275,-0.546696662902832,3.27594256401062,3.4925386905670166,-1.1599483489990234,6.803369522094727,1.1061546802520752,-4.199024200439453,-4.53006649017334,-3.682400703430176,-4.545528888702393,-5.660155773162842,-6.995711803436279,-1.405248761177063,1.4815751314163208,-1.0882459878921509,-6.3243727684021,-4.5450849533081055,-9.894935607910156,7.394597053527832,-3.8686275482177734,7.0245537757873535,6.357791900634766,-1.4380625486373901,2.604252338409424,-3.81001353263855,5.799914360046387,-0.8060151934623718,-3.875681161880493,-1.5136348009109497,-0.7042497992515564,0.08939190208911896,-4.754123210906982,2.0548131465911865,2.594383716583252,0.09441424161195755,-0.9915238618850708,0.5198060870170593,-0.9420477747917175,-1.4788827896118164,-0.8964157104492188,-0.8215439319610596,0.603881299495697,2.5802488327026367,1.5615313053131104,2.978722095489502,-3.3519551753997803,-2.3096914291381836,-0.8877989649772644,-5.649190425872803,0.47270965576171875,0.5507451891899109,-5.653730392456055,2.384399652481079,-2.177065372467041,-5.678011417388916,-4.530040740966797,5.49990177154541,-9.967114448547363,-5.5931243896484375,-0.6562615633010864,3.4278206825256348,-1.039961338043213,-0.9434717893600464,-2.309692859649658,-1.4937459230422974,-5.1426873207092285,2.9755942821502686,-3.7086305618286133,2.9753355979919434,0.7967027425765991,2.9787213802337646,2.3705594539642334,1.3423386812210083,-0.896750807762146,-4.53005313873291,-0.21723408997058868,-1.6878432035446167,-6.119315147399902,6.006356239318848,2.220236301422119,5.309594631195068,-1.0355379581451416,-3.782125234603882,4.854537487030029,5.47606897354126,-6.460364818572998,-6.463529586791992,-1.4933221340179443,-0.3847089409828186,-4.2282209396362305,0.4995872378349304,-3.8611958026885986,1.9553946256637573,0.08339819312095642,0.5068268179893494,1.9056867361068726,0.5066298246383667,0.562448263168335,-0.4378240406513214,-1.6515450477600098,11.127338409423828,2.2307825088500977,-3.82601261138916,-3.91884446144104,-2.506566047668457,-0.3112601637840271,9.70550537109375,3.4805691242218018,-1.5991158485412598,-4.838342666625977,2.5484700202941895,2.5601699352264404,-0.43255335092544556,-1.4199713468551636,-1.558849573135376,2.1302273273468018,-4.771637916564941,-4.6315412521362305,-0.38897064328193665,-0.36304566264152527,-0.1302616447210312,3.524752616882324,-1.0903022289276123,2.722506046295166,3.428617477416992,9.947685241699219,-1.2110539674758911,-4.931924343109131,1.4587066173553467,-0.2131960391998291,-1.4140712022781372,-1.085759162902832,1.7000826597213745,5.790900707244873,-3.8050715923309326,-0.5929141044616699,7.176229476928711,-7.134659290313721,-0.896726667881012,3.4690053462982178,-1.1679199934005737,-4.647405624389648,6.006336688995361,-4.4281792640686035,-1.207062840461731,1.9284902811050415,-0.7530996799468994,-3.0105643272399902,-5.134832382202148,-0.9931360483169556,0.45522576570510864,-0.6482179164886475,-6.37074613571167,-3.7306909561157227,-3.637497663497925,-0.6417410969734192,-1.4996892213821411,3.428051233291626,-0.010088087059557438,1.7816096544265747,-0.9025744199752808,-1.2692885398864746,-1.050923228263855,-3.7484660148620605,0.046632539480924606,4.295949935913086,-3.7033579349517822,-1.4970042705535889,-1.3938660621643066,-1.1978260278701782,-3.8317711353302,-3.905851364135742,-1.3327271938323975,5.521982192993164,1.4979760646820068,5.898273468017578,3.4631567001342773,-0.9972121119499207,-4.572964668273926,2.997609853744507,0.728756844997406,0.42384082078933716,-5.51747465133667,5.8675761222839355,-2.163940668106079,-3.2886228561401367,-0.8511563539505005,2.7326810359954834,-1.6232177019119263,-1.6211847066879272,1.6795175075531006,-6.4162750244140625,-0.738027036190033,-1.0903220176696777,-0.30503782629966736,-5.499930381774902,2.0732672214508057,-0.8216445446014404,-5.011927604675293,-4.748389720916748,-0.36381641030311584,-5.583208084106445,1.7189414501190186,1.7485202550888062,-2.7864322662353516,2.717992067337036,-1.3302313089370728,1.7945542335510254,-6.492917060852051,-6.431975364685059,2.596712112426758,-6.995916366577148,1.6453441381454468,-5.4763689041137695,-2.2607362270355225,-0.1450013965368271,-2.5055510997772217,-5.063199520111084,-1.4875729084014893,-0.9700591564178467,1.4768134355545044,1.0907455682754517,-1.0494149923324585,-0.5383822917938232,-5.76385498046875,1.7601374387741089,2.4651949405670166,-3.885507106781006,-7.134179592132568,-2.3095035552978516,-0.7251160740852356,3.3407411575317383,-1.900709629058838,9.668635368347168,9.669034004211426,0.022968143224716187,2.221015214920044,-6.662638187408447,3.4180843830108643,-4.753028392791748,2.606140375137329,-0.28627628087997437,-0.42946431040763855,0.5219263434410095,3.9092228412628174,-1.1906625032424927,-4.991156578063965,-4.6534552574157715,-4.29744291305542,-5.032454013824463,1.6424132585525513,-9.262134552001953,1.7853894233703613,0.8689154386520386,3.4492995738983154,-5.592387676239014,5.99866247177124,8.205635070800781,-3.759505271911621,-1.4707690477371216,-4.438839912414551,3.697226047515869,-1.491868257522583,0.09273628145456314,0.839998185634613,-5.026904106140137,-4.545603275299072,0.6080446243286133,6.111371994018555,2.7719743251800537,1.3697841167449951,1.5951777696609497,9.667649269104004,3.4272639751434326,0.9573532342910767,5.395353317260742,1.6822524070739746,-6.453976154327393,-0.3207928538322449,-6.454471111297607,0.5668970346450806,-1.0830398797988892,3.4247779846191406,-5.163075923919678,-0.3269044756889343,-1.5486575365066528,2.460777997970581,-1.4875664710998535,-4.850062370300293,-0.5832357406616211,1.9333447217941284,-1.4750713109970093,3.450723648071289,-3.885981321334839,2.716564893722534,-1.1169458627700806,-1.337018609046936,-1.8712388277053833,-0.5403421521186829,-6.897917747497559,-5.14538049697876,-6.712707996368408,-6.899210453033447,3.426846504211426,1.2850854396820068,-5.321713447570801,-4.393704414367676,-0.29677167534828186,-3.9415528774261475,7.558627605438232,-6.941860675811768,5.292054176330566,1.6376737356185913,-0.37376976013183594,-1.0613747835159302,-3.6713829040527344,-0.3979044556617737,-1.6118006706237793,-1.541074514389038,-4.9417405128479,-3.7101588249206543,-1.3665071725845337,-0.9014891386032104,-0.5982711315155029,-1.863940715789795,-6.9005351066589355,1.3005677461624146,1.7960593700408936,-6.9723029136657715,-4.85029411315918,0.41905394196510315,5.641379356384277,-0.5283092260360718,1.6058083772659302,-2.356964111328125,0.35803574323654175,-4.656879425048828,1.6785780191421509,-1.476254940032959,3.5115809440612793,3.489339590072632,-8.987052917480469,-6.267000198364258,7.662568092346191,5.456384181976318,-1.0787969827651978,3.930209159851074,1.312274694442749,1.9888445138931274,-5.112175941467285,0.4261975586414337,-8.986747741699219,-1.1062273979187012,-6.060903549194336,0.938209056854248,1.0481443405151367,4.794351100921631,-0.675858199596405,-3.8074631690979004,1.2607345581054688,-4.7036895751953125,-1.3495179414749146,-0.9528796076774597,2.256662130355835,-1.8888319730758667,2.603269577026367,0.5308716297149658,-1.8593146800994873,-4.098316669464111,5.96336030960083,5.963345050811768,3.4407896995544434,7.466426849365234,-6.989682197570801,3.4260616302490234,2.0143563747406006,-1.10552179813385,2.4087018966674805,0.5520384907722473,3.9438650608062744,-5.473516941070557,3.768644094467163,3.1442325115203857,4.033222198486328,-3.8300414085388184,0.6432381868362427,2.378861665725708,5.984549045562744,-1.3414273262023926,-9.357797622680664,-9.260289192199707,-0.013311550952494144,-5.545839309692383,-5.5283098220825195,-3.787672281265259,2.4724416732788086,-1.8963325023651123,4.423796653747559,-1.394217848777771,-0.6323683261871338,7.375382900238037,-6.645912170410156,-6.645336151123047,-0.9197661876678467,1.3343098163604736,-0.9380027055740356,1.071830153465271,-1.5791701078414917,-3.4565443992614746,1.5398787260055542,7.3793535232543945,1.065521001815796,7.581026554107666,-1.3233144283294678,1.239907145500183,-3.8490805625915527,1.8774759769439697,7.6626081466674805,-3.744141101837158,-3.755380392074585,-1.3654990196228027,-3.2062222957611084,0.1128377765417099,-0.8094280958175659,4.805377006530762,0.43195492029190063,-1.3453054428100586,1.4883921146392822,-5.382917881011963,4.947937965393066,2.6690053939819336,8.843415260314941,9.540559768676758,2.1367955207824707,-1.3572921752929688,-0.7391130328178406,-0.7977940440177917,-0.7922335267066956,-5.403242588043213,-4.33745002746582,2.7173357009887695,-1.4794483184814453,0.6324897408485413,-3.1994783878326416,3.41534423828125,-0.34338220953941345,-1.7327802181243896,1.071614146232605,-4.530084133148193,3.7387993335723877,-5.420886516571045,0.46200841665267944,-3.203819751739502,1.0631378889083862,-2.3132193088531494,-1.7182127237319946,-0.8036513924598694,0.6054005026817322,6.050473213195801,-6.658465385437012,-0.729419469833374,1.7119611501693726,-3.5283308029174805,2.8706765174865723,-5.577327728271484,-2.0575108528137207,2.872431755065918,-8.573609352111816,-0.8217633366584778,2.872443914413452,-1.8651834726333618,7.43856143951416,-1.4637477397918701,-4.337347507476807,-0.9437384009361267,-1.4001051187515259,-5.529637336730957,-3.234779119491577,-2.0214622020721436,-4.495575904846191,-4.709011554718018,-5.444773197174072,-10.391809463500977,-4.70619010925293,-0.5224466919898987,-1.8712719678878784,-0.33156487345695496,-0.013596607372164726,-5.439187049865723,1.3823909759521484,2.544877290725708,6.649231910705566,-2.486957550048828,2.223909854888916,0.4661066234111786,-0.8337004780769348,-1.2989295721054077,-1.2852555513381958,2.4203381538391113,7.358268737792969,8.849038124084473,6.649228572845459,-1.1834032535552979,-2.982142448425293,8.849889755249023,8.851723670959473,1.4134650230407715,4.490945339202881,-0.5532070994377136,4.759413242340088,4.354257583618164,-1.401026964187622,2.3139123916625977,-0.7078946828842163,2.6408045291900635,-0.5286718606948853,-4.214685440063477,-0.9710537195205688,0.5228995084762573,-5.066333770751953,1.8739657402038574,-6.648132801055908,-3.8898026943206787,3.7328145503997803,9.446842193603516,5.30051326751709,-0.5225408673286438,-0.7371525168418884,1.462859869003296,0.6664250493049622,3.991830348968506,-0.7737258076667786,1.5048893690109253,-0.3432391583919525,-0.8040651679039001,1.5306321382522583,2.200918674468994,-2.964874267578125,0.3867802917957306,2.4604263305664062,-0.4796499013900757,-0.781333863735199,0.46725884079933167,-0.7554636001586914,-0.25837936997413635,2.0000834465026855,-0.8197909593582153,4.599661350250244,-4.502279281616211,9.845333099365234,-5.615480899810791,1.1541519165039062,-0.8095725774765015,-1.9887473583221436,7.255244255065918,6.963106632232666,6.467207431793213,-1.9100669622421265,-1.442100167274475,0.1475764364004135,-2.0753211975097656,0.08313662558794022,-0.5931694507598877,-0.7624476552009583,0.6550388932228088,3.3490657806396484,2.5071136951446533,-4.691651821136475,-1.3322882652282715,1.1345605850219727,-0.37055477499961853,-3.7727413177490234,3.7065000534057617,7.444596290588379,1.576436996459961,-0.44348499178886414,-0.2192421704530716,1.4802665710449219,7.496267795562744,-5.870353698730469,2.2542431354522705,-0.5064473152160645,3.0553781986236572,-0.8001654744148254,-0.8001080751419067,-5.474501609802246,-1.866146445274353,-1.8280376195907593,-0.26607248187065125,2.1014182567596436,-0.016829106956720352,7.456849575042725,-2.9626827239990234,4.822085380554199,3.3512191772460938,3.351318597793579,-6.26298713684082,-5.016761779785156,0.5890804529190063,1.7268844842910767,-0.8910348415374756,-0.8753584623336792,5.167079448699951,1.994942307472229,6.035497665405273,2.5973494052886963,3.825294256210327,-0.8002168536186218,-3.5969533920288086,-3.813441514968872,-2.744467258453369,-3.680476665496826,1.8594056367874146,2.856607437133789,5.515483379364014,2.4726641178131104,-0.8906566500663757,2.4743540287017822,3.119246244430542,-5.930588722229004,-0.5110848546028137,-4.634300231933594,-0.7805355191230774,-1.5092394351959229,3.4230055809020996,-0.2855793833732605,3.25677752494812,-8.573351860046387,-9.242669105529785,-3.1742260456085205,-10.528132438659668,-0.2608429491519928,-10.52747917175293,5.499831676483154,0.42116639018058777,6.467291355133057,1.2916377782821655,-4.617309093475342,-3.210951566696167,-0.5112046599388123,2.9748852252960205,2.4110851287841797,-1.8756436109542847,-1.0484760999679565,-1.0464826822280884,-1.6841309070587158,2.233921766281128,-4.43890380859375,1.534379482269287,1.1523447036743164,-2.533787727355957,0.36867767572402954,2.9787278175354004,-1.396562933921814,1.6694140434265137,3.501216173171997,6.124000549316406,-1.3418618440628052,-0.9040131568908691,-0.8944793939590454,-1.6638023853302002,3.159637928009033,2.398613691329956,6.467389106750488,-9.966861724853516,2.1442365646362305,2.847799301147461,2.8139867782592773,1.8364317417144775,-2.2090134620666504,-4.941789150238037,-0.3264939486980438,-0.32324275374412537,5.1300950050354,-6.112005710601807,-3.2047009468078613,2.3352982997894287,-0.30592915415763855,4.277019500732422,1.2964988946914673,-1.4334741830825806,2.4121828079223633,1.7827731370925903,1.6017175912857056,-0.576107919216156,3.059093475341797,-0.947827160358429,-4.029411792755127,2.0067262649536133,-4.225633144378662,-2.659459352493286,-4.830832004547119,2.8939480781555176,-0.7160714268684387,-4.963537693023682,-0.7191216945648193,2.6414074897766113,7.365477561950684,-1.126301884651184,-1.1263151168823242,7.194848537445068,2.3322925567626953,-0.5418623089790344,-2.073546886444092,0.6387720108032227,-0.40684574842453003,-0.5146149396896362,-0.5135349631309509,2.559370517730713,-1.5020043849945068,-0.7371470332145691,-1.090816617012024,-1.031601905822754,4.0345940589904785,1.1433426141738892,-5.77402400970459,-10.538436889648438,-1.0219537019729614,-1.4484366178512573,5.428099155426025,-10.538129806518555,4.789231777191162,2.529775857925415,-5.4288835525512695,-0.42700591683387756,-4.251959323883057,-5.5811686515808105,-0.9952927231788635,-1.9020373821258545,2.559412717819214,2.6358249187469482,-1.3189690113067627,-0.5629634857177734,0.04157894849777222,6.669947147369385,-1.5382814407348633,2.4412338733673096,1.7749040126800537,1.997722864151001,2.089721441268921,-1.6146845817565918,-1.253773808479309,-1.2169371843338013,-5.123267650604248,6.356539726257324,6.12570858001709,3.615342855453491,-2.628260612487793,-0.14242671430110931,-1.394223690032959,13.37166690826416,-0.49557119607925415,-1.6536275148391724,6.107120037078857,0.5584369897842407,4.682697772979736,-2.5022904872894287,3.1708412170410156,2.0741515159606934,-0.5446289777755737,3.0528719425201416,1.479403018951416,-0.42555099725723267,-0.44278231263160706,-0.9609149098396301,1.1414240598678589,3.944283962249756,-1.6157724857330322,5.620363235473633,5.752752780914307,1.1769142150878906,3.9409193992614746,5.64716100692749,2.0805022716522217,-3.204617738723755,6.146846294403076,1.383819341659546,1.9658139944076538,7.250681400299072,5.160721778869629,3.0570521354675293,1.9038763046264648,6.127371311187744,-0.5375651121139526,2.6676249504089355,4.594812393188477,-0.5217792987823486,0.9424974918365479,3.6079211235046387,-3.8910558223724365,-2.8261806964874268,2.6517276763916016,4.0056843757629395,2.6279401779174805,-1.4629966020584106,3.9790611267089844,-0.35532060265541077,0.8946467638015747,-0.3173695206642151,-0.3595457077026367,4.950323104858398,-1.91776442527771,-0.16251127421855927,3.593187093734741,-0.1831723004579544,-4.458734512329102,-5.472290515899658,-0.4837816059589386,1.46611487865448,2.1161320209503174,-2.584216833114624,-3.0920615196228027,-10.294756889343262,1.2468310594558716,-0.32484564185142517,4.491032123565674,4.795178413391113,3.082327127456665,1.817863941192627,-5.942803859710693,3.5946078300476074,-8.57374095916748,-0.7620506286621094,-8.573668479919434,-10.530001640319824,-3.6730494499206543,1.1780976057052612,1.9184882640838623,-0.47896432876586914,0.4471805691719055,1.9047138690948486,6.121371269226074,-5.427023410797119,-1.4684253931045532,-5.451354503631592,-0.6127598285675049,2.630504846572876,1.989463210105896,3.208014488220215,2.157717227935791,-0.7489027380943298,-0.7337944507598877,-2.5478732585906982,2.1473028659820557,2.15728497505188,2.0288188457489014,1.7817384004592896,0.7137194275856018,0.7138466835021973,1.7704317569732666,-0.41801783442497253,6.686720848083496,-3.5765810012817383,-5.003503322601318,-0.35899993777275085,7.463992118835449,11.129509925842285,2.7632641792297363,-1.3923557996749878,-0.7200543880462646,-0.7587621808052063,-0.7677674889564514,3.614150047302246,1.5495103597640991,-5.338648796081543,-5.143193244934082,-5.28799295425415,3.8095736503601074,0.8152008056640625,-1.9117226600646973,1.4775573015213013,4.659968852996826,1.679486870765686,6.119524955749512,2.0188021659851074,1.9480206966400146,6.128235816955566,-1.3665236234664917,0.6247444152832031,4.354941368103027,-1.8629571199417114,4.037435531616211,0.3769257366657257,0.11374320834875107,-9.60695743560791,-1.8894946575164795,-4.373556613922119,2.1511926651000977,-0.8814019560813904,1.6792356967926025,-3.106973886489868,3.2149975299835205,0.4519081115722656,3.1537768840789795,2.152191638946533,-1.0489213466644287,-1.8882075548171997,-3.2255382537841797,-0.7887626886367798,-0.9299643635749817,-0.3167954087257385,-9.236665725708008,0.6749134659767151,-0.7685330510139465,-1.6156251430511475,-9.139540672302246,-0.32925093173980713,-0.34469082951545715,3.3223743438720703,1.7122185230255127,-2.836209297180176,-3.640648126602173,1.9488725662231445,-2.7435038089752197,2.960115671157837,3.0915422439575195,-0.568412184715271,-0.46032965183258057,1.1279503107070923,-2.528383255004883,-2.9190845489501953,2.5850913524627686,5.102961540222168,-0.5267748236656189,-5.773858070373535,5.139228820800781,1.5228089094161987,-0.6769811511039734,3.345360040664673,-3.2046852111816406,-9.966699600219727,-3.2817065715789795,-1.8930174112319946,-4.734052658081055,2.5869433879852295,0.46139001846313477,5.140931606292725,-0.45487770438194275,-9.228755950927734,4.38558292388916,-0.707403302192688,-1.4946128129959106,1.97175931930542,-6.424810409545898,-3.1742756366729736,2.0728306770324707,-6.374880790710449,7.259995937347412,2.274876117706299,-0.19774603843688965,7.090790271759033,-1.1168173551559448,-0.7272524833679199,-0.516753077507019,2.5885727405548096,-0.043950676918029785,5.521328926086426,5.468613147735596,-3.7240376472473145,-0.2584035098552704,-3.6774580478668213,3.34399676322937,-1.3437787294387817,4.19490385055542,7.240999698638916,3.0659284591674805,7.236530780792236,1.9600132703781128,4.161147594451904,-0.34004539251327515,-1.2987381219863892,-0.5157604217529297,-1.8619849681854248,-0.6329922676086426,-3.6748178005218506,4.794469833374023,-3.1168198585510254,-9.966520309448242,1.9894862174987793,6.209681510925293,-0.5857467651367188,-1.9068491458892822,-3.693265676498413,-3.202885627746582,-4.536870002746582,-4.536969184875488,-0.5707027912139893,-3.65191388130188,3.440988063812256,1.467300534248352,-1.8638451099395752,-4.701296806335449,-6.437061786651611,-4.706766605377197,4.23683500289917,-1.6474939584732056,-0.5369710922241211,2.4194118976593018,2.00891375541687,6.1619133949279785,-5.3460893630981445,8.205554008483887,4.643632888793945,4.234220027923584,-0.7599300742149353,-6.029425144195557,-1.405456304550171,4.868884086608887,2.080976724624634,2.1214921474456787,2.1927053928375244,-0.7091957330703735,-6.3072967529296875,-3.4429516792297363,5.998626232147217,4.215655326843262,-0.5707971453666687,5.339168071746826,9.845471382141113,-4.960541725158691,1.3857181072235107,-10.294794082641602,5.16519832611084,-1.1066714525222778,-0.5429906249046326,-8.558690071105957,-1.3059391975402832,3.9957568645477295,4.853394508361816,1.9258846044540405,-5.925920009613037,3.18271541595459,-3.0877280235290527,-0.9864960312843323,1.5859851837158203,2.0228688716888428,1.5860660076141357,-5.039452075958252,-0.023656848818063736,-0.024007054045796394,1.2716938257217407,4.225557804107666,-0.02343921549618244,-0.02141282707452774,-1.4015761613845825,2.1598598957061768,4.836329936981201,2.6360065937042236,4.5487518310546875,0.6318172812461853,0.9852993488311768,0.9679609537124634,6.315321922302246,2.158851146697998,-0.3664989173412323,-1.488039255142212,2.1718993186950684,3.1054434776306152,-1.8622174263000488,-5.597233295440674,0.43964794278144836,3.0092477798461914,-2.6432418823242188,-3.206685781478882,-3.205617904663086,-0.9572221040725708,-0.3636614978313446,6.741469860076904,2.7069764137268066,-1.4813365936279297,4.906737804412842,-1.3319580554962158,-0.32961341738700867,-9.537374496459961,-0.5735433101654053,7.4996747970581055,4.8615899085998535,-1.3182895183563232,3.8020308017730713,-4.64676570892334,-0.6181215047836304,6.1070733070373535,-3.1979005336761475,-0.5665796399116516,2.7740135192871094,7.793963432312012,11.129483222961426,7.662603855133057,1.8741819858551025,-3.1724154949188232,3.716083526611328,9.446304321289062,-2.0587239265441895,-0.3687998056411743,-2.277965545654297,-0.8952731490135193,-5.625858306884766,-0.5110679268836975,-5.620530605316162,-5.630266189575195,-5.93050479888916,3.0117173194885254,4.610312461853027,-1.9152899980545044,-6.289339065551758,-0.06056997552514076,0.019705671817064285,-3.6796083450317383,-2.209388494491577,1.1054599285125732,1.6077936887741089,2.8739748001098633,-0.7508172988891602,-4.965427875518799,-6.141343593597412,-0.5170471668243408,-4.645517826080322,-0.08499035984277725,-0.6847431659698486,-0.6873969435691833,2.6509275436401367,-1.035666584968567,-0.6686734557151794,-0.7981480360031128,-3.6942152976989746,2.7369673252105713,-0.8855890035629272,-0.2046418935060501,-0.7925050854682922,-8.985343933105469,-0.043948087841272354,6.160464763641357,0.0627996176481247,-1.2734365463256836,-4.059278964996338,-1.4312776327133179,-0.7573993802070618,-1.2646946907043457,-1.2452373504638672,2.049103021621704,-0.7503219246864319,-3.720919132232666,-3.7327287197113037,6.753347873687744,-4.536525726318359,-5.312506198883057,-0.5102214813232422,-0.9623081088066101,-1.917969822883606,-1.0493887662887573,-5.544741630554199,-3.0868802070617676,-3.1937785148620605,-0.576433002948761,3.649742364883423,-0.8947848677635193,-0.6384187936782837,2.1710894107818604,7.455064296722412,-9.60693359375,4.278580188751221,1.5431005954742432,6.711151599884033,-0.4615071415901184,4.825442790985107,-2.4520652294158936,-0.0262759942561388,6.072272300720215,6.107085704803467,-3.6853160858154297,-4.5023112297058105,1.932813048362732,6.066825866699219,0.7028965950012207,-6.14047908782959,-3.0105478763580322,8.852137565612793,-1.5376263856887817,4.791940212249756,1.702356219291687,7.428684711456299,-1.5012822151184082,-1.4867290258407593,0.24936896562576294,-1.4700496196746826,8.239592552185059,2.7747697830200195,-0.7028741240501404,4.838048458099365,-2.7430191040039062,1.525417447090149,-4.945110321044922,-1.3300762176513672,7.253079414367676,7.253730773925781,-0.7850852012634277,-0.2693891227245331,3.269848346710205,0.7024290561676025,3.8001623153686523,0.23087039589881897,-5.032832145690918,-0.7390053868293762,-0.6471025943756104,3.9434657096862793,-2.9308176040649414,-0.6951831579208374,-3.2429232597351074,-0.5840962529182434,-0.6951256990432739,-3.24283504486084,2.6671557426452637,-0.8235837817192078,-0.7962953448295593,-5.44632625579834,0.41267505288124084],"type":"scatter3d"},{"hovertemplate":"label=quality_7\u003cbr\u003eproj0=%{x}\u003cbr\u003eproj1=%{y}\u003cbr\u003eproj2=%{z}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"quality_7","marker":{"color":"#00cc96","symbol":"circle"},"mode":"markers","name":"quality_7","scene":"scene","showlegend":true,"x":[1.9809552431106567,-1.5439835786819458,2.480337142944336,4.216349124908447,2.7408905029296875,3.3303110599517822,-1.8338205814361572,4.43057918548584,3.8422744274139404,5.008541107177734,6.247141361236572,4.138040542602539,5.069498538970947,-4.867152690887451,5.166419506072998,10.963343620300293,3.2714807987213135,-2.053492307662964,3.3467490673065186,2.3572847843170166,3.3520309925079346,5.470128059387207,1.733564019203186,4.182761192321777,4.063981533050537,3.967813730239868,4.7588911056518555,3.348118543624878,4.8912153244018555,3.6989359855651855,3.8486576080322266,4.537752628326416,-2.0535223484039307,1.5401501655578613,4.778330326080322,3.952714204788208,-3.3117406368255615,-1.9428731203079224,3.832258701324463,5.214323043823242,3.3105742931365967,3.30448579788208,5.276461124420166,4.783911228179932,4.780387878417969,3.4000356197357178,4.7823615074157715,3.845043659210205,3.3402600288391113,3.9111785888671875,4.500879764556885,5.2322282791137695,4.80306339263916,4.613030433654785,3.4031012058258057,-1.4506070613861084,-4.959186553955078,1.9535603523254395,4.765102863311768,-3.063807249069214,-1.9454972743988037,1.9025533199310303,1.7996386289596558,6.1539435386657715,1.6733291149139404,6.5031819343566895,-3.407456159591675,3.6041195392608643,1.748813271522522,1.4150795936584473,-4.867525100708008,-4.210789680480957,-3.3122057914733887,-3.2894704341888428,5.01688289642334,2.0110998153686523,2.6611316204071045,2.4862208366394043,-3.2770121097564697,1.3477987051010132,1.7827857732772827,1.99721360206604,3.3413021564483643,-4.126683235168457,4.892333507537842,4.894599914550781,-3.120940685272217,4.669641017913818,4.507780075073242,-3.1113829612731934,1.6563297510147095,1.66576087474823,3.573530435562134,2.170053720474243,-3.2158472537994385,-3.305756092071533,1.6121952533721924,2.4805421829223633,-4.201291561126709,1.8449417352676392,2.5414373874664307,-3.054659366607666,-3.4049465656280518,2.2918410301208496,-3.4023489952087402,-3.1686763763427734,-3.1096935272216797,1.8832534551620483,-1.1736630201339722,-1.1743414402008057,-1.1740931272506714,4.815494060516357,2.001081943511963,1.9578230381011963,5.157433032989502,1.9097453355789185,-3.276987075805664,5.289296627044678,1.909942865371704,4.615405082702637,-3.0650248527526855,1.3472424745559692,4.981561183929443,4.981509685516357,1.6392481327056885,4.752810001373291,-3.3932361602783203,2.107577085494995,-3.4223833084106445,2.0660862922668457,-4.2154998779296875,1.9759798049926758,2.769789218902588,1.7116527557373047,-4.185542583465576,1.872578740119934,2.4901609420776367,-1.9866259098052979,3.8318681716918945,-3.0856034755706787,2.1144328117370605,1.801123857498169,1.5075417757034302,2.4865925312042236,10.148303985595703,10.148138999938965,6.174925327301025,-4.204581260681152,-3.0786168575286865,2.6574454307556152,2.862835168838501,2.2932097911834717,2.0015721321105957,9.107547760009766,-1.7240415811538696,-3.285930871963501,2.0229105949401855,1.6093993186950684,5.863781452178955,-1.9878790378570557,-1.9761801958084106,9.10745620727539,1.911612868309021,1.445044755935669,-3.0771305561065674,2.6694679260253906,2.019815444946289,6.432313442230225,2.8880069255828857,-1.457263708114624,-0.07080981880426407,4.660098075866699,4.233319282531738,5.176049709320068,5.881570339202881,5.417515277862549,-6.645982265472412,-1.6953669786453247,-1.705546498298645,-1.2737925052642822,-1.7445257902145386,6.367532253265381,4.552242279052734,2.9361681938171387,-1.4803818464279175,-1.6041346788406372,-2.45361328125,-0.5124731659889221,5.858590126037598,2.9287374019622803,6.501121997833252,7.811910629272461,-1.0720566511154175,0.08774984627962112,5.011215686798096,5.001678466796875,5.666496753692627,-6.580456733703613,4.86836576461792,7.8089470863342285,5.897266387939453,2.4651248455047607,-1.1965047121047974,6.104944229125977,3.479506254196167,4.534017562866211,6.495887756347656,4.742123603820801,11.389190673828125,-3.6261463165283203,3.2141451835632324,-1.0698109865188599,-2.593919277191162,6.723481178283691,2.341566562652588,6.00534200668335,6.26777458190918,7.360236644744873,-2.135354995727539,-2.4739749431610107,5.031734466552734,-0.1832755208015442,-2.0093250274658203,2.8879852294921875,7.249213695526123,6.298579692840576,6.724808216094971,3.5611374378204346,4.885443687438965,-2.331477642059326,3.313464403152466,5.034726619720459,0.7682819962501526,7.237776279449463,-0.7912318110466003,-1.7652112245559692,-2.3192739486694336,4.708822727203369,3.034224033355713,6.465425968170166,-2.4970362186431885,-1.2037376165390015,-2.500242233276367,-2.045874834060669,7.2411298751831055,6.724681854248047,7.336204528808594,6.638296604156494,-6.509575366973877,-0.201651930809021,0.1524435132741928,1.9701181650161743,4.547701358795166,11.354413032531738,5.993292808532715,5.9682111740112305,5.9717326164245605,5.436673641204834,2.921086072921753,4.78542423248291,-1.1500662565231323,7.631649494171143,-3.0821683406829834,3.0069496631622314,1.4408351182937622,-4.358508110046387,-3.87960147857666,4.524045467376709,4.520834445953369,5.189230918884277,4.88104772567749,1.3129241466522217,1.3283473253250122,-1.8497967720031738,5.322587013244629,4.355124473571777,0.8749544024467468,-1.9053395986557007,-0.14702634513378143,1.3567131757736206,12.860011100769043,12.860054016113281,4.610882759094238,2.925567150115967,4.487428188323975,1.270318627357483,-3.9467570781707764,-6.32225227355957,-6.322310924530029,-1.6785622835159302,-1.201368808746338,4.404031753540039,6.014777660369873,6.064657688140869,4.746670246124268,4.901296138763428,-0.7574287056922913,0.1923234462738037,0.1051005944609642,12.525097846984863,4.527464389801025,6.180549621582031,-0.43521276116371155,0.853795051574707,0.22647181153297424,2.0581412315368652,2.0580084323883057,-1.1724499464035034,4.250268936157227,4.7400054931640625,-0.7892271280288696,-2.5074169635772705,10.207402229309082,12.860047340393066,0.06759490817785263,0.1244354322552681,6.90250301361084,5.170505523681641,5.903332710266113,4.173304080963135,4.924998760223389,4.881485462188721,1.297890067100525,0.15238799154758453,2.965027332305908,1.5044889450073242,1.783265471458435,-0.019609784707427025,-0.9840086698532104,5.190652847290039,2.501497507095337,5.49884557723999,-5.407357215881348,4.785423755645752,6.202689170837402,-0.9861891865730286,-0.7844505310058594,5.653196334838867,-2.503182888031006,-1.3796418905258179,-4.082545757293701,4.720991611480713,5.59597635269165,-1.0497405529022217,-2.8197224140167236,-2.839550495147705,-0.2575061619281769,3.2027735710144043,-1.8442153930664062,-3.8803932666778564,-1.1465092897415161,-1.2063802480697632,-1.1347745656967163,5.707389831542969,-3.9312219619750977,-0.9194542169570923,3.2681713104248047,4.861827373504639,-1.434788465499878,5.4915947914123535,-0.8674630522727966,1.7743661403656006,-2.9555201530456543,-2.45015549659729,-1.0227065086364746,-0.38013821840286255,-2.0668067932128906,-1.007530927658081,6.522623538970947,-2.1912503242492676,-1.6287184953689575,-1.1012145280838013,-2.138608455657959,4.264944076538086,0.17530213296413422,-1.2056968212127686,-4.149209976196289,-1.613413691520691,-0.9690054059028625,6.471128463745117,5.942536354064941,-2.1201391220092773,-1.3656152486801147,-2.0804097652435303,1.0727314949035645,11.413108825683594,4.507617950439453,8.110130310058594,-2.558098077774048,2.586081027984619,0.9130300879478455,1.4444864988327026,-2.7604291439056396,-1.0920014381408691,-2.904677391052246,-2.120232582092285,4.164011478424072,-2.906705379486084,-1.618321418762207,-4.82623291015625,-1.428201675415039,5.4918365478515625,5.486728668212891,7.644475936889648,5.476819038391113,-4.893293857574463,-0.7056451439857483,0.6160911917686462,5.841557025909424,-0.6726038455963135,7.6233367919921875,4.166970729827881,12.519949913024902,-2.8395345211029053,7.70147705078125,7.701448917388916,-0.6708466410636902,-1.0336930751800537,-2.7397913932800293,-2.674574375152588,-1.7740544080734253,4.836730480194092,7.316575527191162,5.709106922149658,5.708060264587402,6.010213375091553,4.932125091552734,3.2349791526794434,-1.3238691091537476,-6.8859052658081055,0.16119788587093353,-1.4735511541366577,3.1530489921569824,-2.2611730098724365,5.993199348449707,-5.762895584106445,-1.579010009765625,3.004014253616333,4.115931987762451,-1.1332134008407593,5.0317559242248535,4.227063179016113,-2.4441704750061035,-5.407299518585205,-0.5870346426963806,-0.18904422223567963,-1.7762210369110107,-3.747797727584839,-4.767794132232666,4.912981986999512,-3.9311716556549072,-7.391870498657227,-1.1207972764968872,-2.757263660430908,-1.3532204627990723,-2.107682704925537,1.072983980178833,7.8090009689331055,4.366371154785156,6.065093994140625,-1.3152014017105103,4.232933521270752,-0.6941879987716675,4.821479320526123,4.816278457641602,-1.32789146900177,1.1223982572555542,2.898379325866699,4.658578395843506,-1.6948446035385132,-5.779506683349609,6.756019592285156,0.06353965401649475,0.8499746918678284,0.8501059412956238,-1.3604851961135864,1.2720954418182373,-1.86797034740448,5.031652927398682,5.4926652908325195,-2.0412542819976807,-1.562498688697815,-6.615612030029297,-1.1983014345169067,-2.617595911026001,7.342690944671631,-0.8660120964050293,0.20623502135276794,-1.7879263162612915,-3.93082857131958,4.7735090255737305,8.07030200958252,6.157663345336914,5.0213093757629395,-1.9954888820648193,-2.0132391452789307,-6.510871887207031,4.5340070724487305,1.3274155855178833,11.37946605682373,11.37691879272461,-0.7006177306175232,5.968056678771973,6.9411234855651855,5.498337745666504,-2.0806610584259033,11.388299942016602,3.006758213043213,2.6776716709136963,-6.889098644256592,-3.0745012760162354,-3.074302911758423,6.5048651695251465,5.017448425292969,3.4794247150421143,-2.336071252822876,11.389225959777832,2.677554130554199,-2.0120012760162354,-2.3389828205108643,6.130573272705078,-2.367433547973633,0.5567660927772522,-5.928037643432617,-5.928149700164795,-1.0724647045135498,-1.482138991355896,-2.772444009780884,-1.336902141571045,-0.33268117904663086,1.1196361780166626,-0.42760464549064636,-1.0779635906219482,5.790289402008057,-2.151440382003784,-1.4991729259490967,-0.7892966866493225,-1.2057154178619385,5.623545169830322,-1.096113681793213,1.7752017974853516,3.447489023208618,0.25822913646698,-1.3482307195663452,-6.510597229003906,-8.15656566619873,-0.15335632860660553,-6.893838882446289,6.274137020111084,-1.020517349243164,11.333182334899902,11.331925392150879,-2.460616111755371,-0.8419932126998901,-2.087974786758423,11.337932586669922,11.40400218963623,4.886785507202148,4.240037441253662,-1.1390780210494995,-3.5769829750061035,-0.36258968710899353,-1.8547964096069336,-0.08225414156913757,-0.692518413066864,-3.661809206008911,-0.4273863434791565,-0.0212785005569458,-0.1463152915239334,-0.13037030398845673,-0.15319813787937164,-2.356111526489258,-6.895644187927246,-0.14502699673175812,-2.177917242050171,-2.3326938152313232,-4.341949462890625,5.042023658752441,0.3639588952064514,-0.3583783209323883,11.411369323730469,-0.5170272588729858,-1.6005064249038696,-1.0731890201568604,2.887805700302124,-1.5110242366790771,-1.1597622632980347,5.878028392791748,-1.1484885215759277,3.087653875350952,-0.521028995513916,5.767935752868652,-2.252753496170044,-2.1783859729766846,-0.7679405808448792,-1.376975417137146,11.401397705078125,-1.700065016746521,-0.779829740524292,-2.9355826377868652,-1.8427566289901733,-1.2551072835922241,-2.3739304542541504,-0.41985973715782166,4.3281426429748535,-0.48772507905960083,-1.7497363090515137,4.264695167541504,8.517535209655762,-0.5057488083839417,11.414206504821777,-2.2559351921081543,8.51770305633545,7.317745208740234,-1.558214545249939,0.13724221289157867,4.902647495269775,6.123849868774414,0.10508808493614197,0.09124437719583511,0.108668752014637,-2.3837718963623047,-5.121163845062256,6.289163112640381,-2.3450615406036377,-2.149746894836426,-0.7067468762397766,-0.8743338584899902,-4.342020034790039,-2.1510608196258545,-2.237394094467163,11.412689208984375,5.478664398193359,-2.634437322616577,-2.3953824043273926,4.837067127227783,0.9320414066314697,4.566781997680664,5.992673873901367,-1.0681039094924927,-2.0580995082855225,1.7696831226348877,-0.15234535932540894,-2.1050450801849365,5.393435478210449,5.743557453155518,3.532517194747925,-1.6472371816635132,-2.3769760131835938,-2.1543996334075928,-1.4004379510879517,-1.0026196241378784,-2.4576029777526855,0.2540929317474365,4.785431861877441,-2.3633053302764893,-2.4752280712127686,-0.49062681198120117,-2.2752370834350586,0.16349327564239502,5.993210315704346,-8.156744003295898,7.7572407722473145,-2.0353002548217773,-1.2919589281082153,4.279709815979004,4.316370487213135,-1.1417803764343262,2.430664300918579,2.6493802070617676,5.515859127044678,-1.9950265884399414,-1.8391873836517334,7.758111000061035,-1.0869364738464355,-2.397928237915039,-0.8197996616363525,0.15926866233348846,2.9380834102630615,0.1236211434006691,-0.4869300127029419,5.880040645599365,-1.154988408088684,0.3646243214607239,5.478577613830566,0.36702314019203186,0.3670624792575836,5.836444854736328,-1.1793575286865234,-0.8377500176429749,-4.341995716094971,5.120233058929443,-2.0828592777252197,-3.457899332046509,5.754645824432373,-2.274449348449707,-5.220638275146484,-6.520480632781982,-1.6801267862319946,3.7382965087890625,2.89989972114563,1.7583931684494019,-2.226799488067627,-0.49940672516822815,-0.4375731945037842,0.3544911742210388,-3.2886414527893066,5.968041896820068,-5.121372699737549,7.129929065704346,7.129997253417969,0.07272275537252426,11.389183044433594,-2.0864410400390625,-0.9125889539718628,10.237529754638672,0.3642551600933075,-2.0587105751037598,6.289040565490723,7.036388397216797,-1.4137014150619507,4.380556106567383,4.332484245300293,-2.57080078125,-2.2770731449127197,0.1601589173078537,-1.365214467048645,-1.3797533512115479,-1.151062250137329,-1.6893057823181152,-1.1041457653045654,-1.137707233428955,7.354260444641113,4.19826602935791,4.7850751876831055,-0.4666469693183899,-1.7486547231674194,-6.9161505699157715,4.785086631774902,-2.0037636756896973,-2.386308431625366,4.221399307250977,-0.8411369323730469,5.961012840270996,-0.0332961231470108,5.558101177215576,-2.0685338973999023,4.551676273345947,5.676258563995361,-0.7808386087417603,5.348271369934082,-0.8764595985412598,-0.42166075110435486,7.0538105964660645,7.055339813232422,-1.8270877599716187,-0.4053937792778015,-2.055894613265991,-1.5834981203079224,-1.0724172592163086,-0.4394499361515045,11.401862144470215,-2.2371726036071777,5.885373592376709,0.5578226447105408,-2.0009288787841797,6.992283821105957,2.4307456016540527,-1.0020904541015625,-1.6978490352630615,-2.756753444671631,-2.4576642513275146,-0.3369421362876892,-0.4096359312534332,4.232660293579102,-2.016890048980713,-3.138796806335449,9.67604923248291,-6.527234077453613,5.618055820465088,-1.5154483318328857,-1.5239431858062744,-2.6007814407348633,4.233644962310791,-2.1040565967559814,-0.611930787563324,-2.2123262882232666,0.11058482527732849,-0.08749286085367203,-0.8783984184265137,-0.5147424936294556,-0.17708709836006165,-1.2914979457855225,-2.1359317302703857,4.666066646575928,2.0358726978302,2.0347089767456055,2.9571402072906494,7.127926826477051,-2.390678644180298,-2.0885891914367676,-1.1971046924591064,4.398066997528076,5.229164123535156,-2.0746915340423584,-2.0628390312194824,4.7851948738098145,-2.062932014465332,-1.1658778190612793,-0.9100080132484436,-0.9668623208999634,7.075771331787109,2.9245505332946777,7.127439975738525,-2.292182445526123,4.877192497253418,5.726568222045898,-0.4820133447647095,-0.7784735560417175,-8.352921485900879,-8.352974891662598,-1.007899522781372,-1.0079408884048462,-1.007943868637085,-1.0079199075698853,-1.842033863067627,10.23727798461914,7.053515434265137,-2.167646884918213,3.7676384449005127,-1.6136912107467651,-2.0635242462158203,0.041790273040533066,-0.691619873046875,-0.7923227548599243,5.903328895568848,10.237732887268066,-4.050197124481201,5.279173851013184,-1.9820258617401123,1.675515055656433,4.938854694366455,-1.842615008354187,4.943486213684082,-2.110765218734741,0.9362112283706665,-0.35861435532569885,5.968059539794922,-2.175945281982422,-0.7823302149772644,-0.7943661212921143,-2.931466817855835,3.0514416694641113,-6.540017604827881,-2.1064515113830566,7.12928581237793],"y":[3.89703106880188,9.768813133239746,1.1990067958831787,8.615967750549316,3.0814285278320312,5.499405860900879,2.096090793609619,8.766365051269531,11.53701114654541,9.911171913146973,14.762826919555664,11.011164665222168,10.790644645690918,-0.4196392297744751,10.702374458312988,6.6651411056518555,5.456471920013428,14.27073860168457,10.862517356872559,4.674966812133789,10.860026359558105,9.842472076416016,2.473017454147339,11.176519393920898,10.990227699279785,11.023209571838379,9.36620044708252,10.910118103027344,8.327526092529297,11.040781021118164,11.528837203979492,11.241394996643066,14.270783424377441,2.0585834980010986,9.392385482788086,-5.720285415649414,4.4307732582092285,2.1572048664093018,10.94328498840332,9.871870040893555,10.755105018615723,10.757643699645996,9.8281831741333,9.38150405883789,9.373716354370117,10.847651481628418,9.383853912353516,11.591856002807617,10.852887153625488,10.997188568115234,11.196639060974121,9.869705200195312,9.400927543640137,11.17420768737793,10.64279556274414,10.611102104187012,9.01613712310791,4.219936847686768,11.042966842651367,3.819565534591675,2.159332036972046,4.486526966094971,4.4024834632873535,14.849435806274414,4.29191780090332,2.4650485515594482,3.8706743717193604,10.470836639404297,2.489023447036743,4.186426162719727,-0.4210396111011505,12.022208213806152,4.432462215423584,4.05116605758667,8.273258209228516,4.5689616203308105,3.4074511528015137,4.002142906188965,4.3404951095581055,4.169510841369629,4.334438323974609,4.562994003295898,10.709758758544922,11.909987449645996,8.345349311828613,8.345111846923828,4.154983997344971,9.270112037658691,11.276724815368652,4.182875633239746,4.299278736114502,4.291437149047852,10.581398963928223,4.446981906890869,4.192923545837402,4.415703296661377,4.239249229431152,1.508591890335083,12.009698867797852,1.9217760562896729,1.5554232597351074,3.813699960708618,3.871549606323242,1.531732439994812,3.875678300857544,4.133181571960449,4.184316158294678,4.444849967956543,13.36191463470459,13.363274574279785,13.363092422485352,8.451324462890625,4.565027713775635,4.521027565002441,10.70985221862793,4.485360622406006,4.308059215545654,9.822258949279785,1.4912742376327515,11.178784370422363,3.8191680908203125,4.167661666870117,7.411455154418945,7.411462306976318,4.260171413421631,11.025015830993652,3.910921335220337,4.055959224700928,3.8908183574676514,1.5554375648498535,12.030893325805664,4.548419952392578,4.638458728790283,4.3331193923950195,11.982935905456543,4.454622745513916,1.2031762599945068,2.1848092079162598,10.946016311645508,3.9032485485076904,4.653109550476074,1.5411334037780762,4.238289833068848,1.5054445266723633,2.47149395942688,2.4715402126312256,14.82997989654541,12.012521743774414,3.830796480178833,3.4216601848602295,5.215069770812988,4.786579132080078,4.562613487243652,4.71198844909668,2.2748119831085205,4.052818298339844,0.863201916217804,4.280613422393799,4.746465682983398,2.188231945037842,2.177180290222168,4.711886882781982,1.4941351413726807,4.2029218673706055,3.87094783782959,3.396162986755371,4.578402042388916,2.6163532733917236,3.3169515132904053,3.425020933151245,7.391528606414795,1.5884162187576294,4.97511100769043,3.0092012882232666,-0.8832984566688538,1.7027146816253662,9.075721740722656,6.182923793792725,6.231571197509766,5.9931440353393555,3.6814396381378174,5.224025249481201,2.622025489807129,2.9658799171447754,6.926784038543701,5.697793483734131,5.0442657470703125,3.37087345123291,1.6433926820755005,3.35308837890625,2.466858386993408,8.404915809631348,15.825072288513184,7.421104907989502,2.6515398025512695,2.654066324234009,6.612399578094482,8.981986999511719,-4.133153438568115,9.45366382598877,8.505518913269043,8.358661651611328,6.801724910736084,1.8019770383834839,-1.7100934982299805,7.25992488861084,4.5108323097229,2.270479917526245,-0.030634025111794472,6.351180076599121,3.936091184616089,6.652929782867432,0.9709486961364746,8.291009902954102,3.5286073684692383,1.5006083250045776,1.916218638420105,0.6419512629508972,5.338061809539795,5.759564399719238,6.478399753570557,2.2172276973724365,5.864014148712158,3.308042526245117,8.252422332763672,0.9652355313301086,1.6490533351898193,1.8965903520584106,6.28544807434082,2.752937078475952,1.8430038690567017,8.238231658935547,6.727975845336914,8.258683204650879,-2.657010555267334,3.6461246013641357,2.7629477977752686,8.656026840209961,0.537532389163971,2.464810371398926,5.790510177612305,5.968479156494141,5.790292263031006,6.869809627532959,8.259852409362793,1.6489512920379639,8.1924409866333,8.405652046203613,4.930694103240967,7.322139263153076,1.2109127044677734,0.8513645529747009,2.796603202819824,0.02582218497991562,-5.686830997467041,1.4712247848510742,1.4608731269836426,6.429555892944336,3.3469724655151367,18.01634979248047,6.694660186767578,8.372465133666992,6.603644371032715,9.425501823425293,2.0391883850097656,6.9021806716918945,4.759042263031006,7.290903568267822,7.298890113830566,6.65986967086792,6.289148807525635,-0.5394304394721985,-0.5280994772911072,7.691640377044678,-2.3544557094573975,6.7948503494262695,9.441146850585938,9.786348342895508,6.027749061584473,4.166699409484863,3.04264235496521,3.0426418781280518,2.989161491394043,3.172853469848633,7.337181091308594,4.133155822753906,3.2476494312286377,-0.835595965385437,-0.8356186151504517,2.5423340797424316,6.803046703338623,6.912222862243652,1.198656678199768,-1.591475486755371,8.431607246398926,6.153722763061523,2.3968350887298584,0.9900456666946411,3.5526368618011475,7.472770690917969,2.725634813308716,1.058342695236206,3.4499967098236084,7.721468925476074,0.7958669066429138,-1.8292222023010254,-1.8294289112091064,3.3740768432617188,6.6086506843566895,2.272271156311035,7.781991958618164,4.924915790557861,11.089933395385742,3.0425286293029785,-0.5455990433692932,3.42221736907959,8.342327117919922,8.352767944335938,8.475513458251953,-3.270066022872925,6.026944160461426,6.2880659103393555,4.170094013214111,-0.4437160789966583,2.872962236404419,2.039855480194092,2.029600143432617,5.364223003387451,4.246846675872803,6.642177581787109,8.556327819824219,6.389992713928223,12.784411430358887,18.016357421875,1.890333652496338,6.8125810623168945,3.6680374145507812,1.3831404447555542,5.801529407501221,2.840317726135254,6.526824474334717,8.707442283630371,7.1143646240234375,3.339409112930298,2.549785614013672,0.9796106815338135,5.341827392578125,3.7719202041625977,6.995665073394775,4.7596917152404785,3.3322770595550537,3.4179372787475586,4.203811168670654,8.825785636901855,9.672256469726562,3.940373659133911,4.090669631958008,8.661687850952148,6.086419582366943,-2.1186094284057617,-2.653637170791626,2.0365726947784424,6.047748565673828,3.600801944732666,4.414559364318848,3.5021774768829346,12.73769760131836,4.658680438995361,12.92088508605957,6.609793186187744,5.6651411056518555,3.684448003768921,5.575769901275635,5.011318683624268,1.0240975618362427,5.98179292678833,7.799441814422607,5.686854362487793,4.595434188842773,2.4669294357299805,2.9927730560302734,6.728438377380371,6.877334117889404,2.2877604961395264,-4.675351619720459,8.675318717956543,2.7315492630004883,8.494770050048828,3.8504881858825684,1.6595098972320557,6.638792514801025,2.2362053394317627,6.3440775871276855,4.793975353240967,-0.300909161567688,5.713191509246826,-3.2567484378814697,-0.30433687567710876,5.676117420196533,5.3944993019104,6.08332633972168,-2.118568181991577,-2.12245512008667,8.374588012695312,0.8870488405227661,5.642422199249268,3.6932895183563232,9.078052520751953,12.67514705657959,-1.5231846570968628,8.371098518371582,-3.2607834339141846,7.467696666717529,0.9796535968780518,11.910956382751465,11.9110107421875,-1.5200536251068115,4.674224376678467,3.148887872695923,7.426133632659912,6.214310169219971,-3.7951202392578125,-2.393831491470337,8.826006889343262,8.827783584594727,2.8948850631713867,6.027303218841553,4.019383430480957,3.266650676727295,8.50367546081543,-0.4113767445087433,6.102772235870361,0.7409299612045288,4.42739200592041,-5.6867475509643555,5.224185466766357,5.766686916351318,9.424689292907715,6.477567195892334,6.713903427124023,6.478175640106201,6.646749496459961,6.2031660079956055,12.784509658813477,7.066635608673096,-0.30257585644721985,3.1373279094696045,3.052448272705078,6.831273078918457,6.048863887786865,9.672224044799805,3.9546234607696533,6.698856353759766,-0.1892874538898468,6.24943208694458,5.732454299926758,-4.674690246582031,9.453701972961426,0.04776435345411301,-1.5917975902557373,3.273190498352051,6.630229949951172,3.514880895614624,-3.685185432434082,-3.667384147644043,9.70173168182373,6.9163899421691895,2.560635805130005,1.588839054107666,6.257857799530029,5.231435298919678,3.296647548675537,7.269769668579102,10.581090927124023,10.581247329711914,3.2875895500183105,4.1374406814575195,6.982121467590332,6.4782328605651855,0.7998665571212769,6.856244087219238,6.14170503616333,9.032936096191406,5.996549606323242,0.9470034837722778,8.194029808044434,1.518101453781128,0.7666563987731934,2.4081549644470215,9.671991348266602,7.496495723724365,4.762307167053223,1.849368691444397,2.655304431915283,6.902826309204102,6.888195037841797,4.931222915649414,5.4184675216674805,4.162331581115723,0.0007867483654990792,0.012826845981180668,4.351031303405762,-3.899204730987549,3.1377055644989014,-2.112506151199341,2.269038438796997,-0.027126407250761986,9.425515174865723,-4.241437911987305,8.506284713745117,13.159873962402344,13.159859657287598,2.466848611831665,6.463586807250977,-1.7099794149398804,2.7499444484710693,-0.030589258298277855,-4.241239547729492,5.864723205566406,5.216679573059082,6.883565425872803,5.174036979675293,9.325963973999023,9.754140853881836,9.754178047180176,15.824426651000977,6.929988861083984,-0.20047064125537872,3.2729408740997314,7.164930820465088,6.928915023803711,3.3152341842651367,4.386865615844727,7.617588520050049,5.330200672149658,3.2196147441864014,4.359436988830566,5.981177806854248,5.302444934844971,6.68377161026001,2.0230560302734375,1.8696942329406738,2.0711238384246826,3.6612884998321533,4.930932521820068,8.652595520019531,5.138988494873047,8.5099458694458,2.1116297245025635,4.118419647216797,0.04404624179005623,0.04558825120329857,3.945470094680786,3.751223087310791,2.7345054149627686,0.040185846388339996,8.675638198852539,5.32627010345459,4.99053955078125,4.208179950714111,6.351548194885254,7.138911724090576,3.1774742603302,8.345773696899414,3.437516689300537,6.353146076202393,3.4495937824249268,5.371039390563965,6.028173923492432,6.045522212982178,5.135019779205322,5.169301509857178,8.51220703125,6.0315470695495605,3.4076380729675293,5.1932806968688965,1.915489912033081,3.13413405418396,8.611099243164062,13.387029647827148,8.67525863647461,6.606069087982178,3.939213514328003,4.929283142089844,3.3070104122161865,5.876378059387207,4.10944938659668,8.138725280761719,5.721098899841309,3.436511278152466,-0.10016804188489914,8.039406776428223,5.9242939949035645,6.629870414733887,3.4151134490966797,6.8202056884765625,8.675804138183594,6.251111030578613,6.225518703460693,-0.3848952054977417,7.303933620452881,6.94204568862915,5.118655204772949,3.462965250015259,0.08945459872484207,6.665383338928223,4.523434162139893,4.941718101501465,-1.7421785593032837,3.3718838691711426,8.674885749816895,5.924578666687012,-1.7422871589660645,-2.3933680057525635,6.956842422485352,-0.44452399015426636,5.316774368286133,6.87766695022583,-0.525276243686676,-0.4755026698112488,-0.4499679207801819,3.491145133972168,5.6915669441223145,-1.5625629425048828,5.1885457038879395,3.4509143829345703,3.5610392093658447,4.455215930938721,1.9151909351348877,3.4506428241729736,2.9690420627593994,8.674755096435547,8.471428871154785,0.9327096939086914,5.671644687652588,3.6177265644073486,16.121753692626953,-2.0140511989593506,-5.6858744621276855,5.547607421875,3.5366411209106445,2.0188894271850586,6.019622802734375,4.305735111236572,0.9868642091751099,8.01760482788086,12.058568000793457,5.627269268035889,5.159079074859619,3.4434478282928467,3.4836559295654297,4.153655529022217,3.9551639556884766,2.0663928985595703,18.016359329223633,5.1654744148254395,3.928748607635498,6.673004627227783,5.9449381828308105,8.055736541748047,-5.686825275421143,8.652791976928711,-1.4674702882766724,5.24476432800293,4.571672439575195,6.5699334144592285,6.697438716888428,4.218837261199951,-3.09454607963562,1.4681590795516968,5.248388290405273,4.402640342712402,7.335573673248291,-1.4680917263031006,4.178987503051758,5.676238536834717,4.501952648162842,-0.43933355808258057,2.768326759338379,-0.4345206022262573,5.5936384201049805,-0.8825250864028931,5.75318717956543,0.687792181968689,8.460591316223145,0.681617796421051,0.6817137598991394,8.058466911315918,5.860894203186035,0.22990767657756805,1.9153509140014648,5.864827632904053,2.744868516921997,2.8206934928894043,8.820115089416504,5.9452595710754395,5.387179374694824,8.884613037109375,5.594320297241211,-3.458515167236328,2.5722501277923584,2.0317060947418213,3.4104859828948975,3.4697318077087402,6.572348594665527,0.7126532793045044,2.7004170417785645,-3.8992700576782227,5.69162130355835,-3.1528971195220947,-3.1529414653778076,5.9349517822265625,-0.030553502961993217,2.734919548034668,7.134922981262207,1.6636407375335693,0.6889956593513489,5.817071914672852,-1.5626840591430664,3.0310440063476562,3.410727024078369,5.170870304107666,0.08821729570627213,0.9397833943367004,5.947019100189209,-0.41095298528671265,3.270570993423462,5.788196563720703,4.213428020477295,6.296252727508545,6.663628578186035,4.4238972663879395,8.18602466583252,-3.3148045539855957,18.015623092651367,7.1395769119262695,4.526283264160156,8.532608985900879,18.015684127807617,4.404352188110352,5.165469646453857,-3.3380870819091797,3.9396204948425293,1.412246823310852,8.315515518188477,8.53361988067627,2.757902145385742,2.6218795776367188,8.717955589294434,5.361141204833984,7.056900501251221,4.47010612487793,3.3180325031280518,3.027655601501465,3.027209758758545,7.665205001831055,3.302168369293213,5.820219039916992,3.9081625938415527,4.935073375701904,3.4449336528778076,8.675809860229492,5.907559394836426,8.195394515991211,9.327286720275879,4.403461456298828,3.078711748123169,-3.0945706367492676,7.167045593261719,6.240178108215332,6.3021721839904785,6.069443225860596,7.157391548156738,7.111727237701416,4.989868640899658,5.867376804351807,4.559128284454346,-0.5683771967887878,8.849102020263672,8.635354995727539,4.7701029777526855,4.7807416915893555,0.9784039258956909,5.020613670349121,5.702892780303955,3.453913688659668,4.197388172149658,-0.5142174363136292,-0.35093986988067627,4.474255084991455,6.619762897491455,5.990441799163818,4.569258213043213,2.3726181983947754,8.314303398132324,9.81973648071289,9.818952560424805,2.8423821926116943,-3.1511709690093994,5.054654598236084,2.7232825756073,6.8006486892700195,4.257291316986084,2.8805508613586426,3.3005075454711914,6.895851135253906,18.015953063964844,6.889467239379883,3.4428677558898926,4.12968111038208,3.456120014190674,4.66278076171875,3.3238255977630615,-3.151240587234497,5.238101959228516,3.3545215129852295,1.5625725984573364,6.683940887451172,3.597022294998169,13.284061431884766,13.284106254577637,16.750669479370117,16.750699996948242,16.750770568847656,16.750720977783203,7.274803638458252,1.6619616746902466,3.029829263687134,2.67574143409729,3.324639081954956,3.200514316558838,5.824522972106934,-3.7022809982299805,6.060174942016602,4.347471714019775,1.3560867309570312,1.6635148525238037,8.041610717773438,1.8297580480575562,3.003554344177246,1.012270450592041,5.315465450286865,7.306473731994629,5.3156256675720215,2.334077835083008,16.132158279418945,13.387145042419434,-3.89931321144104,6.63500452041626,7.734513759613037,4.36722993850708,5.699220180511475,9.447967529296875,8.825533866882324,3.335573434829712,-3.1523377895355225],"z":[5.843814373016357,4.006680488586426,4.250214099884033,4.958175182342529,5.185543537139893,-5.850451946258545,6.702871322631836,5.000494480133057,5.311695575714111,5.020599842071533,3.2994132041931152,5.628293991088867,4.581169128417969,-4.593453884124756,4.524685859680176,-4.636131286621094,-5.858208656311035,0.2626025974750519,5.110826015472412,6.209087371826172,5.121799945831299,5.262016773223877,6.006499290466309,4.209309101104736,5.333176136016846,5.861534595489502,4.993475914001465,5.156954765319824,5.505650043487549,5.2096147537231445,5.315337181091309,4.308848857879639,0.26262375712394714,6.288861274719238,4.996715068817139,6.561914443969727,-6.7592949867248535,6.670250415802002,4.512898921966553,5.074880599975586,4.720921516418457,4.720602512359619,5.06438684463501,4.999637126922607,4.999801158905029,5.025295734405518,5.0001325607299805,5.309067726135254,5.0970048904418945,5.955068111419678,5.311058044433594,5.0869221687316895,5.002484321594238,5.232882499694824,4.487170219421387,5.329946041107178,0.9354632496833801,-5.523559093475342,5.230715751647949,-6.187987327575684,6.6718668937683105,-5.9024176597595215,-5.8914031982421875,3.3633930683135986,-5.837069034576416,-1.2048395872116089,-6.224002838134766,4.309896469116211,6.005081653594971,-6.219833850860596,-4.592475414276123,3.6255390644073486,-6.760447025299072,-6.390803813934326,5.256551742553711,-5.7962164878845215,5.5480217933654785,6.14126443862915,-6.669342994689941,-6.303382873535156,-5.700956344604492,-5.83930778503418,4.6178789138793945,3.5988447666168213,5.4021687507629395,5.396420955657959,-6.481700897216797,4.994475364685059,4.248805999755859,-6.507480144500732,-5.932853698730469,-5.847649097442627,4.252911567687988,6.478736400604248,-6.526703834533691,-6.744152545928955,-5.831157684326172,4.589915752410889,3.6246516704559326,6.179014205932617,4.571464538574219,-6.1812424659729,-6.225680351257324,4.769586086273193,-6.229835510253906,-6.469350814819336,-6.508028030395508,-5.718635082244873,-0.00024304524413309991,-0.001149785122834146,-0.0009112196275964379,5.251245021820068,-5.8423686027526855,-5.754256725311279,4.5250468254089355,-5.896176815032959,-6.636451244354248,5.058481693267822,5.160290718078613,5.237401962280273,-6.188037395477295,-6.302367687225342,-9.537325859069824,-9.537297248840332,-5.830309867858887,5.204537868499756,-6.261538028717041,-5.289968967437744,-6.242055416107178,5.89685583114624,3.6213276386260986,-5.8588948249816895,5.823461532592773,-5.902159214019775,3.6261448860168457,-5.940649509429932,4.239589691162109,6.66165018081665,4.512567043304443,-6.262239456176758,-5.8181071281433105,5.927752494812012,-6.113338947296143,4.564114570617676,-6.253454208374023,-6.253386497497559,3.3546760082244873,3.629878520965576,-6.198631763458252,5.56857442855835,-5.897693157196045,-5.851820945739746,-5.855249404907227,-6.171031951904297,-0.4874449074268341,-6.3917412757873535,6.24680757522583,-5.993268013000488,5.957818508148193,6.658767223358154,6.667691707611084,-6.171136379241943,5.159071922302246,-6.187544822692871,-6.235079765319824,5.54544734954834,-5.8450798988342285,-0.7485201954841614,-1.0523567199707031,1.5283981561660767,1.707590937614441,-6.271459102630615,-1.343957781791687,-0.7366663813591003,4.5101423263549805,0.41601118445396423,-6.339962959289551,4.2809157371521,4.311994552612305,3.70531964302063,0.46431833505630493,-6.107580184936523,5.225641250610352,-0.8022933006286621,1.4823731184005737,-3.1486475467681885,4.961566925048828,5.181237697601318,-0.09303773194551468,-1.0467402935028076,-1.2045799493789673,-1.1304774284362793,-5.553565979003906,1.5894948244094849,-0.14413140714168549,-0.14189551770687103,1.1589514017105103,-6.251481533050537,-0.2145470827817917,3.6811399459838867,0.5832688808441162,-3.788313627243042,1.7572001218795776,-0.35895535349845886,6.460141658782959,-1.4488482475280762,-5.713764667510986,0.4483647644519806,5.411105632781982,1.4029539823532104,-1.1633695363998413,1.8572336435317993,4.659046173095703,-1.385510802268982,-1.2374860048294067,-0.9265626668930054,-0.767520546913147,-4.721579074859619,4.8998870849609375,4.45376443862915,-8.068943977355957,-3.685232400894165,5.316803932189941,-1.049798846244812,-1.293642282485962,0.20739051699638367,-2.587550163269043,1.1608436107635498,-3.2755258083343506,1.2629047632217407,1.2536190748214722,-1.4976340532302856,-1.2448686361312866,-1.302208662033081,7.123992443084717,0.4479214549064636,1.2709394693374634,-4.780523777008057,0.8108507990837097,-1.1829698085784912,4.44931173324585,3.612945795059204,4.4453630447387695,0.7999338507652283,-1.299896478652954,-2.5874950885772705,-1.2255905866622925,-1.4092000722885132,8.572198867797852,1.8078058958053589,-3.7152671813964844,-0.8221571445465088,2.515610456466675,5.417109966278076,-4.2119669914245605,-0.9328399300575256,-0.9402075409889221,0.6727505922317505,-1.0416899919509888,-2.4521243572235107,1.8556385040283203,-1.1404119729995728,-4.131416320800781,0.09042763710021973,1.5418668985366821,1.9217146635055542,-4.967191219329834,-1.456563949584961,-1.4588923454284668,-3.7979516983032227,-3.278507709503174,-0.5592106580734253,-0.557173490524292,-10.557008743286133,-0.7115746736526489,-1.4269284009933472,-2.207585334777832,-5.6293840408325195,-6.6870036125183105,-1.6216002702713013,0.9350740313529968,0.935071587562561,2.575096845626831,-0.9300467371940613,-1.4854326248168945,-6.414628028869629,5.772414684295654,-0.20922531187534332,-0.209257110953331,2.726025104522705,1.7542577981948853,-1.4341195821762085,-0.017740478739142418,-5.219547748565674,-4.744979381561279,-3.1374242305755615,-0.6999691724777222,-3.4443421363830566,-11.01502799987793,3.4889516830444336,2.4854323863983154,0.1204582005739212,5.500511169433594,-5.017953872680664,-3.27437686920166,-3.7144954204559326,-3.7145040035247803,5.253331184387207,-1.4318422079086304,0.45118141174316406,-0.5396435260772705,5.018989086151123,1.6535606384277344,0.9349669218063354,-6.273040771484375,-1.3551925420761108,-1.4130364656448364,-1.4981220960617065,0.5946730971336365,3.922285795211792,-3.0007917881011963,-3.2777490615844727,-1.6361433267593384,-5.908253192901611,-0.7196596264839172,1.566017746925354,1.2752646207809448,-1.2886568307876587,4.620388507843018,-3.7929177284240723,-4.088927268981934,0.719317615032196,0.6202894449234009,-2.4518587589263916,-0.5288352370262146,1.969487190246582,3.142643451690674,-0.1699972003698349,4.447101593017578,1.485870599746704,1.777496099472046,-4.7579545974731445,-4.359791278839111,5.321320533752441,6.1205644607543945,-6.873342990875244,-1.3314741849899292,-1.0640687942504883,1.0999064445495605,-4.967484951019287,5.320235729217529,5.185891151428223,4.627774238586426,-1.4738255739212036,9.540830612182617,-3.9599201679229736,-1.0437836647033691,-1.5157732963562012,3.942876100540161,-0.44416379928588867,7.554449081420898,1.257683515548706,-3.724714756011963,3.9539029598236084,3.6279489994049072,5.465124130249023,-5.7307538986206055,3.3069729804992676,1.4466313123703003,0.5624383687973022,-3.1864893436431885,4.792408466339111,4.893527984619141,-1.3288512229919434,-3.483518362045288,3.623110771179199,2.2116177082061768,-3.160820484161377,2.6643569469451904,-1.1887494325637817,-0.9947194457054138,0.6725975871086121,1.5945639610290527,6.566498756408691,-4.873401641845703,4.606557369232178,2.4903604984283447,-1.0587286949157715,-0.1217029020190239,1.4748088121414185,-1.3064020872116089,1.8827494382858276,0.39388033747673035,3.3136322498321533,6.8326873779296875,5.126801490783691,3.913381338119507,6.837854385375977,-3.1746485233306885,1.5700868368148804,3.9344935417175293,-0.4445185661315918,-0.429670125246048,-1.1406556367874146,-0.40696069598197937,2.4780728816986084,3.6512694358825684,-3.6732118129730225,-8.13867473602295,3.0693092346191406,-1.1705728769302368,3.916048288345337,3.4839882850646973,-6.873324394226074,-3.5308308601379395,-3.530850410461426,3.0682666301727295,3.3118207454681396,1.9835807085037231,-2.506027936935425,-0.9962131381034851,-0.2443355768918991,-4.426241874694824,-1.4744504690170288,-1.4733681678771973,-0.9691594839096069,-2.9962539672851562,-1.19076669216156,5.397393703460693,1.7989046573638916,-5.8533148765563965,3.991940498352051,0.6947757601737976,2.605436325073242,-4.21192741394043,2.9515323638916016,11.124837875366211,0.09334427118301392,0.07575421780347824,1.8605310916900635,-8.068971633911133,-1.4789237976074219,-0.11011862009763718,0.6204016208648682,1.9528067111968994,-5.80391263961792,5.617461204528809,5.852167129516602,1.8953605890274048,-3.027043104171753,9.540583610534668,3.447903871536255,1.8708006143569946,6.7086501121521,-1.2492872476577759,5.137365818023682,-4.873360633850098,3.6811647415161133,-3.14233136177063,-5.219992637634277,5.387714385986328,-1.4584954977035522,5.090942859649658,-0.2778516411781311,-0.28851303458213806,-5.747611045837402,-5.4846930503845215,-0.6445055603981018,-6.271433353424072,4.32102108001709,2.9711244106292725,-4.729310512542725,-1.278314471244812,0.6209665536880493,0.6208649277687073,5.375948429107666,-6.417716026306152,1.0684758424758911,-8.068964004516602,-0.3805740177631378,0.8086331486701965,4.106598377227783,-6.299993515014648,3.6334989070892334,4.69601583480835,-1.223820686340332,-9.357768058776855,-3.297734498977661,2.7727537155151367,9.539710998535156,-1.442049503326416,2.425382375717163,-0.44300931692123413,-0.1562405377626419,0.8901866674423218,0.8617461919784546,8.573174476623535,-1.2509223222732544,-1.6237373352050781,5.428343772888184,5.437812328338623,2.994615316390991,2.9351377487182617,-4.951257705688477,-0.47419485449790955,6.592548370361328,5.41313362121582,0.0907568708062172,-0.14711669087409973,1.7996881008148193,3.9187047481536865,3.9186689853668213,-1.205601453781128,-3.5202836990356445,6.460079193115234,1.262026309967041,5.411139965057373,-0.14739498496055603,5.317685127258301,4.825729846954346,1.7507250308990479,4.834600448608398,-2.471376657485962,-2.294527053833008,-2.294456720352173,-5.553753852844238,1.4799388647079468,6.726856708526611,5.390305042266846,2.0271122455596924,-5.473018646240234,5.110997200012207,4.652966499328613,-4.269577980041504,4.8920087814331055,5.479198455810547,4.0902018547058105,3.622706651687622,-1.0422840118408203,1.8765583038330078,1.324676752090454,1.2041783332824707,-3.4798760414123535,1.8751347064971924,8.573100090026855,6.035470008850098,-0.9867972135543823,1.8043246269226074,-0.7437485456466675,4.1588263511657715,5.406509876251221,5.406345844268799,3.0216972827911377,3.794273853302002,6.140552043914795,5.408740043640137,4.608323097229004,-1.1488741636276245,-1.3356298208236694,4.628134250640869,1.3416705131530762,2.0190539360046387,5.5729193687438965,-1.8536474704742432,5.237123012542725,1.4553323984146118,5.520535945892334,-1.2931954860687256,-6.684271812438965,-6.66906213760376,-0.9829123616218567,4.834547519683838,1.806943655014038,-6.676201820373535,5.302361011505127,4.850208282470703,8.239316940307617,-0.7729859948158264,6.977181434631348,8.843228340148926,4.606847763061523,2.1889257431030273,2.4227535724639893,3.2372241020202637,-1.049303650856018,-1.6485954523086548,2.909675121307373,0.646880030632019,3.367971897125244,-1.0213221311569214,-5.618472576141357,0.6633481979370117,5.611796855926514,0.5806055068969727,5.2581000328063965,-0.7773891091346741,4.608860969543457,4.315495491027832,2.090186357498169,7.001403331756592,1.2172383069992065,-0.7659283876419067,4.589077472686768,5.491373538970947,-3.153768539428711,2.182204008102417,3.287334680557251,-1.314126968383789,-3.0389933586120605,5.172243118286133,4.606358528137207,5.616034984588623,-3.039088249206543,-4.428114414215088,1.4029364585876465,-5.911089897155762,-1.1455727815628052,1.7429858446121216,-6.145921230316162,-6.040863990783691,-5.995544910430908,4.498047828674316,-3.0845515727996826,-3.57481050491333,4.854075908660889,5.349270820617676,5.073561668395996,3.8881828784942627,8.239496231079102,5.356709957122803,1.7225871086120605,4.606351852416992,0.3234763443470001,4.709558486938477,4.486321926116943,-0.6953598856925964,-1.8578202724456787,-0.008169633336365223,-4.211713790893555,-1.9132721424102783,0.1994161605834961,1.3892028331756592,-6.696526527404785,3.1962268352508545,-0.5766353607177734,0.6772456169128418,9.845434188842773,-3.2949140071868896,4.8398518562316895,5.34598445892334,1.3861256837844849,4.569631576538086,3.0123655796051025,-3.483184337615967,-2.451840877532959,4.731548309326172,3.0233049392700195,2.182042121887207,5.656386375427246,-1.5879329442977905,-4.211935520172119,6.035467147827148,2.2110493183135986,4.8803534507751465,4.7164998054504395,-1.3967351913452148,-1.415884017944336,4.636831760406494,1.0143088102340698,1.4182924032211304,-1.0328783988952637,3.2398462295532227,1.2370747327804565,2.2116894721984863,4.597021579742432,4.479522228240967,5.119091510772705,-5.903290271759033,-0.7061406373977661,-5.917129039764404,-1.4695936441421509,4.509433746337891,3.3983547687530518,-4.0520477294921875,0.3256879150867462,-4.0561652183532715,-4.056009769439697,0.7167785167694092,3.503228187561035,-5.447695255279541,8.23935604095459,0.5133984684944153,6.134003639221191,6.048081874847412,0.3989548981189728,5.653698921203613,2.4851889610290527,-6.16034460067749,-3.4097044467926025,7.793987274169922,-0.6503831744194031,1.386546015739441,5.083754062652588,5.408536911010742,-1.2804521322250366,-4.035093784332275,6.140932083129883,2.9350099563598633,-3.084686040878296,1.9080836772918701,1.9080365896224976,-1.4002020359039307,5.410980224609375,6.142370223999023,-0.5923159122467041,-0.9413238167762756,-4.0513739585876465,5.261884689331055,-3.574697256088257,-5.142632484436035,1.1872788667678833,-1.2964900732040405,-3.1634585857391357,4.7713494300842285,5.656551361083984,-5.860807418823242,5.396101951599121,-1.727715253829956,4.62260103225708,4.3380608558654785,1.874320387840271,4.662998676300049,-1.2139118909835815,3.9427120685577393,-2.4527628421783447,1.964446783065796,3.278486967086792,1.8271490335464478,-2.452733278274536,3.240654945373535,4.838501930236816,3.9651618003845215,4.756964206695557,-0.9691312313079834,-1.9116960763931274,0.3657131791114807,6.120195388793945,5.22529411315918,0.39057981967926025,-1.8933563232421875,0.45451217889785767,3.8918142318725586,5.110018253326416,-5.154729843139648,-5.144937038421631,-10.537020683288574,5.092026233673096,5.266062259674072,2.3278539180755615,3.2306737899780273,5.5028910636901855,4.608741760253906,5.591193675994873,0.6690086722373962,-2.488036870956421,3.241077184677124,-5.056751728057861,1.0142766237258911,-0.7094331979751587,4.308867454528809,-3.9736642837524414,-0.5133621096611023,2.0284509658813477,2.0011980533599854,-1.3299131393432617,5.3182454109191895,-0.41891083121299744,3.9439122676849365,-6.129301071166992,0.38189297914505005,4.763540267944336,4.755598545074463,4.655428886413574,-1.30642831325531,5.132960319519043,5.225158214569092,3.1447594165802,-6.116693019866943,-5.8505706787109375,3.892172336578369,2.1982312202453613,-6.727904319763184,4.713891506195068,6.480704307556152,-4.831902980804443,-9.622364044189453,-9.620444297790527,-0.7218589186668396,1.9086713790893555,4.084945201873779,6.154142379760742,1.7577496767044067,-0.9681475162506104,-0.6480598449707031,5.399697303771973,0.7817332148551941,-2.45249080657959,0.7804483771324158,0.9077989459037781,4.586920261383057,2.9122719764709473,2.027873992919922,-1.0163021087646484,1.9087944030761719,4.891711711883545,-0.7820366621017456,-0.4203459620475769,2.1853456497192383,3.2567503452301025,-2.2779839038848877,-2.2779693603515625,1.8240993022918701,1.824257731437683,1.824246883392334,1.8242377042770386,1.2139047384262085,-0.9405859708786011,-5.136326313018799,6.1922526359558105,-0.9101442694664001,1.5443124771118164,5.271108150482178,-1.6157561540603638,2.145315170288086,4.119189262390137,-0.9428513646125793,-0.9413556456565857,2.239349842071533,0.45805227756500244,1.4657237529754639,-0.6517732739448547,-1.1325398683547974,1.2232463359832764,-1.1323662996292114,6.519033908843994,-1.8622241020202637,8.843482971191406,2.9350290298461914,0.574918270111084,-0.5615835189819336,4.097521781921387,-0.50892174243927,0.050636157393455505,-6.105638027191162,5.363275051116943,1.9085322618484497],"type":"scatter3d"},{"hovertemplate":"label=quality_4\u003cbr\u003eproj0=%{x}\u003cbr\u003eproj1=%{y}\u003cbr\u003eproj2=%{z}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"quality_4","marker":{"color":"#ab63fa","symbol":"circle"},"mode":"markers","name":"quality_4","scene":"scene","showlegend":true,"x":[-1.0767269134521484,-1.565078854560852,-1.565982699394226,6.229913711547852,-1.5723586082458496,3.5942041873931885,-1.6448034048080444,-4.70417594909668,3.318570375442505,-1.5297352075576782,-1.697246789932251,-1.0885190963745117,5.847332954406738,-1.6626324653625488,-5.051971912384033,-3.6747381687164307,4.511613845825195,11.63644790649414,-5.909236907958984,-1.6286423206329346,2.2513320446014404,1.8143136501312256,1.9932475090026855,5.000888824462891,-1.0079253911972046,2.5564651489257812,2.285353183746338,5.048763275146484,6.0019965171813965,2.3096978664398193,-6.683176040649414,3.852243185043335,1.7795729637145996,1.4621326923370361,-1.7145533561706543,-6.787954330444336,-3.4751861095428467,-1.0432368516921997,1.6820554733276367,-1.035446286201477,-1.0340328216552734,1.6296007633209229,-1.248785376548767,2.428537130355835,-5.909552097320557,1.9853068590164185,2.1010801792144775,10.513374328613281,-1.0376561880111694,3.0628750324249268,2.522555351257324,10.513422966003418,1.0855592489242554,-3.2798049449920654,10.838805198669434,3.1274778842926025,3.076150417327881,-1.6906676292419434,1.727042317390442,2.16621470451355,-7.033997058868408,1.9837604761123657,10.352554321289062,-1.7007867097854614,7.555245399475098,4.20271110534668,-1.6066851615905762,13.332280158996582,3.0090084075927734,-0.9663294553756714,1.8398646116256714,5.499053478240967,-1.7009594440460205,8.3377103805542,1.6699739694595337,5.797008514404297,-1.7009965181350708,11.03541374206543,4.906961917877197,2.185488700866699,6.3396525382995605,5.5259785652160645,-4.42910099029541,-4.42879581451416,5.970781326293945,2.5826754570007324,-7.056040287017822,-4.429936408996582,4.508981227874756,7.854404926300049,4.879096031188965,9.367918968200684,0.3259205222129822,-4.406883716583252,-0.9027547836303711,0.4383092224597931,1.7590878009796143,2.1791234016418457,0.3149668276309967,5.700547695159912,5.57135009765625,-7.13674259185791,-7.1374382972717285,0.2641611099243164,2.584012031555176,7.85183572769165,4.668295383453369,10.324329376220703,2.9701764583587646,0.6638954281806946,2.1786770820617676,5.356015682220459,0.26427415013313293,-0.014721130952239037,5.772432804107666,-0.9080070853233337,5.383968353271484,0.534598171710968,-0.872732400894165,0.49314385652542114,0.6110355854034424,9.367668151855469,-3.002270221710205,5.441410064697266,6.335206985473633,2.256690263748169,-1.5039854049682617,5.536699295043945,-1.50399911403656,-0.6535987257957458,0.3053271770477295,-2.2371327877044678,5.530939102172852,-1.5039886236190796,3.96553111076355,3.9896490573883057,4.473397254943848,7.331106185913086,7.4239301681518555,1.278049111366272,-6.863999366760254,1.2412458658218384,4.841445446014404,-4.163723468780518,-4.163694858551025,1.2280633449554443,1.2291357517242432,-1.6474987268447876,-4.4070963859558105,-4.406911849975586,5.331144332885742,10.022603988647461,10.022601127624512,-1.5942883491516113,6.816304683685303,0.5721928477287292,5.641007423400879,2.322021007537842,-0.336717814207077,0.3111732006072998,-0.8741222023963928,-0.09533058106899261,5.4415998458862305,5.517770767211914,5.781702995300293,-2.9292707443237305,6.185274124145508,-0.6235702633857727,-4.4222002029418945,-9.295223236083984,2.192394256591797,-1.0100980997085571,7.854404926300049,3.965222120285034,1.8438955545425415,-1.6867600679397583,2.525578737258911,1.7661538124084473,-1.8831156492233276,4.332461357116699,3.061717987060547,-0.9216569066047668,7.854323387145996,-0.5981535911560059,-0.10809256881475449,-0.255149245262146,4.770780086517334,-0.8206472396850586,-1.0859429836273193,7.853671073913574,0.8030877113342285,1.573775291442871,2.2144060134887695,6.3324503898620605,6.320157051086426,-1.629450798034668,-2.964651346206665,-5.856689453125,0.3313121199607849,2.1764492988586426,2.289482593536377,1.072735071182251,2.1866579055786133,4.093019008636475,6.344193458557129,2.1740612983703613],"y":[10.03670597076416,10.682659149169922,10.121077537536621,14.77936840057373,10.110105514526367,0.5394138693809509,2.071322441101074,-3.923940896987915,5.522514343261719,9.692787170410156,10.647976875305176,10.797714233398438,4.865286827087402,10.678145408630371,8.655288696289062,-0.3619900047779083,11.682260513305664,7.361530780792236,3.020733118057251,10.672609329223633,4.305115222930908,4.9283952713012695,-5.299612522125244,5.482904434204102,10.885208129882812,2.626883029937744,4.154343128204346,8.211236000061035,3.4848217964172363,3.7008895874023438,1.5249881744384766,11.047833442687988,2.3001351356506348,2.1820361614227295,10.628936767578125,13.753738403320312,-0.42326974868774414,10.11110782623291,2.2763140201568604,10.851896286010742,10.108205795288086,4.252968788146973,10.313932418823242,4.121496677398682,3.020205497741699,3.8934683799743652,4.522435188293457,-2.20932936668396,10.852910041809082,3.56445574760437,1.7914005517959595,-2.2092654705047607,-5.042063236236572,12.285629272460938,6.250856399536133,3.5379750728607178,4.742595672607422,3.6149649620056152,1.9877300262451172,10.293028831481934,3.758216142654419,0.8785122036933899,1.2244490385055542,-6.0717291831970215,4.7409868240356445,2.0863826274871826,3.3262057304382324,2.662025213241577,4.809764862060547,-3.192472219467163,1.9357556104660034,-2.1106832027435303,-6.071791172027588,-0.6186531186103821,11.01811695098877,8.933239936828613,-6.071897506713867,12.627752304077148,4.755544185638428,9.47270679473877,14.6708984375,7.92930793762207,12.49164867401123,12.488525390625,5.801212310791016,8.989311218261719,0.7690800428390503,12.493000030517578,4.2124738693237305,9.37022590637207,-2.8073811531066895,8.877760887145996,8.10315990447998,12.40819263458252,9.05073070526123,3.2683141231536865,3.1383755207061768,10.280679702758789,-6.38589334487915,8.824600219726562,-2.703171491622925,7.800201892852783,7.800321102142334,9.885122299194336,8.98405647277832,9.369112014770508,3.2819206714630127,1.2455544471740723,14.469964981079102,5.841572284698486,9.485738754272461,7.329197883605957,9.88521957397461,5.328891277313232,4.035009860992432,-2.7969155311584473,8.512550354003906,3.7299931049346924,-2.6611838340759277,3.258561372756958,7.874512195587158,8.877300262451172,11.843314170837402,-0.7611343264579773,8.645963668823242,9.940646171569824,-2.0641417503356934,6.343201160430908,-2.064169406890869,9.461126327514648,8.077127456665039,2.9710991382598877,6.353180408477783,-2.064206123352051,1.9715189933776855,2.0029802322387695,2.692178249359131,2.1550400257110596,1.8950186967849731,7.9222493171691895,3.951559543609619,7.973140239715576,2.1867856979370117,-1.7272034883499146,-1.7272940874099731,-3.6275641918182373,-3.6261820793151855,4.007564067840576,12.408503532409668,12.40809154510498,6.220499515533447,-4.4062180519104,-4.406210899353027,9.736379623413086,3.5198750495910645,3.39985728263855,7.088795185089111,10.365290641784668,2.479274272918701,8.750149726867676,3.2609457969665527,5.315532207489014,-0.7611360549926758,6.500500202178955,8.948201179504395,6.4485578536987305,8.760299682617188,2.3886020183563232,12.46670150756836,4.138604640960693,10.222811698913574,4.069650650024414,9.37024974822998,1.9713481664657593,-5.543176174163818,3.2518064975738525,11.797652244567871,3.096635341644287,2.60434627532959,0.08826974779367447,0.78648442029953,-2.9373185634613037,9.370196342468262,3.736572504043579,4.775187015533447,5.1554365158081055,9.235321998596191,9.471208572387695,10.803292274475098,9.369901657104492,7.755017280578613,-0.0561024434864521,9.54901123046875,14.680471420288086,14.692975997924805,3.334990978240967,-0.543951153755188,10.07040023803711,5.971770286560059,10.278166770935059,10.323576927185059,-4.675405025482178,10.266509056091309,2.043384552001953,14.669828414916992,10.290183067321777],"z":[4.870516300201416,5.398769855499268,4.140426158905029,3.3663244247436523,4.126768589019775,11.778675079345703,6.702963352203369,-1.529595971107483,-5.810084819793701,3.932251214981079,4.0755534172058105,5.898006916046143,5.713072776794434,4.124739170074463,1.0731948614120483,1.5191326141357422,5.156853199005127,-0.10979381203651428,-5.212350368499756,3.930713415145874,6.1461710929870605,7.049157619476318,-8.35842227935791,4.886114120483398,6.060009956359863,5.083428382873535,6.017657279968262,5.282936096191406,6.401119232177734,5.942936420440674,-4.686765193939209,5.918619155883789,6.194195747375488,6.214234828948975,4.104977607727051,2.3523595333099365,7.500373363494873,4.91141939163208,6.051355361938477,6.009091854095459,4.918395519256592,-5.828325271606445,4.852303504943848,6.1413493156433105,-5.213134288787842,5.848628520965576,6.588018894195557,4.608034133911133,6.004790306091309,6.812422275543213,4.613753795623779,4.608133792877197,5.832829475402832,-4.682823181152344,-4.007811069488525,6.9112548828125,10.034547805786133,0.5339752435684204,1.577385663986206,-5.030613899230957,3.481900930404663,-0.8358787298202515,3.4900338649749756,3.034733772277832,2.0466227531433105,0.84820955991745,0.6562247276306152,-3.3730316162109375,10.16285514831543,7.06840181350708,1.5455501079559326,-0.5674301981925964,3.034754991531372,5.287492275238037,12.469687461853027,0.34952640533447266,3.034642457962036,-4.30424690246582,-1.6570656299591064,-4.956096649169922,3.178546905517578,0.4416579604148865,3.2796566486358643,3.2837798595428467,-5.562244892120361,-4.631789207458496,-4.223080635070801,3.2794723510742188,-0.48667943477630615,-5.674466609954834,-0.5422444343566895,-4.4499382972717285,-1.3120118379592896,3.3529927730560303,-5.540933609008789,-1.5970150232315063,6.173877239227295,-5.01677942276001,-4.754096984863281,-1.4750170707702637,-5.491415500640869,1.0365232229232788,1.0364278554916382,12.298443794250488,-4.630006790161133,-5.67387056350708,2.708284616470337,3.465872287750244,-4.991756439208984,-3.096348524093628,-4.956353664398193,-5.031606674194336,12.298495292663574,-1.2567312717437744,6.167756080627441,7.326070785522461,-1.4896684885025024,-1.4738962650299072,7.543595790863037,-1.6382269859313965,-5.282562255859375,-4.4491095542907715,0.9310420155525208,6.057523250579834,-1.4558783769607544,-4.976439476013184,-8.316295623779297,0.7340644598007202,-8.316323280334473,-5.136631488800049,-1.3333395719528198,1.7375068664550781,0.7352146506309509,-8.316366195678711,1.0405246019363403,0.9848442077636719,2.471770763397217,-5.323590278625488,-5.269826412200928,6.412544250488281,3.5757575035095215,6.481968402862549,0.4450398087501526,-6.313711166381836,-6.31374979019165,4.294275760650635,4.293112754821777,0.36586618423461914,3.3529093265533447,3.3532872200012207,0.5553686022758484,-2.295675277709961,-2.2956693172454834,-5.682821750640869,-5.615654945373535,-1.609658122062683,-5.015980243682861,-4.826455116271973,-3.701219320297241,6.768385410308838,-0.6955257058143616,-1.2389404773712158,6.057464599609375,0.8609728217124939,0.3425705134868622,-4.04293966293335,-1.480104923248291,-0.7635298371315002,3.3026697635650635,-0.35925203561782837,-5.010619163513184,4.227392673492432,-5.674505233764648,1.0407389402389526,4.2369256019592285,1.3459432125091553,13.371621131896973,6.245527267456055,0.5495505332946777,-3.1635687351226807,0.8222858905792236,7.173206806182861,-5.674533843994141,3.8009984493255615,-1.3439884185791016,-0.952645480632782,-4.431655406951904,-5.182949542999268,5.908368110656738,-5.674227237701416,-5.052460670471191,-0.48408031463623047,-5.016943454742432,3.188856840133667,3.205134630203247,0.6055701375007629,7.230109214782715,2.68731689453125,1.1294584274291992,-5.019777297973633,-4.869675636291504,-4.873398780822754,-5.0103759765625,0.9172699451446533,3.1792972087860107,-5.022807598114014],"type":"scatter3d"},{"hovertemplate":"label=quality_8\u003cbr\u003eproj0=%{x}\u003cbr\u003eproj1=%{y}\u003cbr\u003eproj2=%{z}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"quality_8","marker":{"color":"#FFA15A","symbol":"circle"},"mode":"markers","name":"quality_8","scene":"scene","showlegend":true,"x":[2.072655439376831,3.489274024963379,-1.7869024276733398,4.2449727058410645,3.527653694152832,4.739840030670166,3.8429412841796875,6.1307525634765625,1.9085546731948853,3.5455644130706787,-4.361264228820801,2.6125481128692627,1.823103904724121,-1.9776619672775269,2.8635361194610596,-1.7165858745574951,-1.727543830871582,-0.5999388694763184,7.616954326629639,4.584247589111328,-0.7833086848258972,-1.0060609579086304,4.920416831970215,-0.8826799392700195,-1.8528895378112793,-0.9494242072105408,-1.8118515014648438,-4.3491668701171875,5.2695841789245605,9.944330215454102,5.5084991455078125,-0.3822040557861328,-5.260919094085693,5.993303298950195,-0.6770378947257996,-0.8663849234580994,4.802940368652344,-3.8622477054595947,-0.9564823508262634,5.405998706817627,4.40462064743042,4.591844081878662,-0.8654046654701233,4.225411891937256,0.105124831199646,5.437817096710205,7.697847843170166,-4.7384185791015625,6.268274307250977,-1.1973642110824585,3.298938751220703,1.1165448427200317,0.3755609393119812,0.37351271510124207,3.224379777908325,-1.7236343622207642,-2.913182258605957,-2.2163615226745605,-3.0967180728912354,-1.063345193862915,-0.29860594868659973,-2.600066900253296,-1.4235937595367432,-3.8621394634246826,5.48515510559082,10.169751167297363,4.636929035186768,-0.20255248248577118,4.365670204162598,-2.981414318084717,-2.7775893211364746,-2.831343412399292,12.446561813354492,5.957241058349609,-4.349127769470215,6.4426045417785645,4.890538692474365,4.412428855895996,-2.4320435523986816,4.825630187988281,-1.0627386569976807,9.944381713867188,-1.3626155853271484,5.485717296600342,5.581573486328125,7.279338836669922,-4.905244827270508,-1.8284908533096313,-4.341141223907471,4.548402309417725,7.864630222320557,7.864680290222168,-0.16273480653762817,-1.0568259954452515,3.7381322383880615,-6.524510383605957,0.42223456501960754,-0.4073798358440399,-6.518678188323975,-4.341136455535889,0.5047480463981628,-0.5712586045265198,-2.684452533721924,-1.1543488502502441,-2.867845296859741,-1.596260905265808,-2.001718759536743,2.455216646194458,-3.627619504928589,-3.629316806793213,-3.6244170665740967,5.78969144821167,-2.2290310859680176,-0.8886266350746155,3.738194227218628,0.9306391477584839,5.3604254722595215,0.4525810182094574,-2.873140811920166,-0.4250907003879547,-2.1843111515045166,5.532083511352539,5.8135480880737305,5.4282755851745605,3.737982749938965,-2.2738468647003174,-2.214932680130005,-0.8086144328117371,-5.0989484786987305,-0.7929832935333252,-0.480518639087677,-1.2692711353302002,-4.443070411682129,-2.8738584518432617,4.785292148590088,5.339172840118408,-2.368337392807007,-3.3039262294769287,-0.44003695249557495,-1.709256649017334,-4.443089962005615,7.00471305847168,-1.3434476852416992,-2.7178151607513428,0.9304341077804565,-2.7130179405212402,-2.7104451656341553,-2.4286346435546875],"y":[4.620687961578369,10.578314781188965,2.0751125812530518,11.076513290405273,10.961606979370117,9.373756408691406,10.9663667678833,14.870655059814453,1.182845115661621,10.543498039245605,12.309009552001953,1.5053293704986572,4.3040266036987305,2.1758294105529785,5.2152299880981445,2.281343936920166,2.273970603942871,3.8315348625183105,8.367781639099121,7.292627811431885,7.753602027893066,3.994990825653076,6.027414798736572,4.716121196746826,7.254274368286133,4.427622318267822,7.014484882354736,11.438480377197266,5.091712951660156,6.910592555999756,-2.0692670345306396,7.211215496063232,6.32717752456665,-5.686830997467041,3.5012311935424805,-2.654494524002075,3.434288740158081,12.430800437927246,3.354705333709717,0.7017762064933777,5.405715465545654,6.100480556488037,-2.649479627609253,6.646972179412842,3.5526459217071533,0.7711158990859985,8.294608116149902,5.403627872467041,1.9748657941818237,5.980870723724365,4.1029510498046875,4.3726983070373535,0.6639235019683838,0.6673579216003418,3.4569947719573975,3.6984190940856934,-0.31577178835868835,2.8220267295837402,6.359267711639404,4.6828789710998535,-0.24881066381931305,6.069766044616699,6.081035137176514,12.430778503417969,1.0568856000900269,11.131473541259766,7.357264995574951,5.504305839538574,0.03957453742623329,6.358547210693359,6.3487467765808105,6.352063179016113,7.392574787139893,1.6984652280807495,11.43834114074707,2.616819143295288,5.29764986038208,5.2535319328308105,6.241743087768555,-3.7152633666992188,4.4135847091674805,6.910578727722168,3.2662765979766846,6.422898769378662,1.123260498046875,8.235992431640625,6.792520523071289,2.7788405418395996,1.9150279760360718,2.2092819213867188,6.194456100463867,6.194580078125,5.146320343017578,5.531201362609863,-3.4582126140594482,8.854541778564453,5.9898786544799805,3.469151735305786,8.873799324035645,1.9151612520217896,9.585307121276855,6.440795421600342,0.8084402084350586,4.129895210266113,-0.2834753394126892,5.779818534851074,2.8301305770874023,1.717494249343872,6.350803852081299,6.350815296173096,6.351004123687744,8.060226440429688,2.8135359287261963,7.150135040283203,-3.458301544189453,16.118133544921875,1.7543867826461792,5.988791465759277,-0.283011794090271,6.912282466888428,3.4088258743286133,8.396409034729004,8.854693412780762,1.6941372156143188,-3.457794427871704,5.943695545196533,5.891814708709717,5.45155143737793,5.637632369995117,4.161923408508301,6.684869289398193,6.92883825302124,10.828775405883789,-0.2827121317386627,18.01616096496582,6.988561630249023,2.5857269763946533,-0.44041261076927185,3.4441843032836914,6.251188278198242,10.82889461517334,3.067809820175171,2.8368141651153564,0.7752194404602051,16.117515563964844,0.7774817943572998,0.7805367708206177,1.0155023336410522],"z":[-5.822525501251221,4.344087600708008,6.712979793548584,5.576059818267822,5.143455505371094,4.988598823547363,4.606316566467285,3.3725686073303223,6.085500717163086,4.221464157104492,3.437213182449341,4.560258865356445,-5.664422512054443,6.668071746826172,-5.900626182556152,-0.48563846945762634,-0.48705756664276123,3.984309673309326,-1.1060079336166382,-1.4451252222061157,-0.5534496307373047,4.310571193695068,-3.0033957958221436,2.7768478393554688,1.2064094543457031,4.9025492668151855,1.1450432538986206,-4.615020275115967,-0.990229070186615,-0.5206764340400696,-0.7656631469726562,1.9187971353530884,1.9938230514526367,-4.211955547332764,5.083858013153076,7.554879665374756,-0.7245224118232727,-1.4586848020553589,5.312912940979004,-0.4820045232772827,-0.7212986350059509,-0.13033337891101837,7.562094211578369,-1.4784440994262695,-11.014937400817871,-0.4456497132778168,-1.0856600999832153,1.4844272136688232,-0.7246859073638916,3.617197275161743,-1.3997795581817627,-1.8153767585754395,-4.068398475646973,-4.066000461578369,-1.0115207433700562,0.48798519372940063,6.857242107391357,1.3192585706710815,0.7834041118621826,3.352614641189575,-5.753378391265869,-0.1659235656261444,3.9265494346618652,-1.4583768844604492,-0.6582066416740417,1.693580150604248,-1.4437611103057861,-1.3609048128128052,-3.0944478511810303,0.6498085856437683,0.4235013425350189,0.47803547978401184,3.408902645111084,-0.16161420941352844,-4.615084648132324,-0.7381559610366821,-1.1630829572677612,-1.2484694719314575,-0.04065519943833351,-0.2697875499725342,4.625728607177734,-0.5207698345184326,5.400448799133301,0.7115946412086487,-0.7339805364608765,-1.2738690376281738,1.9392670392990112,0.7849003076553345,8.239087104797363,0.6006969213485718,6.763660430908203,6.763692378997803,-0.9993186593055725,-1.927794337272644,7.79396390914917,-6.134450912475586,1.098427414894104,5.541438579559326,-6.150971412658691,8.23904800415039,-3.100979804992676,2.1788883209228516,4.825734615325928,2.928530216217041,6.834348201751709,11.105731010437012,6.03005838394165,1.4946025609970093,1.4039554595947266,1.4074150323867798,1.3998597860336304,0.6572786569595337,1.3130613565444946,-0.5813141465187073,7.7939229011535645,-1.856048822402954,0.45208823680877686,1.0847508907318115,6.832357406616211,2.09438419342041,5.2550482749938965,0.3750576078891754,0.4275030493736267,0.4001566171646118,7.7938761711120605,5.655766010284424,5.525359630584717,-1.8210633993148804,2.0183749198913574,4.367796897888184,2.1857337951660156,-0.7713611125946045,4.555665969848633,6.8310699462890625,-2.45220947265625,0.4333227872848511,6.252589225769043,7.40037202835083,5.503627777099609,4.297604560852051,4.555679798126221,-5.078510284423828,2.7303524017333984,4.845297813415527,-1.8557689189910889,4.8419060707092285,4.840970039367676,4.86090612411499],"type":"scatter3d"},{"hovertemplate":"label=quality_3\u003cbr\u003eproj0=%{x}\u003cbr\u003eproj1=%{y}\u003cbr\u003eproj2=%{z}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"quality_3","marker":{"color":"#19d3f3","symbol":"circle"},"mode":"markers","name":"quality_3","scene":"scene","showlegend":true,"x":[4.387259006500244,-1.5027645826339722,-0.9817931056022644,5.053327560424805,1.7707725763320923,-0.9618287086486816,5.1192307472229,-1.6500868797302246,1.792114019393921,10.513383865356445,4.564693450927734,6.249516487121582,-6.703511714935303,0.7161439657211304,3.9783413410186768,0.26439419388771057,11.046860694885254,1.4941198825836182,4.806739330291748,7.2177534103393555,5.7886552810668945,5.867387294769287,4.652429103851318,-1.3574854135513306,0.27874699234962463,-0.8971576690673828,5.888055324554443,5.210217475891113,9.484987258911133,5.858305931091309],"y":[11.607767105102539,16.329078674316406,10.9113130569458,8.209915161132812,4.9945292472839355,10.9320707321167,-5.349599361419678,10.65604019165039,4.962207794189453,-2.2093536853790283,-1.846204400062561,1.8309270143508911,6.614138126373291,6.956887722015381,1.984008550643921,9.885425567626953,12.639142990112305,2.220189094543457,-3.1336588859558105,-4.355535984039307,8.898831367492676,8.974958419799805,3.1191015243530273,3.036665439605713,8.566739082336426,9.029084205627441,8.945425987243652,2.90647554397583,0.86454176902771,8.971221923828125],"z":[5.180883884429932,4.63861083984375,6.103328704833984,5.274094104766846,7.1222076416015625,6.136607646942139,-0.6659491658210754,4.134705543518066,7.086577415466309,4.608014106750488,0.07857689261436462,-0.8026477098464966,7.014915943145752,-1.0233267545700073,1.0202789306640625,12.298178672790527,-4.315505027770996,1.8525885343551636,-0.3758212924003601,-3.426652431488037,0.3737063407897949,0.33500733971595764,2.630174160003662,0.018371231853961945,-1.9011200666427612,-5.560544967651367,0.46981218457221985,-0.652182936668396,-7.533763885498047,0.28938373923301697],"type":"scatter3d"},{"hovertemplate":"label=quality_9\u003cbr\u003eproj0=%{x}\u003cbr\u003eproj1=%{y}\u003cbr\u003eproj2=%{z}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"quality_9","marker":{"color":"#FF6692","symbol":"circle"},"mode":"markers","name":"quality_9","scene":"scene","showlegend":true,"x":[-1.0817288160324097,-1.089928150177002,5.452215194702148,-2.1637163162231445,5.512966156005859],"y":[4.748453140258789,6.830667972564697,0.79836106300354,5.694655895233154,0.8805058002471924],"z":[-3.6942670345306396,1.8318626880645752,-0.4342164397239685,4.682685852050781,-0.3518257439136505],"type":"scatter3d"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"scene":{"domain":{"x":[0.0,1.0],"y":[0.0,1.0]},"xaxis":{"title":{"text":"proj0"}},"yaxis":{"title":{"text":"proj1"}},"zaxis":{"title":{"text":"proj2"}}},"legend":{"title":{"text":"label"},"tracegroupgap":0},"title":{"text":"dimension reduction with umap"}},                        {"responsive": true}                    ).then(function(){

var gd = document.getElementById('ea9f248f-387b-434d-9c81-0c0db6f6d171');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                            </script>        </div>
</body>
</html>


# Models and parameters


More formally, if $E$ is the set of examples and $L$ is a set that includes the labels, then what we call the *model* is a family of functions $f_{\rm \bf w}$ that depends on a set of parameters ${\rm \bf w}$: $$f_{\rm \bf w}: E  L.$$

It can be more convenient to express the function as depending on the parameters ${\rm \bf w}$ as well as the example ${\rm \bf x}$. The model's predicted label $\hat{y}$ for the example ${\rm \bf x}$ is:

$$\hat{y}=f_{\rm \bf w}({\rm \bf x})= f({\rm \bf x}; {\rm \bf w}).$$

ML practicioners use an enormous variety of models, depending on the problem at hand and on the available computational resources to train the model. Models include convolucional neural networks (CNN) for image classification (resnet and other kind of CNNs), neural networks (NN) for classification of tabular data, linear regression models, decision and regression trees, random forest and other ensemble models, among many other models.


<img src="https://drive.google.com/uc?export=view&id=1g7cUxaqoa1ujkRV-BjQKPWsVDFBAYWMb" width="600" >



## Example of a simple model (simple linear regression)

Suppose that our examples are scalar numbers $x_1,\dots, x_n$ and the labels are continuous labels $y_1, \dots, y_n$. We call $x$ the explanatory variable and $y$ the response variable.

Let's consider the simple linear regression model:
$f_{\rm a,b}(x)= a \, x + b$. The model parameters are ${\rm \bf w}=(a,b)$ and the predicted values are given by$\\[1em]$
$$\hat{y}=f(x; {\rm a,b})=a\, x + b.$$

The target  or actual label values are the $y_1, \dots, y_n$, and the predicted label values are the $\hat{y}_1,\dots,\hat{y}_n$.



## Example of a simple model (quadratic regression)

In notebook [Lesson3_edited_04-how-does-a-neural-net-really-work.ipynb](Lesson3_edited_04-how-does-a-neural-net-really-work.ipynb), a similar simple example is discussed. The only difference is that the model $f_{\rm a,b,c}$ in that example is quadratic instead of linear:

$$f_{\rm a,b,c}(x)= f(x;a,b,c)= a \, x^2 + b \, x + c.$$



# Loss function for regression

In ML, it is usual to call *loss* to the **dissimilarity** between actual and predicted label values for a *set* of labeled examples.

Let ${\rm \bf x}_1, \dots , {\rm \bf x}_n$ be a set of examples with labels $y_1, \dots , y_n$. Let $f_{\rm \bf w}$ be our model. Therefore, the predicted labels are

$$\hat{y}_1=f_{\rm \bf w}({\rm \bf x}_1), \dots, \hat{y}_n=f_{\rm \bf w}({\rm \bf x}_n).$$

The loss over that set of examples is some dissimilarity measure between the actual labels $y_1, \dots , y_n$ and the predicted labels $\hat{y}_1, \dots , \hat{y}_n$.



## Dissimilarity measures to define *loss*


To define loss, we then need to choose an appropriate dissimilarity metric between a set of actual $y_1, \dots , y_n$ and predicted labels $\hat{y}_1, \dots , \hat{y}_n$. The choice depends on the type of problem, and while MAE or RMSE are adequate for *regression* problems, other dissimilarities are used for *classification* problems.




## Examples of loss functions for regression problems (MAE, MSE, Huber)



Above, two common loss functions for regression problems were listed

1. Mean absolute error (MAE), given by $\frac{1}{n}\sum_{i=1}^n |y_i-\hat{y}_i|$; or

2. Mean square error (MSE), given by $\frac{1}{n}\sum_{i=1}^n \left(y_i-\hat{y}_i\right)^2$

In the one hand, MAE is not differentiable everywhere, which is an undesirable property for ML. On the other hand, MSE penalizes too much large differences between actual and predicted values, which means that a single example can constraint strongly the solution.

An alternative is called the Huber loss function, which is differentiable everywhere, and behaves like MSE near the origin and like MAE for large $|y_i-\hat{y}_i|$.


## Examples: simple linear regression and quadratic regression



For the linear regression example, the response variable is continuous. We wish to measure the dissimilarity between the set of actual label values $y_1, \dots , y_n$  and the set of values predicted by the model
$f_{\rm a,b}(x)= a \, x + b$:

$$\hat{y}_1=a\, x_1+ b, \dots, \hat{y}_n=a\, x_n+ b.$$

Since the response is continuous, it makes sense to use a function like the Mean absolute error (MAE), or the  Mean square error (MSE) for the loss.

Notebook [Lesson3_edited_04-how-does-a-neural-net-really-work.ipynb](Lesson3_edited_04-how-does-a-neural-net-really-work.ipynb) discusses a slightly more complex example, with one additional parameter. The only difference is that the generating function is quadratic instead of linear. In that new example, loss is given by MAE, i.e. $\frac{1}{n}\sum_{i=1}^n |y_i-\hat{y}_i|$.


```python
def mae(preds, acts):
  return (torch.abs(preds-acts)).mean()
```

The notebook includes code to interactively change the model weights and compute the corresponding values for the MAE loss function.


# ML as an optimization problem



Now, we can define a ML problem as a optimization problem. Given

1.  a set of examples  ${\rm \bf x}_1, \dots , {\rm \bf x}_n$  with labels $y_1, \dots , y_n$
2. a model $f_{\rm \bf w}$
3. a *loss* function $L$

the goal is to determine the optimal set of parameters ${\rm \bf w}$ that minimize the loss $L$ over that set of examples.

## Gradient descent and learning rate

Informally, a gradient measures how much the output of a function changes if you change the inputs a little bit.

Given a model $f_{\rm \bf w}({\rm \bf x})= f({\rm \bf x}; {\rm \bf w})$ and a batch of examples ${\rm \bf x_1}, \dots, {\rm \bf x_n}$, we have seen how we can define a *loss* function

$$L({\rm \bf x_1, \dots, x_n; w})= L_{\rm \bf x_1, \dots, \rm \bf x_n}(\rm \bf w).$$

We can write $L$ just a function of the weights since the ${\rm \bf x_i}$ are fixed for given batch of examples. Our goal is to find the set of weights ${\rm \bf w}$ that minimize $L({\rm \bf w})$. In order to do this iteratively, starting with an arbitrary set of initial weights, we would like to know how $L$ changes with a small change in the weights $\rm \bf w$ from the current set weights ${\rm \bf w}^{*}$.

This  is given by the gradient of $L$ with respect to ${\rm \bf w}$ at ${\rm \bf w}^{*}$, which is a vector of partial derivatives of $L$ with length equal to $m$=number of model parameters.

$$ \nabla L({\rm \bf w}^{*}) = \frac{\partial L}{\partial \rm \bf w}({\rm \bf w}^{*})= \left(\frac{\partial L}{\partial \rm w_1}({\rm \bf w}^{*}), \dots,  \frac{\partial L}{\partial \rm w_m}({\rm \bf w}^{*}) \right).$$

The computation of $\nabla L({\rm \bf w}^{*})$ is usually done by **back-propagation**, which is an automatic differentiation algorithm for calculating gradients for the weights in a neural network graph structure. Back-propagation (aka *backprop*) is an automatic differentiation algorithm that applies the *chain-rule*.

The vector $\nabla L({\rm \bf w}^{*})$ points to the direction from ${\rm \bf w}^{*}$ along which $L$ grows faster, so gradient descent follows the opposite direction $ - \nabla L({\rm \bf w}^{*})$.

![files_in_hugging_face_corn_disease.png](data:image/png;base64,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![files_in_hugging_face_corn_disease.png](data:image/png;base64,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))<img src="https://drive.google.com/uc?export=view&id=1-KGjbUaR1l3z879V_eJu7JutnSutqbdC" width="500" >


To simplify, let's suppose that all examples are visited before updating the set of weights.
Then, the steps of gradient descent algorithm are the following. In ML, one *epoch* corresponds to the processing of the totally of examples in the data set. So, for instance, if the algorithm runs for 20 epochs, then the model is applied to all examples 20 times.

---

1. Choose an initial set of weights ${\rm \bf w}^{*}$

2. For $i = 1, \dots, E$, where $E$ is the number of epochs, do:

   i) Cumpute $\nabla L({\rm \bf w}^{*})$

   ii) Update ${\rm \bf w}^{*}:={\rm \bf w}^{*} - \eta \, \nabla L({\rm \bf w}^{*}) $, where $\eta >0 $ is the learning rate.

---

The choice of the *learning rate* is critical for a good performance of the algorithm. A very small learning rate will permit a good approximation of the gradient flow by the algorithm (see next figure). But if the step is too small, many epochs will be needed to get a good solution.

<img src="https://drive.google.com/uc?export=view&id=12c4X3po4-xVGUJKzyKC56lwl4ZEmXqWa" width="400" >



ML practicioners use many different techniques to determine the *learning rate*. In particular, the learning rate can be adaptive and change along epochs, which is a standard approach in ML. An adaptive learning rate is provided by the `fine_tune` method used in   [Lesson1_00_is_it_a_bird_creating_a_model_from_your_own_data.ipynb](Lesson1_00_is_it_a_bird_creating_a_model_from_your_own_data.ipynb):

    learn = vision_learner(dls, resnet18, metrics=error_rate)
    learn.fine_tune(3)

In alternative, package `fastai` contains a method `lr_find()` that helps to find a adequate lerning rate, as discussed at 1:20' of Lesson 5 of [Practical Deep Learning for Coders 2022](https://course.fast.ai/), where it is used in the following chunk of code:

    learn = tabular_learner(dls, metrics=accuracy, layers=[10,10])
    learn.lr_find(suggest_funcs=(slide, valley))
    learn.fit(16, lr=0.03)

The value `0.03` used with `learn.fit` is derived from the visual interpretation of the output of `learn.lr_find(suggest_funcs=(slide, valley))` which is the following.

<img src="https://drive.google.com/uc?export=view&id=1bAH7oLrIsuISeZMr749FkI1rkmDp-hw-" width="500" >


Let's consider a very simple example, where we try to fit a model to a pairs of observation that are linearly related. Below, we discuss a `PyTroch` gradient descent script for the linear regression problem, and we compare the result with the optimal coefficients obtained by *least squares*. The code below shows how *training loss* is  computed.

The most specific part of the algorithm is the gradient computation. Note that the *gradient machinery* of `PyTorch` is turned-on for each weight with `requires_grad = True` as in the following case:

    coeffs=torch.tensor([-20.,-10.]).requires_grad_()

Then, the derivatives can be computed for any continuous function of the weights in tensor `coeffs`. In particular, the *loss* $L$ is defined as a function (that can be arbitrarily complicated) of the weights, and the *gradient* $\nabla L({\rm \bf w}^{*})$ for the current set of weights ${\rm \bf w}^{*}$ is computed with

    loss.backward()

Finally, the weights are updated with

    coeffs.sub_(coeffs.grad * step_size)

where method `sub_` is substraction for weight updating ${\rm \bf w}^{*}:={\rm \bf w}^{*} - \eta \, \nabla L({\rm \bf w}^{*})$, and  the learning rate $\eta$ is called `step_size` in the code.

Try changing the learning rate to see what happens (try for instance `step_size=0.1`).


```python
#@title Script for stochastic gradient descent with Pytorch, train only data, applied to synthetic LR data
# This example illustrates: gradient descent with PyTorch, train only, stochastic gradient descent (SGD)
import matplotlib.pyplot as plt
import torch
import numpy as np
torch.manual_seed(42)

step_size = 0.001  # learning rate
iter = 20  # number epochs

############################################ Creating synthetic data
# Creating a function f(X) with a slope of -5
X = torch.arange(-5, 5, 0.1).view(-1, 1)  # view converts to rank-2 tensor with one column
func = -5 * X + 2
# Adding Gaussian noise to the function f(X) and saving it in Y
y = func + 0.4 * torch.randn(X.size())

########################################## Baseline: Linear regression LS solution
from sklearn.linear_model import LinearRegression
reg = LinearRegression().fit(X, y)
print('Least square LR coefficients:',reg.intercept_,reg.coef_)

####################################################### Gradient Descent
# initial weights
coeffs = torch.tensor([-20., -10.], requires_grad=True)

# defining the function for prediction (linear regression)
def calc_preds(x):
    return coeffs[0] + coeffs[1] * x

# Computing MSE loss for one example
def calc_loss_from_labels(y_pred, y):
    return torch.mean((y_pred - y) ** 2) # mean applies to a single value in this case

# lists to store losses for each epoch
training_losses = []

# epochs
for i in range(iter):
    # calculating loss as in the beginning of an epoch and storing it
    y_pred = calc_preds(X)
    loss = calc_loss_from_labels(y_pred, y)
    training_losses.append(loss.item())

    # Stochastic Gradient Descent (SGD): update weights after each data point
    for j in range(X.shape[0]):
        # randomly select a data point
        idx = np.random.randint(X.shape[0])
        x_point = X[idx]
        y_point = y[idx]

        # making a prediction in forward pass
        y_pred = calc_preds(x_point)

        # calculating the loss between predicted and actual values
        loss = calc_loss_from_labels(y_pred, y_point)

        # compute gradient
        loss.backward()

        # update coeffs
        with torch.no_grad():
            coeffs.sub_(coeffs.grad * step_size)
            # zero gradients
            coeffs.grad.zero_() # PyTorch accumulates the gradients on subsequent backward passes. So, the default action has been set to accumulate (i.e. sum) the gradients on every loss.backward() call.

print('coeffs found by stochastic gradient descent:', coeffs.detach().numpy())

# plot training loss along epochs
plt.plot(training_losses, '-g')
plt.xlabel('epoch')
plt.ylabel('loss (MSE)')
plt.show()

```

    Least square LR coefficients: [2.0237875] [[-5.0023813]]
    coeffs found by stochastic gradient descent: [ 1.608836 -4.990727]
    


    
![png](ML_overview_with_examples_files/ML_overview_with_examples_34_1.png)
    


## A simple linear model: the perceptron

The example above envolved a linear regression model. That model is closely related to a basic Machine Learning model which is known as the *perceptron* .

<img src="https://drive.google.com/uc?export=view&id=1HqHfJn7ejJMGeAa5fTTQzW_myubIDsDh" width="600" >


The *perceptron* model can be written as

$$f_{\rm \bf w}(x_1,\dots,x_k)= \sigma (w_0 + w_1 \, x_1 + \dots + w_k \, x_k)$$

where $\sigma(.)$ is called an activation function, which is defined by a discontinuity like the following one

$$\sigma(z) = \left\{\begin{align}
1 &, &  z \ge 0 \\
-1 &, &  z < 0 \\
\end{align} \right.$$

The term $w_0 + w_1 \, x_1 + \dots + w_k \, x_k$ corresponds to the core component of multiple linear regression model:
$$Y=w_0 + w_1 \, x_1 + \dots + w_k \, x_k + \varepsilon,$$
where $\varepsilon$ represents the random errors.

For that model, the predicted values are $\hat{y}=w_0 + w_1 \, x_1 + \dots + w_k \, x_k$ and the absolute loss for each observation is $L=|\hat{y}-y|$. Since the observation $y$ does not depend on the weights, the partial derivates of the loss with respect to the weights are simply
$$\nabla L({\rm \bf w}^{*})=(1, \pm x_1, \dots, \pm x_k),$$
where the sign depends on the sign of $\hat{y}-y$ for each observation.

The following code, which does not use `PyTorch` but just the basic funcionalities of Python and `numpy` does exactly that. This code uses object oriented programming and creates a class `Perceptron` with methods `step_fit`, where the update of the model parameters is performed, `net_input` and `predict` which are needed to apply *gradient descent*. However, since the code is used to illustrate graphically the iterative process, the code also includes several methods that are needed to create an animation with `FuncAnimation`. The algorithm is applied to the `iris` data set, to only two species at the time. Only the two first explanatory variables are used for illustrative purposes.



```python
#@title Script to create a Perceptron class and an animation and apply it to the iris data set
from matplotlib.animation import FuncAnimation
import matplotlib.pyplot as plt
import numpy as np
import random
import pandas as pd

class Perceptron():
    #initialize hyperparameters (learning rate and number of iterations)
    def __init__(self, eta=0.1, n_iter=50, nameA='', nameB=''):
        self.eta = eta
        self.n_iter = n_iter
        self.nameA = nameA
        self.nameB = nameB

    def step_fit(self, X, y):
        #iterate over labelled dataset updating weights for each features accordingly (stochastic gradient descent)
        for xi, label in zip(X, y):
            update = self.eta * (label-self.predict(xi))
            self.w_[1:] += update * xi
            self.w_[0] += update
        return self

    #compute the net input i.e scalar sum of X and the weights plus the bias value
    def net_input(self, X):
        return np.dot(X, self.w_[1:]) + self.w_[0]

    #predict a classification for a sample of features X
    def predict(self, X):
        return np.where(self.net_input(X) >= 0.0, 1, -1)

    def init_plot(self):
        self.line.set_data([],[])
        return self.line,

    def animate(self, iteration_number, X, y):
        self.step_fit(X, y)
        x, y = self.plot_line(X)
        self.line.set_data(x, y)
        if iteration_number%2==0:
            self.ax.text(max(X[:,0])-0.5, min(X[:,1])+0.5, f'Iteration: {iteration_number}', fontsize=12)  # Update iteration number
        else:
            self.ax.text(max(X[:,0])-0.5, min(X[:,1])+0.5, 'Iteration:'+' '*8, fontsize=12, bbox=dict(facecolor='white', alpha=1))
        return self.line,

    def plot_line(self, X):
        x = []
        y = []
        slope = -(self.w_[0]/self.w_[2])/(self.w_[0]/self.w_[1])
        intercept = -self.w_[0]/self.w_[2]
        for i in np.linspace(np.amin(X[:,0])-0.5,np.amax(X[:,0])+0.5):
            #y=mx+c, m is slope and c is intercept
            x.append(i)
            y.append((slope*i) + intercept)

        return x, y

    def animated_fit(self, X, y):
        self.w_ = [random.uniform(-1.0, 1.0) for _ in range(1+X.shape[1])] #randomly initialize weights

        #here figure must be defined as a variable so it can be passed to FuncAnimation
        self.fig = plt.figure()

        #setting x and y limits with a 0.5 offset
        self.ax = plt.axes(xlim=(min(X[:,0])-0.5, max(X[:,0])+0.5), ylim=(min(X[:,1])-0.5, max(X[:,1])+0.5))

        #plotting our training points
        self.ax.plot(X[0:50, 0],X[0:50, 1], "bo", label=self.nameA)
        self.ax.plot(X[50:100, 0],X[50:100, 1], "rx", label=self.nameB)

        #labelling
        self.ax.legend(loc='upper left')

        #initialization of separation line and our animation object
        self.line, = self.ax.plot([], [], lw=2)
        anim = FuncAnimation(self.fig, self.animate, init_func=self.init_plot, fargs=(X, y,), frames=self.n_iter, interval=200, blit=True)
        anim.save('learning_process.gif', writer='imagemagick')

#import dataset
df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)

SPECIES_1= {'name':"Iris-setosa",'s':0,'end':50} #0:50 # small size
SPECIES_2= {'name':"Iris-versicolor",'s':50,'end':100} # 50:100
SPECIES_3= {'name':"Iris-virginica",'s':100,'end':150} # 100:150
spA,spB=SPECIES_2,SPECIES_3

#preparing our data to be understood by our model
X = df.iloc[np.r_[spA['s']:spA['end'],spB['s']:spB['end']], [0,2]].values
y = df.iloc[np.r_[spA['s']:spA['end'],spB['s']:spB['end']], 4].values
#y = np.where(y == 'Iris-setosa', -1, 1)
y = np.where(y == spB['name'], -1, 1)

ppn = Perceptron(eta=0.1, n_iter=150, nameA=spA['name'], nameB=spB['name']) #initializing a new perceptron
ppn.animated_fit(X, y)
```

    WARNING:matplotlib.animation:MovieWriter imagemagick unavailable; using Pillow instead.
    


    
![png](ML_overview_with_examples_files/ML_overview_with_examples_37_1.png)
    


## Mini-Batch Gradient Descent

The example before is a case of *stochastic gradient descent* (SGD), where weights are updated after each single example.

However, SGD can be quite erratic since all individual observation affect the search. If one observation has a large *loss*, then the weights will change abruptely to accomodate that, and this and that behaviour can jeopardize the convergence of gradient descent.

In the above example using the `iris` data set, one tried to separate one species from another using SGD (relying just on two explanatory variables for the sake of visualization). As one can see in the animation, the behavior can be pretty erratic, even this is a simple problem that can be solved easily with discriminant analysis techniques.

Given those limitations of SGD, the most common approach in ML is to group examples in batches and update the weights after each batch. One possibility is to include all example in one single batch which leads to the *batch gradient descent* method. In alternative, the examples can be grouped in *mini-batches* of tens or hundreds examples typically.

In short, there are three main possibilities:
1. *stochastic gradient descent*, where weights are updated after each single example;
2. *batch gradient descent*, where weights are updated once per epoch;
3. *mini-batch gradient descent*, which is somewhere in between the other two.

The example below is an adaptation of the earlier  *linear regression* example. It illustrates how weights update are done after each mini-batch of data (i.e. this is a case of *Mini-Batch Gradient Descent*). The *loss* for all examples in a mini-batch are simply combined by the `mean` function.


```python
#@title Script to learn from LR synthetic data, using mini batches, and train only
# This LR example illustrates gradient descent with PyTorch, train only and mini-batches
import matplotlib.pyplot as plt
import torch
import numpy as np
torch.manual_seed(42)

B = 10  # batch size
step_size = 0.01  # learning rate
iter = 20  # number epochs

############################################ Creating synthetic data
# Creating a function f(X) with a slope of -5
X = torch.arange(-5, 5, 0.1).view(-1, 1)  # view converts to rank-2 tensor with one column
func = -5 * X + 2
# Adding Gaussian noise to the function f(X) and saving it in Y
y = func + 0.4 * torch.randn(X.size())

# shuffle data
indices = torch.randperm(X.size(0))
X = X[indices]
y = y[indices]

####################################################### Gradient Descent
# initial weights
coeffs = torch.tensor([-20., -10.]).requires_grad_()

# defining the function for prediction (linear regression)
def calc_preds(x):
    return coeffs[0] + coeffs[1] * x

# Computing MSE loss for one batch of examples
def calc_loss_from_labels(y_pred, y):
    return torch.mean((y_pred - y) ** 2)

# lists to store losses for each epoch
training_losses = []

# epochs
for i in range(iter):
    # mini-batch gradient descent: weights are updated after each batch
    for idx_start in np.arange(0, X.shape[0], B):
        # create batch
        batch_X = X[idx_start:(idx_start + B), :]
        batch_y = y[idx_start:(idx_start + B):]
        # making a prediction in forward pass
        y_pred = calc_preds(batch_X)
        # calculating the loss between predicted and actual values
        loss = calc_loss_from_labels(y_pred, batch_y)
        # compute gradient
        loss.backward()
        with torch.no_grad():
            # update coeffs
            coeffs.sub_(coeffs.grad * step_size)
            # zero gradients (because they add up)
            coeffs.grad.zero_()

    # calculate loss on training data for this epoch
    y_pred_train = calc_preds(X)
    train_loss = calc_loss_from_labels(y_pred_train, y).item() # item returns the value of the tensor as a standard Python number.
    training_losses.append(train_loss)

print('batch size:', B)
print('coeffs found by gradient descent:', coeffs.detach().numpy()) #coeffs.requires_grad_(False))
# plot training losses along epochs
plt.plot(training_losses, '-g')
plt.xlabel('epoch')
plt.ylabel('loss (MSE)')
plt.show()
```

    batch size: 10
    coeffs found by gradient descent: [ 1.6460457 -5.0119123]
    


    
![png](ML_overview_with_examples_files/ML_overview_with_examples_40_1.png)
    


Let's look again that the `iris` data set and the animation of the search process, but now considering mini-batches in alternative to the stochastic strategy. If one tries to discriminate two species which are linearly separable, the perceptron is guaranteed to converge. However, a careful choice of the values of hyperparameters *batch size* and *learning rate* is needed when the classes are not linearly separable.


```python
#@title Script to create a Perceptron class and an animation for the iris data set, using mini batches
from matplotlib.animation import FuncAnimation
from functools import partial
import matplotlib.pyplot as plt
import numpy as np
import random
import pandas as pd

class Perceptron():
    #initialize hyperparameters (learning rate and number of iterations)
    def __init__(self, eta=0.1, n_iter=50, batch_size=10, nameA='', nameB=''):
        self.eta = eta
        self.n_iter = n_iter
        self.batch_size = batch_size
        self.nameA = nameA
        self.nameB = nameB

    def step_fit(self, X, y):
        for i in range(0, X.shape[0], self.batch_size):
            X_batch = X[i:i+self.batch_size]
            y_batch = y[i:i+self.batch_size]
            errors, Loss = self.loss(X_batch,y_batch)
            self.Loss = Loss
            #print(self.Loss)
            #print(errors)
            self.w_[1:] += self.eta * X_batch.T.dot(errors)
            self.w_[0] += self.eta * errors.sum()
        return self

    def net_input(self, X):
        return np.dot(X, self.w_[1:]) + self.w_[0]

    def predict(self, X):
        return np.where(self.net_input(X) >= 0.0, 1, -1)

    def loss(self, X, y):
        errors = y - self.predict(X)
        Loss = ((errors ** 2).sum()) ** 0.5
        return errors,Loss

    def shuffle(self, X, y):
        r = np.random.permutation(len(y))
        return X[r], y[r]

    def init_plot(self):
        self.line.set_data([],[])
        return self.line

    def animate(self, iteration_number, X, y):
        # Shuffling the data
        X, y = self.shuffle(X, y)
        # Fit
        self.step_fit(X, y)
        x, y = self.plot_line(X)
        self.line.set_data(x, y)
        #loss = self.loss(X, y)
        if iteration_number%2==0:
            self.ax.text(min(X[:,0])+2, min(X[:,1])+0.5, f'Iteration: {iteration_number}, Loss: {round(self.Loss,4)}', fontsize=12)  # Update iteration number
        else:
            self.ax.text(min(X[:,0])+2, min(X[:,1])+0.5, 'Iteration:'+' '*30, fontsize=12, bbox=dict(facecolor='white', alpha=1))
        return self.line,
        #self.ax.text(min(X[:,0])+2, min(X[:,1])+0.5, f'Iteration: {iteration_number}, Loss: {self.Loss}', fontsize=12)

    def plot_line(self, X):
        x = []
        y = []
        slope = -(self.w_[0]/self.w_[2])/(self.w_[0]/self.w_[1])
        intercept = -self.w_[0]/self.w_[2]
        for i in np.linspace(np.amin(X[:,0])-0.5,np.amax(X[:,0])+0.5):
            #y=mx+c, m is slope and c is intercept
            x.append(i)
            y.append((slope*i) + intercept)
        return x, y

    def animated_fit(self, X, y):
        self.Loss= 0
        self.w_ = [random.uniform(-1.0, 1.0) for _ in range(1+X.shape[1])] #randomly initialize weights

        #here figure must be defined as a variable so it can be passed to FuncAnimation
        self.fig = plt.figure()

        #setting x and y limits with a 0.5 offset
        self.ax = plt.axes(xlim=(min(X[:,0])-0.5, max(X[:,0])+0.5), ylim=(min(X[:,1])-0.5, max(X[:,1])+0.5))

        #plotting our training points
        self.ax.plot(X[0:50, 0],X[0:50, 1], "bo", label=self.nameA)
        self.ax.plot(X[50:100, 0],X[50:100, 1], "rx", label=self.nameB)

        #labelling
        self.ax.legend(loc='upper left')

        #initialization of separation line and our animation object
        self.line, = self.ax.plot([], [], lw=2)
        #anim = FuncAnimation(self.fig, self.animate, init_func=self.init_plot, fargs=(X, y,), frames=self.n_iter, interval=200, blit=True)
        anim = FuncAnimation(self.fig, partial(self.animate,X=X,y=y) , init_func=self.init_plot, frames=self.n_iter, interval=200) #, blit=True) #partial(self.animate,X=X,y=y)
        anim.save('learning_process.gif') #, writer='imagemagick')

#import dataset
df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)

SPECIES_1= {'name':"Iris-setosa",'s':0,'end':50} #0:50 # small size
SPECIES_2= {'name':"Iris-versicolor",'s':50,'end':100} # 50:100
SPECIES_3= {'name':"Iris-virginica",'s':100,'end':150} # 100:150
spA,spB=SPECIES_2,SPECIES_3

#preparing our data to be understood by our model
X = df.iloc[np.r_[spA['s']:spA['end'],spB['s']:spB['end']], [0,2]].values
y = df.iloc[np.r_[spA['s']:spA['end'],spB['s']:spB['end']], 4].values
y = np.where(y == spB['name'], -1, 1) # discrete response

# Creating an instance of a Perceptron object
ppn = Perceptron(eta=0.0001, n_iter=100, batch_size=25, nameA=spA['name'], nameB=spB['name'])
ppn.animated_fit(X, y)

```


    
![png](ML_overview_with_examples_files/ML_overview_with_examples_42_0.png)
    


## Train and test

So far, we have trained our model using all the available examples. The risk is that the model parameters overfit to the training data, and fail to generalize to new examples, which is the goal of Machine Learning.

In ML, it is costumary to distinguisgh three subsets of the examples:
  1. *training* data set;
  2. *development* or *dev* data set; this is also either called *test* or *validation* data set.
  3. *holdout data set*  to provide an unbiased evaluation of a final model; in the literature, this can be confusingly called *test* or *validation* data set.

In fact, the literature on machine learning often reverses the meaning of *validation* and *test* sets. In `Fastai`, the development data set is refered to as `valid` as in `RandomSplitter(valid_pct=0.2, seed=42)`, but the function in `scikit-learn` to split examples is called `train_test_split`.

To prevent overfitting, one straightforward approach is to split the set of examples in two sets: *train* and *test* (which is also called ). Then, :
  - *gradient descent* is performed over the training set; but
  - *loss* is also computed over the dev set.

The example below shows an adaptation of the code for the *linear regression* example where losses are computed and reported over the dev set.


```python
#@title Script to learn from LR synthetic data, using mini batches, and train&test
# This example illustrates: gradient descent with PyTorch, train&test,  mini-batch
import matplotlib.pyplot as plt
import torch
import numpy as np
from sklearn.model_selection import train_test_split
torch.manual_seed(42)

B=10 # batch size
step_size = 0.1 # learning rate
iter=20 # number epochs

############################################ Creating synthetic data
# Creating a function f(X) with a slope of -5
X = torch.arange(-5, 5, 0.1).view(-1, 1) # view converts to rank-2 tensor with one column
func = -5 * X + 2
# Adding Gaussian noise to the function f(X) and saving it in Y
y = func + 0.4 * torch.randn(X.size())

##################################### Create train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

####################################################### Gradient Descent
# initial weights
coeffs=torch.tensor([-20.,-10.]).requires_grad_()

# defining the function for prediction (linear regression)
def calc_preds(x):
  return coeffs[0] + coeffs[1] * x

# Computing MSE loss for one batch of exemples
def calc_loss_from_labels(y_pred, y):
  return torch.mean((y_pred - y) ** 2)

# lists to store losses for each epoch
training_losses=[]; test_losses=[]

# epochs
for i in range(iter):
  # calculating loss as in the beginning of an epoch and storing it
    y_pred = calc_preds(X_train)
    training_losses.append(calc_loss_from_labels(y_pred, y_train).tolist())
    y_pred = calc_preds(X_test)
    test_losses.append(calc_loss_from_labels(y_pred, y_test).tolist())
    # mini-batch gradient descent: weight are updated after each batch
    for idx_start in np.arange(0,X_train.shape[0],B):
        # create batch
        batch_X=X_train[idx_start:(idx_start+B),:]
        batch_y=y_train[idx_start:(idx_start+B):]
        # making a prediction in forward pass
        y_pred = calc_preds(batch_X)
        # calculating the loss between predicted and actual values
        loss = calc_loss_from_labels(y_pred, batch_y)
        # compute gradient
        loss.backward()
        with torch.no_grad():
            # update coeffs
            coeffs.sub_(coeffs.grad * step_size)
            # zerofy gradients (because they add up)
            coeffs.grad.zero_()

print('batch size:', B)
print('coeffs found by gradient descent:',coeffs.detach().numpy())
# plot training and test losses along epochs
plt.plot(training_losses, '-g',  test_losses, '-r')
plt.gca().legend(('train','test'))
plt.ylim(0, 5)
plt.xlabel('epoch')
plt.ylabel('loss (MSE)')
plt.show()
```

    batch size: 10
    coeffs found by gradient descent: [ 2.0202172 -4.7577853]
    


    
![png](ML_overview_with_examples_files/ML_overview_with_examples_45_1.png)
    


High level packages like `fastai` alows us to very easily specify what is the proportion of examples which is kept aside for *testing* to prevent *overfitting* the parameters.  In general, data sets with $N$ examples is partitioned into a subset with, say $0.2 \times N$ examples for testing and $0.8 \times N$ examples for training like in the notebook  [Lesson1_00_is_it_a_bird_creating_a_model_from_your_own_data.ipynb](Lesson1_00_is_it_a_bird_creating_a_model_from_your_own_data.ipynb):
    
    dls = DataBlock(
        blocks=(ImageBlock, CategoryBlock),
        get_items=get_image_files,
        splitter=RandomSplitter(valid_pct=0.2, seed=42),
        get_y=parent_label,
        item_tfms=[Resize(192, method='crop')] # try crop instead of squish
    ).dataloaders(path)

where `RandomSplitter(valid_pct=0.2, seed=42)` indicates that 20% of the examples are used for testing.

The *training* data set is used to search for the optimal set of weights for the model, typically by iteratively updating the weights from a initial set of weights using *gradient descent* over the loss. The *test* data set is used to compute the same loss metric over an independent set of examples.

By comparing the training and test losses it is possible to assess issues in model behaviour like *high bias/underfitting*,  *high variance/overfitting*,  and *unrepresentativeness* of either training or validation set.

## Data preprocessing and data augmentation

Preprocessing tabular data will be discussed in the "Tabular data" section below.

Preprocessing images typically comes down to (1) resizing them to a particular size (2) normalizing the color channels (R,G,B) using a mean and standard deviation. These are referred to as image transformations.

In addition, one typically performs what is called data augmentation during training (like random cropping and flipping) to make the model more robust and achieve higher accuracy. Data augmentation is also a great technique to increase the size of the training data.

Image transformations can be achieved with *geometric image transformation*. This is the process of altering the geometric properties of an image, such as its shape, size, orientation, or position. It involves applying mathematical operations to the image pixels or coordinates to achieve the desired transformation.

Code and examples for data augmentation and transformation with PyTorch can be found in the documentation https://pytorch.org/vision/main/auto_examples/.

A typical code for data transformation for color image classification can be found below. It resizes the input image, possibly flips horizontally the image, and normalizes each of the image RGB channel.

```
transforms = v2.Compose([
    v2.RandomResizedCrop(size=(224, 224), antialias=True),
    v2.RandomHorizontalFlip(p=0.5),
    v2.ToDtype(torch.float32, scale=True),
    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
out = transforms(img)

plot([img, out])
```

# Loss functions for classification (cross-entropy)

Classification problems have categorical labels. Therefore, the model predictions should return the most likely label for each example.

While in regression the model's output is typically an unbounded response variable (for instance, it is $f(x;a,b) = a\, x + b$ in simple linear regression), for classification problems, it is more convenient to have:
1. one output per label;
2. each output being a value between 0 and 1 that can be interpreted as the probability of the label.


<img src="https://drive.google.com/uc?export=view&id=1iD519g8QbBmOGp9SiOQsIneJnWg53SMQ" width="600" >

Therefore, it is usual to have a model that outputs scores $f_1({\rm \bf x};{\rm \bf w_1}), \dots , f_k({\rm \bf x};{\rm \bf w_k})$ for each of the $k$ possible labels, and an additional model component that converts those *raw* scores into probability-like values for the labels.

You saw that kind of probabilistic output when you trained and deployed an image classifier in notebook [Lesson1_00_is_it_a_bird_creating_a_model_from_your_own_data.ipynb](Lesson1_00_is_it_a_bird_creating_a_model_from_your_own_data.ipynb). When you did predict the label for a new example with

    is_bird,_,probs = learn.predict('bird.jpg')
    print(is_bird,probs)

you got a vector of estimated probabilities like the following:

    bird tensor([0.9980, 0.0020])    

where the values `0.9980, 0.0020` correspond, respectively, to labels *bird* and *forest*.




### Softmax



The unormalized model outputs $f_1, \dots, f_k$ are called *scores*, *logits* or *raw* outputs. Each score $z_i=f_i({\rm \bf x};{\rm \bf w_i})$ is converted into a [0,1] value by the *softmax* function:

$$p_i=\frac{\exp(z_i)}{\sum_{j=1}^k \exp(z_j)} ~~ {\rm which~implies~that} ~~ 0<p_i \le 1.$$

After that transformation, the classification model's probabilistic output is a vector of values $(p_1,\dots,p_k)$, with $p_i \ge 0$ and $\sum p_i=1$ as required for  probability distributions. The predicted label is the one with highest $p$.


### Comparing target and predicted probability distributions



While for *regression* the loss is a dissimilarity between the actual labels $y_1,\dots,y_n$ and the predicted labels $\hat{y}_1,\dots,\hat{y}_n$ for the set with $n$ examples, in *classification*  with *softmax* the loss is then a dissimilarity between the actual labels $y_1,\dots,y_n$ and the $n$ probability vectors $(p_1,\dots,p_k)_1, \dots, (p_1,\dots,p_k)_n$.

To compare vectors of the same dimension, one can express each target label $y_i$ as a probabilitity distribution with 0 uncertainty. Suppose that there are 3 different possible labels (as in the `iris` data set) and the actual label of the example is the first label: then, the *target distribution* would be $(1,0,0)$ which can be  compared with the model's output probability distribution $(p_1,p_2,p_3)$. This will be illustrated with an example below.




###Loss functions in fastai



A list of `fastai` loss functions is available in https://docs.fast.ai/losses.html. In general, they are simple adaptations of `pytorch` loss functions listed in https://pytorch.org/docs/stable/nn.html#loss-functions.  The most common *loss* function for classification is called *cross-entropy*.

Notebook [Lesson1_00_is_it_a_bird_creating_a_model_from_your_own_data.ipynb](Lesson1_00_is_it_a_bird_creating_a_model_from_your_own_data.ipynb) creates the learner with:

    learn = vision_learner(dls, resnet18, metrics=error_rate)

`vision_learner` allows to explicitely define the loss function  with the argument `loss_func`. Since it is not explicitely defined in the code above, we can check the default which is stored in property `loss_func`:

    learn.loss_func

which returns `FlattenedLoss of CrossEntropyLoss()`. This `fastai` loss is described in the Pytorch documentation as the `nn.CrossEntropyLoss` class  [https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html). Hence, the loss function that is applied in the example in the notebook is  `torch.nn.CrossEntropyLoss` that computes the cross entropy loss between input logits and target.



### Cross-entropy and one-hot encoding



So, what is the *loss* you see in the output when you train an image classifier in notebook [Lesson1_00_is_it_a_bird_creating_a_model_from_your_own_data.ipynb](Lesson1_00_is_it_a_bird_creating_a_model_from_your_own_data.ipynb)?

    epoch 	train_loss 	valid_loss 	error_rate 	time
    0 	    1.184412 	  1.291252 	  0.364865 	  00:03


Suppose that your model (`resnet18`) was trained to classify photos of birds and forests. The last layer of the model outputs *scores* $z_1=f_1(x_1,\dots,x_n), z_2=f_2(x_1,\dots,x_n)$, where $z_1$ is associated to *bird*, $z_2$ is associated to *forest*.

By default the loss is computed with *cross-entropy* which will allow to measure the dissimilarity between the *distribution* of scores with the target value, for all the examples in a batch of examples.

Consider an hypothetical case with 4 examples and 3 classes (i.e. 3 possible labels, as for the `iris`data set). Suppose that the output of the `predict` method for those 4 examples area the *scores* in the following 4$\times$3 tensor (rows are examples, columns are classes). Columns could for instance correspond to *setosa*, *versicolor* and *virginica*.


```python
import torch
scores=torch.tensor([[4.5,2.1,0.2],
                     [1.3,8.3,0.8],
                     [1.2,1.5,4.2],
                     [5.1,0.4,2.3]])
```

Looking at the tensor above, it looks like the first and fourth examples have a higher likelihood of being *setosa*, while the second example is more likely to be *versicolor* and the third one seems to be *virginica*. Let's apply *softmax* to convert the raw outputs into probability-like values.


```python
 softs = scores.softmax(dim=1)
 torch.set_printoptions(sci_mode=False)
 softs
```




    tensor([[    0.9056,     0.0822,     0.0123],
            [    0.0009,     0.9985,     0.0006],
            [    0.0446,     0.0602,     0.8953],
            [    0.9347,     0.0085,     0.0568]])



It is clear that the softmax rule did amplify the difference in values for each example. In each row there is one value close to 1 and the remainder values are close to 0. For each example, *cross-entropy* compares the target distribution with the probability distribution returned by *softmax*.

Let's suppose that in fact, the first and last examples have actually label *setosa*, the second is actually *versicolor* and the third is actually *virginica*. Then the target distribution will be 1 for the correct label and 0 for the remaining labels. This is known as  *one-hot encoding* and it's illustrated by the rows of the following tensor (each row is one example):


```python
one_hot_target=torch.tensor([[1.,0.,0.],
                            [0.,1.,0.],
                            [0.,0.,1.],
                            [1.,0.,0.]])
standard_target=torch.tensor([0,1,2,0],dtype=torch.long)
```

Cross-entropy loss measures the dissimilarity between the probability distribution $(p_1,p_2,p_3)$ returned by *softmax* and the target distribution $(t_1,t_2,t_3)$ for the $i$-th example with the expression, and takes values between 0 (optimal value associated to minimum uncertainty) and 1 (maximum value associated to maximum uncertainty, i.e. all probabilities are equal):

$$L_i=-\left( t_1 \, \log(p_1) + t_2 \, \log(p_2) + t_3 \, \log(p_3) \right) \in [0,1].$$

In the expression above, we suppose that the probabilities are non zero.
For the whole batch of $n$ examples, the cross-entropy loss is given by the average of the $n$ individual loss values:

$$L=\frac{1}{n} \left( L_1+L_2+ \dots,L_n\right).$$

This can be computed with `nn.CrossEntropyLoss()` directly from the *scores* and *target* values as shown below:


```python
#! pip install torch -U
import torch.nn as nn
# compute loss
loss = nn.CrossEntropyLoss()
output = loss(scores, one_hot_target)
print('Cross Entropy Loss: ', output)
# or, simply
output = loss(scores, standard_target)
print('Cross Entropy Loss: ', output)
```

    Cross Entropy Loss:  tensor(0.0697)
    Cross Entropy Loss:  tensor(0.0697)
    

# Assessing ML performance

## Error rate and accuracy

Notebook [Lesson1_00_is_it_a_bird_creating_a_model_from_your_own_data.ipynb](Lesson1_00_is_it_a_bird_creating_a_model_from_your_own_data.ipynb) uses  `error_rate` when creating the `learner`:

    learn = vision_learner(dls, resnet18, metrics=error_rate)
    learn.fine_tune(3)

where `dls` is a `DataLoaders` object with the input data, `resnet18` is a deep CNN model. The argument `metrics=error_rate` indicates that the overall performance of the algorithm is measured proportion of mismatches between the set of actual labels $y_1, \dots , y_n$ and the set of predicted labels $\hat{y}_1, \dots , \hat{y}_n$. A very similar metric is `accuracy` which is simply `1-error_rate`.

When one trains a classifier in `fastai`, the output for the training epochs looks like

<img src="https://drive.google.com/uc?export=view&id=1PJ36nJ-Isu-LThJTFwRj7XwVY5qWeRfJ" width="300" >

One could wonder how the  `error_rate` is computed when the data set is divided into *training* and *test* sets (as mentioned earlier, the test set is also called a *development set* of examples). In `fastai`, because the training set and test set are integrated into a single class (`dataloaders`),  by default the metrics displayed during training (as in the output above) use the test set, so the `error_rate`is computed over the validation set.

At this point it should be clear what `epoch`, `train_loss`, `valid_loss` and `error_rate` in the above training output are:
1. `epoch`: number of times that the whole set of examples has been used for prediction;
2. `train_loss`: the value of the loss function computed with the model weights for that epoch and the training examples;
3. `valid_loss`: the value of the loss function computed with the model weights for that epoch and the validation examples;
4. `error_rate`: proportion of mismatches between the set of actual labels $y_1, \dots , y_n$ and the set of predicted labels $\hat{y}_1, \dots , \hat{y}_n$ computed with the model weights for that epoch and the validation examples.

There are many metrics other than `error_rate` for measuring the performance of *regression* and *classification* problems. For instance, `scikit-learn` provides the metrics listed in
https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics. The package `fastai`  includes  all metrics from `scikit-learn` and some additional ones. The documentation is available at https://docs.fast.ai/metrics.html.




## Confusion matrix (error matrix)

The confusion matrix, also called error matrix, is a very useful tool to evaluate the precision of a classifier.

To compute the error matrix for a classifier ${\bf f_{\bf w}}({\bf x})$ trained with a given training set of examples, the steps are the following.

1. Consider a test set of examples $({\bf x}, y)$ that were not used for training;

2. Predict the labels $\hat{y}={\bf f_{\bf w}}({\bf x})$ for all examples in the test set;

3. Compare the predicted labels $\hat{y}$ with the true labels $y$ and create a two-way table where the rows represent the actual labels ($y$)  and the columns represent the predicted labels $\hat{y}$.

### How to calculate and interpret a confusion matrix

The following code illustrated how to compute a confusion matrix for a classification task with two classes, labeled 0 and 1, and plot the result with `matplotlib.

The matrix compares the true labels of the examples `y_true` with the labels predicted by the classifier `y_pred`


```python
#@title Script that computes a confusion metrics from lists of predicted and actual labels
from sklearn.metrics import confusion_matrix
import numpy as np
import matplotlib.pyplot as plt # to plot
# Actual labels
y_true = np.array([0, 1, 0, 1, 1, 0, 1, 0, 0, 1])
# Predicted labels
y_pred = np.array([0, 1, 1, 1, 0, 0, 1, 0, 1, 1])
# Compute confusion matrix
cm = confusion_matrix(y_true, y_pred)
# Define class labels
classes = ['Zero', 'One']
# Plot confusion matrix
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.colorbar()
tick_marks = np.arange(len(classes))
plt.xticks(tick_marks, classes, rotation=45)
plt.yticks(tick_marks, classes)
plt.xlabel('Predicted label')
plt.ylabel('True label')
# Fill in confusion matrix with values
thresh = cm.max() / 2.
for i, j in np.ndindex(cm.shape):
    plt.text(j, i, format(cm[i, j], 'd'),
             horizontalalignment='center',
             color='white' if cm[i, j] > thresh else 'black')
plt.tight_layout()
plt.show()

```


    
![png](ML_overview_with_examples_files/ML_overview_with_examples_74_0.png)
    


### Accuracy metrics derived from the confusion matrix

In general, if there are $n$ different label values, the error matrix is $n \times n$. For simplicity, let's just consider the $2 \times 2$ error matrix, where correct predictions are called TP or TN, and the errors FP or FN.

|           | Predicted Positive | Predicted Negative |
|-----------|--------------------|--------------------|
| Actual Positive | TP=True Positive     | FN=False Negative    |
| Actual Negative | FP=False Positive| TN=True Negative|

The following metrics are computed from the error matrix:

---

1. Classification **accuracy**.

$${\rm accuracy}=\frac{{\rm TP}+{\rm TN}}{{\rm TP}+{\rm FN}+{\rm FP}+{\rm TN}}.$$

If the number of actual positive examples (TP+FN) is very different from the number of negative examples (FP+TN), the largest number is going to dominate the result. For instance, is 5% of some area is burned, but the classifier just labels all pixels as non-burned, the classification accuracy will be 95%.

For that example, the error matrix will look something that the following one.


|           | Predicted Burned | Predicted Non burned |
|-----------|--------------------|--------------------|
| Actual Burned | TP=0   | FN=50    |
| Actual Non burned | FP=0| TN=9050|

---

2. **Precision**, focused on predicted positives

$${\rm precision}=\frac{{\rm TP}}{{\rm TP}+{\rm FP}}.$$

This metric focusses only on the positive examples. For the burned area example above, the precision is not defined since no predictions are positive. Consider this other example, where one aims af finding greenhouses in a certain region.


|           | Predicted Greenhouse | Predicted Other |
|-----------|--------------------|--------------------|
| Actual Greenhouse | TP=80   | FN=20    |
| Actual Other  | FP=10| TN=9090|

In that case, precision is $80/(80+10) \approx 89\%$, while overall classification accuracy is $91.7\%$.

Precision is the complement of **commission error**:

$${\rm CE}=\frac{{\rm FP}}{{\rm TP}+{\rm FP}}.$$

---

3. **Recall**, focused on actual positives, and also called **sensitivity** or **true positive rate (TPR)**

$${\rm recall}=\frac{{\rm TP}}{{\rm TP}+{\rm FN}}.$$

The denominator here is the total number of actual positives. This is an interesting metric if we are focused on having a very low error on missing an actual positive (a typical example is missing a tumor in medecine).

For the burned area example, the classifier has the worst possible outcome since it misses all actual positives, and therefore its ${\rm recall}=0\%$. However, a similarly arbitrary classifier that would just predict the *positive* label for all examples would have a perfect ${\rm recall}=100\%$. For the greenhouse example, we have ${\rm recall}=80\%$.

Recall is the complement of **omission error**:

$${\rm OE}=\frac{{\rm FN}}{{\rm TP}+{\rm FN}}.$$

For instance, one wants the *sensitivity* of a disease test to be high to ensure that sick people are detected.

---

4. **Specificity**, is focused on actual negatives, and is also called **true negative rate (TNR)**

$${\rm specificity}=\frac{{\rm TN}}{{\rm TN}+{\rm FP}}.$$

For instance, one wants the *specificity* of a disease test to be high to prevent healthy people from being labeled as sick.

---

5. **F1 score**, which averages equally *precision* and *recall*

$${\rm F1~score}= 2 \times \frac{{\rm precision} \times {\rm recall}}{{\rm precision} + {\rm recall}}=\frac{{\rm 2\, TP}}{{\rm 2\, TP}+{\rm FP}+{\rm FN}}.$$

This is also known as the **Dice coefficient**. For the burned area example ${\rm F1~score}=0$ since in fact the F1 score is the *harmonic mean* of precision and recall. This metric still does not take into consideration true negatives (TN) and is criticized for giving the same importance to precision and recall.

---




`scikit-learn` offers a function that outputs a **classification report** that includes precision, recall and F1 score, for both possible labelings of the examples.


```python
#@title Script that computes a classification report from lists of predicted and actual labels
from sklearn.metrics import classification_report
import numpy as np
# Actual labels
y_true = np.array([0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0])
# Predicted labels
y_pred = np.array([0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0])
# Compute confusion matrix
report = classification_report(y_true, y_pred)
print(report)
```

                  precision    recall  f1-score   support
    
               0       0.83      0.62      0.71         8
               1       0.57      0.80      0.67         5
    
        accuracy                           0.69        13
       macro avg       0.70      0.71      0.69        13
    weighted avg       0.73      0.69      0.70        13
    
    

The output above illustrates the fact that *positive* or *negative* are not interchangeable. Since true negatives are not used to compute *precision* and *recall*, accuracy measures are not invariant with respect to labeling.

### Create a confusion matrix with `fastai`

In [Lesson2_edited_book_02_production.ipynb](Lesson2_edited_book_02_production.ipynb), to understand in detail which  mistakes the model is making,  a confusion matrix (also called an *error matrix*) was created with

    interp = ClassificationInterpretation.from_learner(learn)

    interp.plot_confusion_matrix()

which output was:

<img src="https://drive.google.com/uc?export=view&id=1zsTJ7wh6KneWG7_QuXijy5LDlJiu3K-F" width="300" >


The comment in the notebook for this figure is the following: The rows represent all the black, grizzly, and teddy bears in our dataset, respectively. The columns represent the images which the model predicted as black, grizzly, and teddy bears, respectively. Therefore, the diagonal of the matrix shows the images which were classified correctly, and the off-diagonal cells represent those which were classified incorrectly. This is one of the many ways that fastai allows you to view the results of your model. *It is (of course!) calculated using the validation set*.


## Cross-validation

When assessing accuracy is used to tune the model hyper-parameters, one should not rely solely on the training set to avoid overfitting. This means that three different sets of examples must be considered:

1.  *training* data set is used to search for the optimal set of weights for the model, typically by iteratively updating the weights from a initial set of weights using *gradient descent* over the loss.
2.  *validation* data set is used to compute the same loss metric over an independent set of examples.
3. *test* set to evaluate the performance of the classifier.

However, by partitioning the available data into three sets, we drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets (see https://scikit-learn.org/stable/modules/cross_validation.html)

A solution to this problem is a procedure called **cross-validation** (CV for short. A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. In the basic approach, called $k$-fold CV, the training set is split into k smaller sets (other approaches are described below, but generally follow the same principles). The following procedure is followed for each of the k folds:

* A model is trained using $k-1$ of the folds as training data;

* The resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).

The performance measure reported by $k$-fold cross-validation is then the average of the values computed in the loop. This approach can be computationally expensive, but does not waste too much data.

<img src="https://scikit-learn.org/stable/_images/grid_search_cross_validation.png" width="500" >

The following code uses `scikit-learn` to fit a multi layer perceptron (MLP) to classify the `iris` data set.  To implement *cross-validation*, the code relies on `cross_val_score`. Note that `X` and `y` are not split in `train`and `test` in this example since `cross validation` is used to estimate accuracy.


```python
import numpy as np
from sklearn.model_selection import cross_val_score
from sklearn import datasets
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.pipeline import make_pipeline

X, y = datasets.load_iris(return_X_y=True)

# uses cross-entropy as loss function
pipe = make_pipeline(StandardScaler(),
                     MLPClassifier(solver='sgd',hidden_layer_sizes=(10, 5, 3), max_iter=1000,learning_rate_init=0.01,momentum=0.9))
pipe.fit(X, y)
scores = cross_val_score(pipe, X, y, cv=5,scoring='accuracy') # by default, StratifiedKFold for y categorical
print(scores.mean(), scores.std())
```

    0.9200000000000002 0.09797958971132714
    

To obtain *cross-validation* predicted values, one can use `cross_val_predict` as in the following example.


```python
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import cross_val_predict
y_pred=cross_val_predict(pipe, X, y, cv=5)
confusion_matrix(y,y_pred)
```




    array([[50,  0,  0],
           [ 0, 47,  3],
           [ 0,  1, 49]])



##Learning and validation curves

Since cross validation allow us to obtain estimates of accuracy for a classifier, one can use it to analyze how training and validation accuracy vary with sample size. This can be useful to understand if the reference data set is large enough (with respect to features and the number of examples) and if the model should be made simpler or more complex (e.g. varying parameters and/or regularization).


```python
from sklearn.datasets import load_wine # 3 classes, 178 examples, 13 variables; One could also try load_digits,fetch_olivetti_faces
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt

# load dataset
dataset = load_wine()
X,y=dataset.data, dataset.target

pipe = make_pipeline(StandardScaler(),RandomForestClassifier())

train_sizes, train_scores, test_scores =learning_curve(estimator=pipe,
                               X=X,
                               y=y,
                               train_sizes=np.linspace(0.1, 1.0, 10),
                               cv=10,
                               n_jobs=1)

train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)

plt.plot(train_sizes, train_mean,
         color='blue', marker='o',
         markersize=5, label='Training accuracy')

plt.fill_between(train_sizes,
                 train_mean + train_std,
                 train_mean - train_std,
                 alpha=0.15, color='blue')

plt.plot(train_sizes, test_mean,
         color='green', linestyle='--',
         marker='s', markersize=5,
         label='Validation accuracy')

plt.fill_between(train_sizes,
                 test_mean + test_std,
                 test_mean - test_std,
                 alpha=0.15, color='green')

plt.grid()
plt.title('Learning curve')
plt.xlabel('Number of training examples')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.ylim([0.5, 1.03])
plt.tight_layout()
plt.show()
```


    
![png](ML_overview_with_examples_files/ML_overview_with_examples_90_0.png)
    


While the `learning curve` shows accuracy vs the number of examples, a `validation curve` helps to understand how accuracy varies with model parameter values and choose the best parameter value. In `scikit-learn` a parameter is indicated by `modelName___parameterName` (e.g. `randomforestclassifier__max_depth`).


```python
from sklearn.datasets import load_digits
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import validation_curve
import matplotlib.pyplot as plt

# load dataset
dataset = load_digits()
X,y=dataset.data, dataset.target

pipe = make_pipeline(StandardScaler(),RandomForestClassifier())

param_range = [5, 10, 15, 20, 25, 30]
train_scores, test_scores = validation_curve(
                estimator=pipe,
                X=X,
                y=y,
                param_name='randomforestclassifier__max_depth',
                param_range=param_range,
                cv=5)

train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)

plt.plot(param_range, train_mean,
         color='blue', marker='o',
         markersize=5, label='Training accuracy')

plt.fill_between(param_range, train_mean + train_std,
                 train_mean - train_std, alpha=0.15,
                 color='blue')

plt.plot(param_range, test_mean,
         color='green', linestyle='--',
         marker='s', markersize=5,
         label='Validation accuracy')

plt.fill_between(param_range,
                 test_mean + test_std,
                 test_mean - test_std,
                 alpha=0.15, color='green')

plt.grid()
plt.title('Validation curve')
plt.legend(loc='lower right')
plt.xlabel('Parameter max_depth')
plt.ylabel('Accuracy')
plt.ylim([0.85, 1.03])
plt.tight_layout()
plt.show()
```


    
![png](ML_overview_with_examples_files/ML_overview_with_examples_92_0.png)
    


## Searching the best set of hyperparameters

Library `scikit-learn` provides `GridSearchCV` which allows to compute cross validation estimation of the performance of the model using combinations of hyperparameter values. The combination that leads to the best score is returned as the `best_params_` property.


```python
from sklearn.datasets import load_digits
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
import matplotlib.pyplot as plt

# load dataset
dataset = load_digits()
X,y=dataset.data, dataset.target

pipe = make_pipeline(StandardScaler(),RandomForestClassifier())

max_depth_range=[5, 10, 15, 20, 25, 30]
ccp_alpha_range=[0.0001,0.001, 0.01]

param_grid = [{'randomforestclassifier__max_depth': max_depth_range,'randomforestclassifier__ccp_alpha': ccp_alpha_range}]

gs = GridSearchCV(estimator=pipe,
                  param_grid=param_grid,
                  scoring='accuracy',
                  refit=True,
                  cv=5)
gs = gs.fit(X, y)
print(gs.best_score_)
print(gs.best_params_)


```

    0.9427019498607242
    {'randomforestclassifier__ccp_alpha': 0.0001, 'randomforestclassifier__max_depth': 30}
    

An alternative to `GridSearchCV` is `RandomizedSearchCV`. Here, we follow the standard practice of holding out a test set, using cross-validation to choose the best parametters and finally estimate accuracy with the test set.


```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import RandomizedSearchCV, train_test_split
from sklearn.metrics import accuracy_score

# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Create a Random Forest classifier
rf_classifier = RandomForestClassifier()
# Define the hyperparameter grid for RandomizedSearchCV
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 5, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}
# Create a RandomizedSearchCV instance
random_search = RandomizedSearchCV(estimator=rf_classifier, param_distributions=param_grid, n_iter=10, cv=5, random_state=42)
# Perform the random search
random_search.fit(X_train, y_train)
# Get the best hyperparameters and model
best_params = random_search.best_params_
print(best_params)
best_model = random_search.best_estimator_  # retrieve the best model
# Make predictions on the test set using the best model
y_pred = best_model.predict(X_test)
# Evaluate the accuracy of the best model
accuracy = accuracy_score(y_test, y_pred)
print("Best Model Accuracy:", accuracy)
```

    {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_depth': 10}
    Best Model Accuracy: 1.0
    

## ROC curve and AUC

In case of binary classification, we can focus on the probability of the positive class. The usual threshold for prediction is 50% but there is no *a priori* reason to choose that threshold.

Therefore, we can change this threshold value of 50%. For instance, if the threshold is set as 70%, the model predicts an observation as positive only if the predicted probability is greater than 70%. Adjusting the threshold value changes some of the predicted labels and the overall performance of the classifier. Usually, a high threshold makes the prediction of the positive class less likely. This tends to increase both the false positive rate (FPR) and the true positive rate (TPR).

ROC curves typically feature true positive rate (TPR) on the Y axis, and false positive rate (FPR) on the X axis. This means that the top left corner of the plot is the ideal point - a FPR of zero, and a TPR of one (https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html).

<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/13/Roc_curve.svg/220px-Roc_curve.svg.png" width="400" >
<img src="https://miro.medium.com/v2/resize:fit:702/format:webp/0*pF07ZmzBqbvkvqJO.png" width="400" >

The **AUC** is the area under the ROC curve. Its maximum value is 1, when the classifier is optimal. The *AUC* does not depend on the classification threshold, since it integrates all thresholds.

The following code shows how to compute AUC directly from the estimated classification probabilities. For instance, probabilities could be the output of a NN with a *softmax* output layer.


```python
from sklearn.metrics import roc_auc_score
# Assuming you have predicted probabilities or scores for the positive class
y_true = [0, 1, 1, 0, 1]
y_scores = [0.2, 0.8, 0.6, 0.3, 0.9]
# Compute the AUC score
auc_score = roc_auc_score(y_true, y_scores)
print("AUC:", auc_score)

```

    AUC: 1.0
    

The following code draws the ROC curve and estimates the AUC for a two class problem (`iris` data set restricted to the most similar classes *versicolor* and *virginica*) using a MLP classifier and 6-fold cross-validation stratified by class.


```python
import numpy as np
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt
from sklearn.metrics import auc
from sklearn.metrics import RocCurveDisplay
from sklearn.model_selection import StratifiedKFold
from sklearn.neural_network import MLPClassifier

iris = load_iris()
target_names = iris.target_names
X, y = iris.data, iris.target
X, y = X[y != 0], y[y != 0] # drop the 'setosa' class
n_samples, n_features = X.shape

cv = StratifiedKFold(n_splits=6)
clf = MLPClassifier(solver='sgd',hidden_layer_sizes=(10, 5, 2), max_iter=300,learning_rate_init=0.01,momentum=0.9)

tprs = []
aucs = []
mean_fpr = np.linspace(0, 1, 100) # threshold to be considered to draw the ROC curve

fig, ax = plt.subplots(figsize=(6, 6))
for fold, (train, test) in enumerate(cv.split(X, y)):
    clf.fit(X[train], y[train])
    viz = RocCurveDisplay.from_estimator(
        clf,
        X[test],
        y[test],
        name=f"ROC fold {fold}",
        alpha=0.3,
        lw=1,
        ax=ax,
    )
    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)
    interp_tpr[0] = 0.0
    tprs.append(interp_tpr)
    aucs.append(viz.roc_auc)
ax.plot([0, 1], [0, 1], "k--", label="chance level (AUC = 0.5)")

mean_tpr = np.mean(tprs, axis=0)
mean_tpr[-1] = 1.0
mean_auc = auc(mean_fpr, mean_tpr)
std_auc = np.std(aucs)
ax.plot(
    mean_fpr,
    mean_tpr,
    color="b",
    label=r"Mean ROC (AUC = %0.2f $\pm$ %0.2f)" % (mean_auc, std_auc),
    lw=2,
    alpha=0.8,
)

std_tpr = np.std(tprs, axis=0)
tprs_upper = np.minimum(mean_tpr + std_tpr, 1)
tprs_lower = np.maximum(mean_tpr - std_tpr, 0)
ax.fill_between(
    mean_fpr,
    tprs_lower,
    tprs_upper,
    color="grey",
    alpha=0.2,
    label=r"$\pm$ 1 std. dev.",
)

ax.set(
    xlim=[-0.05, 1.05],
    ylim=[-0.05, 1.05],
    xlabel="False Positive Rate",
    ylabel="True Positive Rate",
    title=f"Mean ROC curve with variability\n(Positive label '{target_names[1]}')",
)
ax.axis("square")
ax.legend(loc="lower right")
plt.show()


```

    /usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.
      warnings.warn(
    /usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.
      warnings.warn(
    /usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.
      warnings.warn(
    


    
![png](ML_overview_with_examples_files/ML_overview_with_examples_103_1.png)
    


# Some models for ML

## Tabular data

Simple linear regression and quadratic regression have *scalar* inputs, i.e. each example was described by a single number.

Examples can also be tabular data, where each example is described by a numeric vector. Formally, the $i$-th example is described by a vector  $(x_{i1}, \dots, x_{ik})$ of length $k$, for examples $i = 1, \dots, n$ and labels are $y_1, \dots,  y_n$  as before.

One example of data organized in a nummerical table are the [Wine quality data set](https://www.kaggle.com/datasets/yasserh/wine-quality-dataset) available n Kaggle. It has 12 explanatory variables and the label is the wine quality.

    Input variables (based on physicochemical tests):\
    1 - fixed acidity
    2 - volatile acidity
    3 - citric acid
    4 - residual sugar
    5 - chlorides
    6 - free sulfur dioxide
    7 - total sulfur dioxide
    8 - density
    9 - pH
    10 - sulphates
    11 - alcohol
    Output variable (based on sensory data):
    12 - quality (score between 0 and 10)

But tables can also have non-numerical (text) columns like (https://www.kaggle.com/datasets/zsinghrahulk/covertype-forest-cover-types)




### Decision trees

All possible examples lie on a multi-dimensional region $R$ which is the feature space. Any classifier determines a partition of $R$ into regions $R_1,\dots,R_c$, where $R_j$ is the decision region for the $j$th class.

A binary **decision tree** represents $R$ as a tree, where the *root node* represents the whole of $R$. Each node can be split into two new subnodes. The nodes that are not split are called *leaf nodes*. A class label is assigned to each *leaf node* to determine the classifier.

#### First example



The example below shows a decision tree for the `iris` data set. The root node represents the 4-dimensional space defined by the variables `sepal length`,`sepal width`, `petal length`, `petal width`. This is a 3-class problem where labels are the varieties `setosa`, `versicolor`, `virginica`.

We call **depth** of the decision tree to the maximum number of splits to define a leaf node. Note that the code establishes `max_depth=4` to prevent the tree from growing more than 4 levels. The figure indicates the number of examples (or training samples) that lie in each node of the tree.


```python
from sklearn.datasets import load_iris
from sklearn import tree
from matplotlib import pyplot as plt
iris = load_iris()

X = iris.data
y = iris.target
print(' labels: ', iris.target_names)

#build decision tree
clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=4,min_samples_leaf=4)
#max_depth represents max level allowed in each tree, min_samples_leaf minumum samples storable in leaf node

#fit the tree to iris dataset
clf.fit(X,y)

#plot decision tree
fig, ax = plt.subplots(figsize=(10, 10)) #figsize value changes the size of plot
tree.plot_tree(clf,ax=ax,feature_names=['sepal length','sepal width','petal length','petal width'])
plt.show()
```

     labels:  ['setosa' 'versicolor' 'virginica']
    


    
![png](ML_overview_with_examples_files/ML_overview_with_examples_111_1.png)
    


#### Impurity and loss

To build a tree, several questions arise:
1. Which feature should be tested at a node?
2. When should a node be declared a *leaf*?
3. If the tree becomes 'too large' how can it be made smaller and simpler (pruning)?
4. If a leaf node is impure, how should the category label be assigned?
5. How should be missing data handled?



To answer to those questions, we first need to define a measure of the quality of the model. As before, one defines a *loss* for a decision tree since the  ultimate goal is to find the model with the lowest loss.

There are two types of decision trees in ML:
1. **Classification trees**, where the labels are categorial as in the `iris`data set example. In that case, the **predicted** label is the most frequent label in the examples that lie the leaf node. For instance, if there are $[0,3,1]$ samples of varieties `[setosa, versicolor, virginica]` in a leaf node, then the label of the leaf node is `versicolor` and this is the predicted label $\hat{y}$ for all examples that lie in that region. To compute the *loss* one relies on the distribution of labels in the leaf node. For instance, the node with $[0,3,1]$ examples has estimated probabilities $\hat{p}_1=0$,  $\hat{p}_2=0.75$, $\hat{p}_3=0.25$ of been assigned to each one of the classes.
2. **Regression trees**, where the labels are continuous. In that case, the label for the node is the mean of all labels of examples that lie in that node, i.e. $\hat{y}=\bar{y}$. The *loss* for the $i$th example is then the dissimilarity between $\bar{y}$ and $y_i$. For *regression trees* the usual *loss* functions  are `mse`and `mae`.


Let's see how the *loss* of a classification tree is computed in general and of a split in particular is computed. The loss is related to the impurity of the leaf nodes of the tree. The highest is the impurity of the leaf nodes, the largest is the classification uncertainty and the loss.

For any given node of the tree, with $n_1,n_2,\dots,n_c$ examples of each class, the estimated probabilities for the $c$ different labels are:

$$\hat{p_1}=\frac{n_1}{n},\dots,\hat{p_c}=\frac{n_c}{n},$$

where $n$ is the total number of examples at the node. For classification trees, the `DecisionTreeClassifier` class in `scikit-learn` uses one of the following criteria:

1. `gini`: This is the default criterion and it measures the impurity of a set of samples as the probability of misclassifying a randomly chosen element from the set. The *loss* is given by  $G = 1 - \sum_{i=1}^n p_i^2$, where $\hat{p}_i$ is the estimated probability of belonging to the $i$th class.
2. `entropy`: This criterion measures the impurity of a set of samples as the amount of information gained about the label from observing the features that define the node. The *loss* is given by $E=-\sum_{i=1}^n \hat{p}_i \log_2 \hat{p}_i$.

Both measures range from 0 (minimum impurity, maximum certainty) to some maximum value (maximum impurity, minimum certainty). For instance, for a 2-class problem, maximum impurity is reached when $p_1=p_2=0.5$, where

$$G=1-0.5=0.5 {\rm ~and ~~} E= - 2 \times (0.5 \, \log_2 0.5)= - 2 \times (-0.5)=1.$$

When the node is split into two new children nodes, the loss function is calculated separately for each subset resulting from the split, and the *total loss* is the weighted sum of the losses of the subsets, where the weights are the fractions of samples in each subset. The split with the lowest total loss (i.e., the greatest reduction in entropy) is chosen as the best split. The expression for the loss of a split is the following, where $L$ can be either the entropy $E$ or the Gini criterion $G$.

$$
 L = \frac{n_{\rm left}}{n} \times L_{\rm left} + \frac{n_{\rm right}}{n} \times L_{\rm right} ~~~~~~~~~(1)$$

The rules above allow us to compute the loss for any tree computed from the data set. For each new split, Equation 1 allows us to update the *loss* of the whole tree.

For the two loss function above (`entropy` and `gini`), it is guaranteed that the *total loss* of the tree cannot increase for any possible split. Therefore, there is always a reduction (strict or not) in *loss*  resulting from a split which is also called *information gain*.




#### Choosing the possible splits

For continuous explanatory variables, all $n$ examples are ordered for the  $j$th feature:

$$x_{j(1)} \le x_{j(2)} \dots \le  x_{j(n)}.$$

Hence, it is not necessary to test more than $n$ splits for each feature $j$. The spliting algorithm is just something like below.

---
Initialize $L$ as an empty list

For $j=1,\dots,k$

$~~~~$ For $ i = 1, \dots n$,

$~~~~$ $~~~~$ Consider the split $x_j \le x_{j(i)}$, compute its loss decrease and append it to $L$.

The best split is the split $x_j \le x_{j(i)}$ which has the lowest value in $L$.

----

For categorical explanatory variables, when there is no order along values, in principle all $2^m$ combinations of the $m$ distinct values that the variable can take should be considered as possible splits.

#### Regularization and pruning

Decision trees are prone to *overfitting* since that if they grow enough they can approximate any decison rule with arbitrary precision. Therefore, there are different techniques to prevent decision trees of being  too large.

1. Criterion to stop growing the tree, which is equivalent to decide when a node should not be splited and should become a leaf node. There are three standard hyper-parameters:
  - Maximum depth of the tree (e.g. 4);
  - Minimum leaf size, i.e., minimum number of examples that lie in a leaf (e.g. 3);
  - Maximum number of nodes (e.g. 20).

2. Pruning. This is a regularization technique that consists on pruning the full grown tree to reduce its size. Pruning can be achieved by:
  - Adding a regularization hyper-parameter to the loss function, like $\alpha(|T|)$ where $\alpha$ is a function of the size (number of leaves) of the tree $T$. If one uses $L_\alpha=L+\alpha$ (consider that $\alpha >0$) instead of $L$ to determine the *loss*, then spliting a node might possibly cause an increase of $L_\alpha$.  If two leaves are pure and have the same label, aggregating them will lower $L_\alpha$ for $\alpha>0$. Pruning aggregates leaf nodes if that reduces $L_\alpha$.
  - Predicting a validation data set with the decision tree. Pruning consists of aggregating leaf nodes if that aggregation increases validation accuracy.

The script below illustrates how a regularization parameter like $\alpha$ can be adjusted when fitting a decision tree (suggestion: try with a different data set).


```python
from sklearn.datasets import load_iris,load_wine,load_digits,fetch_olivetti_faces
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
import matplotlib.pyplot as plt

# load the iris dataset
dataset = load_iris() #fetch_olivetti_faces() #load_digits() #load_wine() #

# split the dataset into training and validation sets
X_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.2, random_state=42)

# create a decision tree classifier
#tree = DecisionTreeClassifier()

alphas=[k/100 for k in range(10)]
trees=[]
# fit tree for each alpha
for alpha in alphas:
  # create decision tree with a cost complexity parameter alpha
  tree = DecisionTreeClassifier(random_state=42, ccp_alpha=alpha)
  # fit
  tree.fit(X_train, y_train)
  # append tree to trees
  trees.append(tree)

# compute accuracies
train_scores = [clf.score(X_train, y_train) for clf in trees]
test_scores = [clf.score(X_test, y_test) for clf in trees]

# plot accuracy vs alpha
fig, ax = plt.subplots()
ax.set_xlabel("alpha")
ax.set_ylabel("accuracy")
ax.set_title("Accuracy vs alpha for training and testing sets")
ax.plot(alphas, train_scores, marker="o", label="train", drawstyle="steps-post")
ax.plot(alphas, test_scores, marker="o", label="test", drawstyle="steps-post")
ax.legend()
plt.show()

```


    
![png](ML_overview_with_examples_files/ML_overview_with_examples_122_0.png)
    


#### Decision tree bias and variance

**Bias** measures how much the predictions of a model differ from the true values. **Variance** measures how much the predictions of a model differ from each other. One possible technique to estimate  bias and variance is **cross-validation**.

In general, cross-validation is a technique used to evaluate the performance of a machine learning model. It involves splitting the dataset into multiple subsets, training the model on some of them, and testing it on the remaining subsets. This allows us to estimate the performance of the model on new, unseen data.

The code below relies on `validation_curve`from `scikit-learn` to estimate bias and variance of a classification model. In fact, the code applies a family of models (decision trees) that depend on the hyper-parameter `max_depth`.  Note that the synthetic data set is generated by `make_classification`.



```python
from sklearn.datasets import make_classification
from sklearn.model_selection import validation_curve
from sklearn import tree
import numpy as np
import matplotlib.pyplot as plt

# generate a toy dataset
X, y = make_classification(n_samples=1000, n_features=10, random_state=42,n_classes=2)

# define the model
model = tree.DecisionTreeClassifier(criterion='entropy', min_samples_leaf=4)

# define the range of hyperparameters to test
param_range = np.arange(4, 10)

# use validation_curve to compute training and validation scores for different hyperparameters
train_scores, test_scores = validation_curve(
    model, X, y,
    param_name="max_depth", param_range=param_range,
    cv=5,
    scoring="accuracy")

# calculate the mean and standard deviation of the training and validation scores for each hyperparameter
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)

# plot the validation curves
plt.plot(param_range, train_mean, label="Training score", color="darkorange")
plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, alpha=0.2, color="darkorange")
plt.plot(param_range, test_mean, label="Cross-validation score", color="navy")
plt.fill_between(param_range, test_mean - test_std, test_mean + test_std, alpha=0.2, color="navy")
plt.legend(loc="best")
plt.xlabel("max_depth")
plt.ylabel("Accuracy")
plt.show()

# calculate bias and variance
bias = (1 - test_mean) ** 2
variance = test_std ** 2

print("Bias:", bias)
print("Variance:", variance)

```


    
![png](ML_overview_with_examples_files/ML_overview_with_examples_125_0.png)
    


    Bias: [0.012996 0.010201 0.013225 0.015129 0.014884 0.013689]
    Variance: [0.000414 0.000424 0.00035  0.000346 0.000306 0.000226]
    

In the example above, *bias* and *variance* are estimated from the validation scores. For instance, for the model with `max_depth=4`, the estimated bias is $\approx 0.013$, which corresponds to an estimated 87% accuracy, while the estimated variance is $\approx 0.0004$, which is the plotted standard deviation ($\approx 0.02$) squared.

Suggestion: try the code above using the `gini` instead of the `entropy` criterion for spliting.

### Ensemble methods

See article about [the wisdow of the crouds](https://www.npr.org/sections/13.7/2018/03/12/592868569/no-man-is-an-island-the-wisdom-of-deliberating-crowds) for an introduction to ensemble methods.

#### Random forests

Random forests (RF) are an ensemble learning method that involves:
  - (bootstraping) Creating a collection of decison trees from bootstrap samples (sampling with replacement);
  - (decorrelating) Decorrelate models by randomly selecting features
  - (aggregating) Ensembling the collection of trees by majority vote.


<img src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*qSZ93ucUL8gpZ3jf.png" width="800" >

The goal of ensemble decision trees with random forests is to reduce the variance. This is illustrated below for  regression trees. The idea for classification trees is similar but the mathematics are more complicated.

Let  $X_i$ be  the random variable  that represents the predition for the regression tree $T_i$ from the collection, with $\rho={\rm cor}[X_i,X_j]$ being the correlation between $X_i$ and $X_j$. The prediction from the ensemble is

$$ \bar{X} = \frac{1}{B} \left( X_1+\dots+X_B \right)$$

and its variance is given by

$${\rm Var}[\bar{X}]=  \rho \, \sigma^2 + \frac{1-\rho}{B} \sigma^2,$$

where ${\rm Var}[X_i]=\sigma^2$ and $B$ is the number of bootstrap samples. As long as $\rho$ does not grow with $B$, using a larger ensemble will increase $B$ and reduce ${\rm Var}[\bar{X}]$, which is the goal of ensembling classifiers.

Therefore, the idea of **bootstrap aggregation**, aka **bagging** is to create an ensemble of low correlated tree models (bootstrap) followed by aggregation in order to reduce the variance of the predictions.

Example of script that creates and fits a RF classifier for a classification problem on the `iris` data set.


```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import pandas as pd

# load the iris dataset
dataset = load_wine() #

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.2)
# Create a Random Forest classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
# Train the classifier
rf_classifier.fit(X_train, y_train)
# Make predictions on the test set
y_pred = rf_classifier.predict(X_test)
# Evaluate the accuracy of the classifier
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

```

    Accuracy: 0.9444444444444444
    

#### AdaBoost

Gradient boosting is one of the variants of ensemble methods where you create multiple **weak** models  and combine them to get better performance as a whole. The original idea behind AdaBoost is described in the paper *Schapire, R.E. The strength of weak learnability. Mach Learn 5, 197227 (1990). https://doi.org/10.1007/BF00116037*.

A weak model, also called weak inducer, is a classifier that performs just better than random. For decision trees (the most common model), the weakest model is a decision tree with depth 1 which is called a **stump**.

As discussed in [(Sagi and Rokach, 2017)](docs/Sagi_2018_Ensemble_learning_A_survey_Wire.pdf), the
main idea of AdaBoost (adaptive boosting) is to focus on instances that were previously misclassified when training a new inducer. The
level of focus given is determined by a **weight** that is assigned to each instance in the training set. In the first iteration,
the same weight is assigned to all of the instances. In each iteration, the weights of misclassified instances are increased,
while the weights of correctly classified instances are decreased. In addition, weights are also assigned to the individual
base learners based on their overall predictive performance.

**AdaBoost** is a *dependent* ML method since each tree is an improvement over previous trees in the sequence. This is the opposite of *random forests* where the tree are grown independently.

<img src="https://www.researchgate.net/profile/Zhuo-Wang-36/publication/288699540/figure/fig9/AS:668373486686246@1536364065786/Illustration-of-AdaBoost-algorithm-for-creating-a-strong-classifier-based-on-multiple_W640.jpg" width="800" >

#### Gradient Boosting

Gradient Boost is also a *dependent* method, since each weak classifier  (they are often decision trees) is an improvement of the earlier model.  Gradient boosting trees usually have from 8 to 32 terminal nodes, i.e. depth between 3 and 5.

Gradient Boosting provides a framework to build an ensemble of trees based on an arbitrary loss function and a **learning rate**. In Gradient Boosting, each new tree is computed over the **residuals** from the previous model.

<img src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*uDdVWe5-MJyy27an4Yp8GQ.png" width="800" >


For details and very nice illustrations, look at the two following posts:

1. [Regression](https://towardsdatascience.com/all-you-need-to-know-about-gradient-boosting-algorithm-part-1-regression-2520a34a502)

2. [Classification](https://towardsdatascience.com/all-you-need-to-know-about-gradient-boosting-algorithm-part-2-classification-d3ed8f56541e)

#### Variable importance

See [(Scornet, 2021)](https://arxiv.org/pdf/2001.04295.pdf) for an in-depth presentation of the topic.

Since interpretability is a concept difficult to define precisely, people eager to gain
insights about the driving forces at work behind random forests predictions often focus
on variable importance, a measure of the influence of each input variable to predict
the output. In Breimans [2001] original random forests, there exist two importance
measures:

1. **Mean Decrease Impurity** [MDI, or Gini importance, see Breiman, 2002],
which sums up the gain associated to all splits performed along a given variable; and

2. **Mean Decrease Accuracy** [MDA, or **permutation importance**, see Breiman, 2001]
which shuffles entries of a specific variable in the test data set and computes the
difference between the error on the permuted test set and the original test set.

Because
of its very definition, MDI is an importance measure that can be computed for trees
only, since it strongly relies on the tree structure, whereas MDA is an instantiation of
the permutation importance that can be used for any predictive model. Both measures
are used in practice even if they possess several major drawbacks:
  - **MDI** is known to favor variables with many categories [see, e.g., Strobl et al.,
2007, Nicodemus, 2011]. Even when variables have the same number of categories,
MDI exhibits empirical bias towards variables that possess a category having a high frequency [Nicodemus, 2011, Boulesteix et al., 2011]. MDI is also biased in presence of correlated features [Nicodemus and Malley, 2009].

  - **MDA** seems to exhibit less bias than MDI but tends to overestimate correlated features Strobl et al. [2008]. See Genuer et al. [2008] and Genuer et al. [2010] for an extensive simulation study about the influence of the number of observations, variables, and trees on MDA together with the impact of correlation on this importance measure.


The following code show how to compute MDI with different ensemble methods, and compares the estimated importances for the `iris`data set explanatory variables.


```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier

# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target
feature_names = iris.feature_names

# Create Random Forest classifier
rf_clf = RandomForestClassifier(random_state=42)
rf_clf.fit(X, y)

# Create AdaBoost classifier with decision tree base estimator
ada_clf = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=3), random_state=42)
ada_clf.fit(X, y)

# Create Gradient Boosting classifier
gb_clf = GradientBoostingClassifier(random_state=42)
gb_clf.fit(X, y)

# Extract feature importance (MDI) for each classifier
rf_importance = rf_clf.feature_importances_
ada_importance = ada_clf.feature_importances_
gb_importance = gb_clf.feature_importances_

# Set up the figure
fig, ax = plt.subplots(figsize=(8, 6))

# Plot the feature importance
x = np.arange(len(feature_names))
width = 0.2

rects1 = ax.bar(x - width, rf_importance, width, label='Random Forest')
rects2 = ax.bar(x, ada_importance, width, label='AdaBoost')
rects3 = ax.bar(x + width, gb_importance, width, label='Gradient Boosting')

# Add labels, title, and legend
ax.set_xlabel('Features')
ax.set_ylabel('Importance')
ax.set_title('Feature Importance Comparison')
ax.set_xticks(x)
ax.set_xticklabels(feature_names)
ax.legend()

# Show the plot
plt.tight_layout()
plt.show()
```


    
![png](ML_overview_with_examples_files/ML_overview_with_examples_144_0.png)
    


`scikit-learn` also provides functions to compute MDA (permutation importance). This measure of importance can be computed for any classification method but it is more computationally demanding since data have to be classified after each variable is shuffled, while MDI just uses the losses (or gains) that are computed while training the classifier.


```python
from sklearn.datasets import load_iris
from sklearn.inspection import permutation_importance
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import RandomForestClassifier
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

iris = load_iris()
target_names = iris.target_names
X, y = iris.data, iris.target
feature_names = iris.feature_names

# models
rf_clf = RandomForestClassifier(random_state=42)
rf_clf.fit(X, y)
mlp_clf = MLPClassifier(solver='sgd',hidden_layer_sizes=(10, 5, 2), max_iter=300,learning_rate_init=0.01,momentum=0.9)
mlp_clf.fit(X, y)

# Compute permutation importance
result_rf = permutation_importance(rf_clf, X, y,n_repeats=10)
result_mlp = permutation_importance(mlp_clf, X, y,n_repeats=10)

# Sort the importances in descending order
sorted_importances_idx = result_rf.importances_mean.argsort()[::-1]

# Plotting
fig, ax = plt.subplots(figsize=(8, 6))
ax.barh(np.arange(len(feature_names)), result_rf.importances_mean[sorted_importances_idx], xerr=result_rf.importances_std[sorted_importances_idx], height=0.6, color='skyblue', label='Random Forest')
ax.barh(np.arange(len(feature_names)), result_mlp.importances_mean[sorted_importances_idx], xerr=result_mlp.importances_std[sorted_importances_idx], height=0.4, color='lightgreen', label='MLP')

ax.set_yticks(np.arange(len(feature_names)))
ax.set_yticklabels(np.array(feature_names)[sorted_importances_idx])
ax.set_xlabel('Importance')
ax.set_title('Feature Importance Comparison')
ax.legend()

plt.tight_layout()
plt.show()
```

    /usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.
      warnings.warn(
    


    
![png](ML_overview_with_examples_files/ML_overview_with_examples_146_1.png)
    


### Perceptron

The *perceptron* is one of the simplest models for numerical tabular data and 0/1 classification problems and has been already discussed earlier. Since it is the basis for neural networks, we review below its formalism.

<img src="https://drive.google.com/uc?export=view&id=1HqHfJn7ejJMGeAa5fTTQzW_myubIDsDh" width="600" >


The model is

$$f_{\rm \bf w}(x_1,\dots,x_k)= \sigma (w_1 \, x_1 + \dots + w_k \, x_k)$$

where $\sigma(.)$ is the activation function, and it is defined by

$$\sigma(z) = \left\{\begin{align}
1 &, &  z \ge t \\
0 &, &  z < t \\
\end{align} \right.$$

where $t$ is some *threshold*.


Here, we consider that the input data as already been pre-processed, and all explanatory variables $(x_{i1},\dots,x_{ik})$ are numerical, while the response variable is $y_i \in \{0,1\}$ for $n$ examples $i=1,\dots,n$. In practice, pre-processing is usually necessary to create the numerical inputs of the neural network.

In matrix form, each row represents one example. For $n$ examples $i=1,\dots,n$, the following matrices represent the examples and the labels.

$
	{\rm \bf X}= \begin{bmatrix}
	x_{11} & \dots & x_{1k} \\
	x_{21} & \dots & x_{2k} \\
	\dots & \dots & \dots\\
	x_{n1} & \dots & x_{nk} \\
	\end{bmatrix}~~~~~~
$
$
	{\rm \bf y}= \begin{bmatrix}
	y_1  \\
	y_2  \\
	\dots \\
	y_n  \\
	\end{bmatrix}
$

Since each example corresponds to a row of ${\rm \bf X}$, the $i$-th example is  $(x_{i1},\dots,x_{ik})$
and has label $y_i \in \{0,1\}$.

Although in the original medel of the Perceptron, the activation function was a step functon (see above), it is currently more common to use a continuous functions for $\sigma(.)$. A typical  candidate is the *sigmoid* function

$$\sigma(z)= \frac{1}{1+e^{-z}}$$

that ranges between 0 and 1. This function is available in `pytorch` through `torch.sigmoid`. Another is the ReLu function in the next section which is still continuous but not differentiable.




```python
import sympy
sympy.plot("1/(1+exp(-z))", xlim=(-5,5));

```


    
![png](ML_overview_with_examples_files/ML_overview_with_examples_150_0.png)
    


### Feed-forward fully connected neural network with ReLu units

The next step is to add more layers to the model. Instead of having just one output from the first (input) layer, the model can have many units in the following *hidden* layers. In general, the number of model outputs (output layer) is equal to the number of labels (one is enough for the Titanic problem, since the prediction is just "survived" or not). As mentioned earlier, typically there is either a *sigmoid* unit or a *softmax* layer to complete the model, so the final outputs can be interpreted as probabilities.

This model is also known as the *Multilayer Perceptron* (MLP).

If all the intermediate layers would just multiply inputs by weights, the model could be reduced to a single matrix multiplication layer. Therefore, there must some non-linearity after each matrix multiplication. That's what is described in the following figure, where $f$ represents some non-linear *activation function* for each layer of the neural network.

<img src="https://drive.google.com/uc?export=view&id=1Ky82nfC7GUp7YBqp1Spzn9aHDrTzbiCH" width="600" >

The layer is called *fully connected* when each matrix multiplication, which returns the dot product $x_1 \, w_1 + \dots + x_n \, w_n$ envolves all neurons from the previous layer.

Typically, all activation functions ($f$ in the figure above) for the *hidden layers* are called rectified linear units (*ReLu*) and they represent the following continuous function, which is the identity function for positive arguments and *zero* for negative arguments.

${\rm ReLu}(z) = \left\{\begin{align}
z &, &  z \ge 0 \\
0 &, &  z < 0 \\
\end{align} \right.$

If there is only one output, the activation layer for the *output layer* is typically the *sigmoid* function. If there is more than one output (as in the figure above), the typical choice of activation function for the output layer is the *softmax* function.

For first *neuron* in the first hidden unit, the calculation goes exactely as  we discussed for the perceptron model, where the inputs are multiplied by the  weights $w_1^{(1)},\dots, w_4^{(1)}$ to return

$$w_1^{(1)} \, x_1 + w_2^{(1)} \, x_2 + w_3^{(1)} \, x_3 + w_4^{(1)} \, x_4 .$$

The same product is computed for the second neuron of the first hidden layer, but for a *different set of weights* $w_1^{(2)},\dots, w_4^{(2)}$, and so on. Hence, in total there are for the example in the figure above, 12 multiplicative weights (4 input variables $\times$ 3 neuros in the hidden layer). The three multiplications (for the three neurons in the hidden layer) can all be done with a single matrix multiplication:

If the weights and input values are  $~~~~~
	{\rm W}= \begin{bmatrix}
	w_{1}^{(1)} & w_{2}^{(1)} & w_{3}^{(1)} & w_{4}^{(1)} \\
	w_{1}^{(2)} & w_{2}^{(2)} & w_{3}^{(2)} & w_{4}^{(2)} \\
	w_{1}^{(3)} & w_{2}^{(3)} & w_{3}^{(3)} & w_{4}^{(3)} \\
	\end{bmatrix}~~~{\rm and}~~~~
$
$
	{\rm x}= \begin{bmatrix}
	x_1  \\
	x_2 \\
	x_3  \\
	x_4  \\
	\end{bmatrix}~~~
$

then, the hidden layer three outputs (before applying the activation function) are just the rows of the product ${\rm W} \, {\rm x}$:

$$ {\rm  W} \, {\rm x}= \begin{bmatrix}
	w_1^{(1)} \, x_1 + w_2^{(1)} \, x_2 + w_3^{(1)} \, x_3 + w_4^{(1)} \, x_4  \\
	w_1^{(2)} \, x_1 + w_2^{(2)} \, x_2 + w_3^{(2)} \, x_3 + w_4^{(2)} \, x_4  \\
	w_1^{(3)} \, x_1 + w_2^{(3)} \, x_2 + w_3^{(3)} \, x_3 + w_4^{(3)} \, x_4  \\
	\end{bmatrix}
.$$

This is very convenient since matrix multiplication can be computed quickly.

Note that it is usual to include also an *additive weight* for each neuron (this is called the *bias*). Without lost of generality, we can think that $x_1$ is an artificial input which value is always 1, and therefore $w_1^{(j)} \times x_1=w_1^{(j)}$ is the additive weight. In alternative, we can add a weight $w_0^{(j)}$ to each neuron, so the neuron output (before applying the activation function) is

$$w_0^{(1)} + w_1^{(1)} \, x_1 + w_2^{(1)} \, x_2 + w_3^{(1)} \, x_3 + w_4^{(1)} \, x_4 $$

in the above example.

Putting everything together, the three outputs of the first hidden layer are:

$$
	 {\rm ReLu} \left(w_0^{(1)} + w_1^{(1)} \, x_1 + w_2^{(1)} \, x_2 + w_3^{(1)} \, x_3 + w_4^{(1)} \, x_4 \right)  \\
   {\rm ReLu} \left(w_0^{(2)} + w_1^{(2)} \, x_1 + w_2^{(2)} \, x_2 + w_3^{(2)} \, x_3 + w_4^{(2)} \, x_4 \right)  \\
 {\rm ReLu} \left(w_0^{(3)} + w_1^{(3)} \, x_1 + w_2^{(3)} \, x_2 + w_3^{(3)} \, x_3 + w_4^{(3)} \, x_4 \right)  \\
$$

Then, calculations proceed to the following layer, and so on, until they reach  the output layer. This network is called *feed-forward* because computations are done sequentially layer by layer.



### Techniques to improve deep learning

**Regularization** is a technique used in machine learning to prevent overfitting by adding a penalty term to the loss function. This encourages the model to learn a simpler representation of the data and reduces its capacity to memorize the training data.

**Self-regularized activation functions** can help improve the generalization performance of a neural network by introducing an implicit form of regularization. This can be achieved through various mechanisms, such as controlling the distribution of the activations or the gradients. For example, the *Mish activation function* has been shown to have a self-regularizing effect due to its non-monotonic and smooth nature, which can help prevent the vanishing gradient problem and improve the training dynamics of deep neural networks.

The *Mish activation function* (https://arxiv.org/abs/1908.08681) is an alternative to *ReLu*. It is a smooth, continuous, self regularized, non-monotonic activation function mathematically defined as

$$f(x)= x \, {\rm tanh} (\ln (1+e^x)).$$

**Dropout** is a regularization technique used in deep learning to prevent overfitting. It works by randomly dropping out or deactivating some of the neurons in a neural network during training. This means that during each forward pass, some of the neurons are temporarily removed from the network, along with all their incoming and outgoing connections.

The idea behind dropout is to introduce randomness and prevent the model from relying too heavily on any single neuron or feature. By randomly dropping out neurons during training, the model is forced to learn a more robust representation of the data that is less sensitive to small changes in the input.

Dropout is typically applied to the hidden layers of a neural network and can be controlled by a hyperparameter called the dropout rate, which specifies the probability that any given neuron will be dropped out during training. A common value for the dropout rate is 0.5, meaning that on average, half of the neurons in a given layer will be dropped out during each forward pass.

During testing or inference, dropout is not applied and all neurons are active. However, to account for the fact that only a fraction of the neurons were active during training, the outputs of the neurons are typically scaled down by the dropout rate.

**Momentum**  is a technique used in deep learning to accelerate the training of neural networks. It is an optimization algorithm that helps the model converge faster by adding a fraction of the previous weight update to the current weight update.

In gradient descent, the weights of a neural network are updated by taking a step in the direction of the negative gradient of the loss function with respect to the weights. This can sometimes result in slow convergence or getting stuck in local minima. Momentum addresses these issues by introducing a momentum term that takes into account the previous weight updates.

The idea behind momentum is to add a fraction of the previous weight update to the current weight update, effectively smoothing out the updates and helping the model converge faster. This can be controlled by a hyperparameter called the momentum coefficient, which specifies how much of the previous weight update should be added to the current weight update. A common value for the momentum coefficient is 0.9.

Momentum can be used with various optimization algorithms, such as stochastic gradient descent (SGD) or Adam, to improve their convergence properties.

**Adam** (short for Adaptive Moment Estimation) is an optimization algorithm commonly used in deep learning to train neural networks. It is an extension of stochastic gradient descent (SGD) that incorporates ideas from other optimization algorithms such as AdaGrad and RMSProp.

Adam works by maintaining an estimate of the first and second moments of the gradients (i.e., the mean and uncentered variance) and using these estimates to adaptively adjust the learning rate for each weight in the network. This allows the algorithm to converge faster and achieve better performance than traditional SGD. For details, look at the [pseudo-code for Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html).

One of the key advantages of Adam is that it requires little tuning of its hyperparameters. The algorithm has three main hyperparameters: the learning rate, the first moment decay rate (beta1), and the second moment decay rate (beta2). The default values for these hyperparameters (0.001, 0.9, and 0.999, respectively) usually work well in practice. Adam has been shown to work well on a wide range of deep learning problems and is often used as the default optimizer in many deep learning frameworks.


**Batch normalization**  tries to maintain a good distribution of activations throughout training. The paper ["Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"](https://arxiv.org/abs/1502.03167) addresses the realization that training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change, which slows down training.

That problem, known as *internal covariate shift* can be addressed by normalizing layer inputs. Details of implementation can be found at [PyTorch documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.batch_norm.html).

The follownig code illustrates, for the CIFAR-10 data set (32$\times$32 color images) the use of `BatchNorm` in PyTorch to create a NN model.

```
nn.Sequential(
      nn.Flatten(),
      nn.Linear(32 * 32 * 3, 64),
      nn.BatchNorm1d(64),
      nn.ReLU(),
      nn.Linear(64, 32),
      nn.BatchNorm1d(32),
      nn.ReLU(),
      nn.Linear(32, 10)
    )
```




**TensorBoard** is a tool for providing the measurements and visualizations needed during the machine learning workflow. It enables tracking experiment metrics like loss and accuracy, visualizing the model graph, projecting embeddings to a lower dimensional space, and much more. See this  [Colab notebook that uses tensorboard with Keras](https://colab.research.google.com/github/tensorflow/tensorboard/blob/master/docs/tensorboard_in_notebooks.ipynb)

### Example of NN classifier implemented with PyTorch

`PyTorch` provides the designed modules and classes `torch.nn` , `torch.optim` , `Dataset` , and `DataLoader` to help you create and train neural networks. On their webpage, you can find a broad range of tutorials [https://pytorch.org/tutorials/](https://pytorch.org/tutorials/) and, in particular, on [`torch.nn`](https://pytorch.org/tutorials/beginner/nn_tutorial.html).

The following script creates a $n$-layer neural network with `PyTorch` and applies it to some available dataset (`iris` or the 8$\times$8 MNIST digit dataset). The script contains many parameters that have been discussed above, namely:
- architecture of the neural network;
- batch size;
- number of epochs
- learning rate;
- regularization parameter;
- momentum;
- dropout proportion.

The script illustrates how to create a dataloader in `PyTorch` which makes it easy to loop through mini-batches while training the model. It also plots train and test loss along epochs. This is useful to choose the best number of epochs to train the model and avoid overfitting.





```python
#@title Script that implements a neural network with PyTorch (over the iris or mnist datasets)
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.datasets import load_iris, load_digits
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import random
import numpy as np

CREATE_CLASS=True # Create class from scratch; otherwise use nn.Sequential to create the class
SGD=False # SGD or Adam
IRIS=False # iris or mnist
SHOW=False # returns picture of digit for mnist

# Load Iris dataset
if IRIS:
    examples = load_iris()
else:
    examples = load_digits() # https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html; 10 digits;  1797 examples
    if SHOW:
        idx=random.randint(0,len(examples.target))
        print(examples.data[idx])
        print(examples.data[idx].reshape(8,8))
        print(examples.target[idx])
        plt.matshow(examples.data[idx].reshape(8,8), cmap=plt.cm.gray_r)
        plt.show()

X = examples.data
y = examples.target

# Splitting data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Convert numpy arrays to PyTorch tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.long)
y_test_tensor = torch.tensor(y_test, dtype=torch.long)

# Instantiate the model
input_size = X_train_tensor.shape[1]
hidden_size = 8
output_size = len(examples.target_names)
batch_size=400
num_epochs = 200
# Optimizer specific options
learning_rate=0.1
regularization_param=0.001
momentum_param=0.9
# Dropout: if p>0
dropout_p=0.25 # During training, randomly zeroes some of the elements of the input tensor with probability p.

# Create dataloader which makes it easier to use mini batches
train_dl=DataLoader(TensorDataset(X_train_tensor,y_train_tensor), batch_size, shuffle=True)

########################################################### NN model
if CREATE_CLASS:
    # Create model, first defining the class with a forward method
    class ThreeLayerNet(nn.Module):
        def __init__(self, input_size, hidden_size, output_size):
            super(ThreeLayerNet, self).__init__()
            self.fc1 = nn.Linear(input_size, hidden_size)
            self.fc2 = nn.Linear(hidden_size, hidden_size)
            self.fc3 = nn.Linear(hidden_size, output_size)
            self.dropout = nn.Dropout(p=dropout_p)  # Dropout layer with dropout probability
        def forward(self, x):
            x = torch.relu(self.fc1(x))
            x = self.dropout(x)  # Apply dropout after first hidden layer
            x = torch.relu(self.fc2(x))
            x = self.dropout(x)  # Apply dropout after second hidden layer
            x = self.fc3(x)
            return x
    model = ThreeLayerNet(input_size, hidden_size, output_size)
else:
    # Or, in alternative, use nn.Sequential
    model=nn.Sequential(
        nn.Linear(input_size, hidden_size),
        nn.ReLU(),
        nn.Dropout(p=dropout_p),
        nn.Linear(hidden_size, hidden_size),
        nn.ReLU(),
        nn.Dropout(p=dropout_p),
        nn.Linear(hidden_size, output_size)
    )
####################################################################################################
# Define loss function and optimizer
# Either torch.nn.NLLLoss or torch.nn.CrossEntropyLoss can be used: CrossEntropyLoss (https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) implements softmax internally
criterion = nn.CrossEntropyLoss() #
# Optimizer: optimizer object that will hold the current state and will update the parameters based on the computed gradients
# for param in model.parameters(): print(param.data)
if SGD:
    optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=regularization_param, momentum=momentum_param)
else:
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=regularization_param)

# Lists to store train and test losses
train_losses = []
test_losses = []

# Training the model
for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    for x_batch, y_batch in train_dl:
        # Forward pass
        pred = model(x_batch) # Returns tensor: nrows=tensor batch_size; ncols=number of classes
        loss = criterion(pred, y_batch)

        # Backward pass and optimization
        optimizer.zero_grad() # Resets the gradients of all optimized tensors
        loss.backward() # Computes gradient
        optimizer.step() # Performs a single optimization step (parameter update).

        train_loss += loss.item() # .item() extracts the scalar value of the loss tensor

    train_loss /= len(train_dl)
    train_losses.append(train_loss)

    # Test the model
    # We also put the model in evaluation mode, so that specific layers
    # such as dropout or batch normalization layers behave correctly.
    model.eval()
    with torch.no_grad():
        outputs = model(X_test_tensor)
        test_loss = criterion(outputs, y_test_tensor)
        test_losses.append(test_loss.item())

    if (epoch+1) % 100 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')

# Plotting train and test losses
plt.plot(range(num_epochs), train_losses, label='Train Loss')
plt.plot(range(num_epochs), test_losses, label='Test Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Train and Test Losses')
plt.legend()
plt.show()

# Testing the model
with torch.no_grad():
    outputs = model(X_test_tensor)
    _, predicted = torch.max(outputs, 1) # Returns a namedtuple (values, indices) where values is the maximum value of each row of the input tensor in the given dimension dim. And indices is the index location of each maximum value found (argmax).

actual=y_test_tensor.numpy()
pred=predicted.numpy()
accuracy = accuracy_score(actual, pred)
print(f'Accuracy on test set: {accuracy:.4f}')
cm=confusion_matrix(actual, pred)
labels = np.unique(actual)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
disp.plot()
plt.show()

```

    Epoch [100/200], Train Loss: 0.8591, Test Loss: 0.3878
    Epoch [200/200], Train Loss: 0.8437, Test Loss: 0.4352
    


    
![png](ML_overview_with_examples_files/ML_overview_with_examples_159_1.png)
    


    Accuracy on test set: 0.8833
    


    
![png](ML_overview_with_examples_files/ML_overview_with_examples_159_3.png)
    



```python
#@title Improved (more modular) script that implements a neural network with PyTorch over the mnist 8 by 8 practice data set
# code adapted from https://github.com/rasbt/machine-learning-book/blob/main/ch14/ch14_part1.py

'''
This code does the following:
    Splits the dataset into training and testing sets.
    Standardizes the features using StandardScaler.
    Reshapes dataset to fit the model
    Instantiates the model (NN)
    Defines the loss function (Cross Entropy Loss) and optimizer (Adam).
    Trains the model for num_epochs epochs.
    Tests the trained model on the test set and evaluates the accuracy.
'''

import torch
import torch.nn as nn
import torch.optim as optim
from torchsummary import summary
from torch.utils.data import DataLoader, TensorDataset
from sklearn.datasets import load_digits # https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import random
import numpy as np

########################################################################### my functions

def train(model, optimizer, loss_fn, num_epochs, train_dl, valid_dl):
    '''
    Main function to train and test the model
    '''
    # lists to strore losses and accuracies
    loss_hist_train = [0] * num_epochs
    accuracy_hist_train = [0] * num_epochs
    loss_hist_valid = [0] * num_epochs
    accuracy_hist_valid = [0] * num_epochs
    # main loop through epochs
    for epoch in range(num_epochs):
        # training mode
        model.train()
        for x_batch, y_batch in train_dl:
            # core of the learning process: predict and fit
            pred = model(x_batch)
            loss = loss_fn(pred, y_batch)
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            # compute train loss and accuracy
            loss_hist_train[epoch] += loss.item()*y_batch.size(0)
            is_correct = (torch.argmax(pred, dim=1) == y_batch).float()
            accuracy_hist_train[epoch] += is_correct.sum()
        # compute average loss per epoch
        loss_hist_train[epoch] /= len(train_dl.dataset)
        accuracy_hist_train[epoch] /= len(train_dl.dataset)
        # we also put the model in evaluation mode, so that specific layers such as dropout or batch normalization layers behave correctly.
        model.eval()
        with torch.no_grad():
            for x_batch, y_batch in valid_dl:
                # predict
                pred = model(x_batch)
                loss = loss_fn(pred, y_batch)
                loss_hist_valid[epoch] += loss.item()*y_batch.size(0)
                is_correct = (torch.argmax(pred, dim=1) == y_batch).float()
                accuracy_hist_valid[epoch] += is_correct.sum()
                if epoch==0:
                    preds,actuals=torch.argmax(pred, dim=1),y_batch
                else:
                    preds=torch.cat((preds,torch.argmax(pred, dim=1)),dim=0)
                    actuals=torch.cat((actuals,y_batch),dim=0)
        # compute average loss per epoch
        loss_hist_valid[epoch] /= len(valid_dl.dataset)
        accuracy_hist_valid[epoch] /= len(valid_dl.dataset)
        # print accuracy
        if (epoch+1) % 100==0:
            print(f'Epoch {epoch+1} accuracy: {accuracy_hist_train[epoch]:.4f} val_accuracy: {accuracy_hist_valid[epoch]:.4f}')
    return loss_hist_train, loss_hist_valid, accuracy_hist_train, accuracy_hist_valid, preds,actuals


def plot_accuracy_from_predictions(hist):
    ''' Creates and prints confusion matrix from a model and a set of examples
    Inputs
    ------
    hist: tuple
        where hist[4] is the list of predicted values for test and hist[5] are the actual labels
    '''
    pred=hist[4].numpy()
    actual=hist[5].numpy()
    labels = np.unique(actual)
    disp = ConfusionMatrixDisplay.from_predictions(actual,pred,labels=labels)
    # print global accuracy
    accuracy=np.sum(np.diagonal(disp.confusion_matrix))/np.sum(disp.confusion_matrix)
    print(f'Accuracy on test set: {accuracy:.4f}')
    plt.show()

def plot_losses(hist):
    ''' plots train and test loss
    Input
    ------
    history, the output of function train()
    '''
    x_arr = np.arange(len(hist[0])) + 1
    fig = plt.figure(figsize=(12, 4))
    ax = fig.add_subplot(1, 2, 1)
    ax.plot(x_arr, hist[0], '-o', label='Train loss')
    ax.plot(x_arr, hist[1], '--<', label='Test loss')
    ax.set_xlabel('Epoch', size=15)
    ax.set_ylabel('Loss', size=15)
    ax.legend(fontsize=15)
    ax = fig.add_subplot(1, 2, 2)
    ax.plot(x_arr, hist[2], '-o', label='Train acc.')
    ax.plot(x_arr, hist[3], '--<', label='Test acc.')
    ax.legend(fontsize=15)
    ax.set_xlabel('Epoch', size=15)
    ax.set_ylabel('Accuracy', size=15)
    plt.show()

################################################################################ Data and parameters

SHOW=False # returns picture of a randomly chosen digit

examples = load_digits() # https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html; 10 digits;  1797 examples
if SHOW:
    idx=random.randint(0,len(examples.target))
    print(examples.data[idx])
    print(examples.data[idx].reshape(8,8))
    print(examples.target[idx])
    plt.matshow(examples.data[idx].reshape(8,8), cmap=plt.cm.gray_r)
    plt.show()

X = examples.data # np.ndarray (1797, 64)
y = examples.target # (1797,)

# parameter constants
test_size=0.2
hidden_size = 8
batch_size= 256
num_epochs = 50
# Optimizer specific options
learning_rate=0.1
regularization_param=0.001
# Dropout: if p>0
dropout_p=0.1 # During training, randomly zeroes some of the elements of the input tensor with probability p.

########################################################################### train and test, pre-processing
# Splitting data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)

# Standardize features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Convert numpy arrays to PyTorch tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.long)
y_test_tensor = torch.tensor(y_test, dtype=torch.long)
print('Number of examples in training set:',X_train_tensor.shape)
print('Number of examples in test set:', X_test_tensor.shape)

# Instantiate the model
input_size = X_train_tensor.shape[1]
output_size = len(examples.target_names)

# Create dataloader with batch_size
train_dl=DataLoader(TensorDataset(X_train_tensor,y_train_tensor), batch_size, shuffle=True)
test_dl=DataLoader(TensorDataset(X_test_tensor,y_test_tensor), batch_size, shuffle=False)

###################################################################################### NN model
model=nn.Sequential(
    nn.Linear(input_size, hidden_size),
    nn.BatchNorm1d(hidden_size),
    nn.ReLU(),
    nn.Dropout(p=dropout_p),
    nn.Linear(hidden_size, hidden_size),
    nn.BatchNorm1d(hidden_size),
    nn.ReLU(),
    nn.Dropout(p=dropout_p),
    nn.Linear(hidden_size, output_size)
)

# Define loss function and optimizer
# Either torch.nn.NLLLoss or torch.nn.CrossEntropyLoss can be used: CrossEntropyLoss (https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) implements softmax internally
loss_fn = nn.CrossEntropyLoss()

# Optimizer: optimizer object that will hold the current state and will update the parameters based on the computed gradients
# for param in model.parameters(): print(param.data)
optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=regularization_param)

# Train the model and predict on test samples to estimate accuracy
# history stores losses, accuracy, actual labels and predictions
history = train(model, optimizer, loss_fn, num_epochs, train_dl, test_dl)

# plot losses along epochs
plot_losses(history)
# plot confusion matrix
plot_accuracy_from_predictions(history)
#plot_accuracy(hist)
```

    Number of examples in training set: torch.Size([1437, 64])
    Number of examples in test set: torch.Size([360, 64])
    


    
![png](ML_overview_with_examples_files/ML_overview_with_examples_160_1.png)
    


    Accuracy on test set: 0.9125
    


    
![png](ML_overview_with_examples_files/ML_overview_with_examples_160_3.png)
    


## Image classification

Image classification applies a class label to an entire image. For example, a simple image classification model might be trained to categorize photographs of crops into their category as in https://www.kaggle.com/datasets/mdwaquarazam/agricultural-crops-image-classification.


Convolutional neural networks (CNN) are the main machine learning models for image classification. They use image filters (aka covolutions) similarly to conventional image processing techniques to extract features from the images. However, CNN  are trained to determine their own image filters from examples. This is unlike the conventional approach where filters have to be carefully coded and are domain specific.



#### Convolutions and kernels

Note: For a detailed overview of convolutional neural networks see the notebook that includes `pytorch`code for computing convolutions:  https://github.com/fastai/fastbook/blob/master/13_convolutions.ipynb. Some concepts and examples from that notebook are included in the text below. For a friendly introduction to convolutions with nice animations, consider watching  the video [https://www.3blue1brown.com/lessons/convolutions](https://www.3blue1brown.com/lessons/convolutions).

A convolution applies a kernel across an image. A kernel is a little matrix, such as the 33 matrix below. The 77 grid to the left is the image we're going to apply the kernel to. The convolution operation multiplies each element of the kernel by each element of a 33 block of the image. The results of these multiplications are then added together. The diagram  shows an example of applying a kernel to a single location in the image, the 33 block around cell 18.

<img src="https://github.com/fastai/fastbook/blob/master/images/chapter9_conv_basic.png?raw=1" id="basic_conv" caption="Applying a kernel to one location" alt="Applying a kernel to one location" width="600">

In the paper ["A Guide to Convolution Arithmetic for Deep Learning"](https://arxiv.org/abs/1603.07285) there are many great diagrams showing how image kernels can be applied. Here's an example from the paper showing (at the bottom) a light blue 44 image, with a dark blue 33 kernel being applied, creating a 22 green output activation map at the top.

<img alt="Result of applying a 33 kernel to a 44 image" width="782" caption="Result of applying a 33 kernel to a 44 image (courtesy of Vincent Dumoulin and Francesco Visin)" id="three_ex_four_conv" src="https://github.com/fastai/fastbook/blob/master/images/att_00028.png?raw=1">

What is the shape of the result? If the original image has a height of `h` and a width of `w`, how many 33 windows can we find? As you can see from the example, there are `h-2` by `w-2` windows, so the image we get has a result as a height of `h-2` and a width of `w-2`.

#### Padding, pooling, stride and activation map

**Feature map** (aka **activation map**) is the output of the application of the filter to some input image (which can be either the raw input image or some feature map) as illustrated in the figures above. The name *feature map* stresses the fact that convolution creates (or extracts) new features from some image. The name *activation map* (not to be confounded with the *activation function* of NNs) stresses that the high values in the output correspond to  parts of the input image that are activated by the convolution operation.

**Padding** consists in creating new cells on the margins of the input, with a given value (in general 0). With appropriate padding, we can ensure that the output **activation map** is the same size as the original image, which can make things a lot simpler when we construct our architectures. The figure below shows how adding padding allows us to apply the kernels in the image corners.

<img src="https://github.com/fastai/fastbook/blob/master/images/chapter9_padconv.svg?raw=1" id="pad_conv" caption="A convolution with padding" alt="A convolution with padding" width="600">

With a 55 input, 44 kernel, and 2 pixels of padding, we end up with a 66 activation map:

<img alt="A 44 kernel with 55 input and 2 pixels of padding" width="783" caption="A 44 kernel with 55 input and 2 pixels of padding (courtesy of Vincent Dumoulin and Francesco Visin)" id="four_by_five_conv" src="https://github.com/fastai/fastbook/blob/master/images/att_00029.png?raw=1">

If we apply to some image a kernel of size `ks` by `ks` (with `ks` an odd number), the necessary padding on each side to keep the same shape is `ks//2`. An even number for `ks` would require a different amount of padding on the top/bottom and left/right, but in practice we almost never use an even filter size.

**Stride**. So far, when we have applied the kernel to the grid, we have moved it one pixel over at a time. But we can jump further; for instance, we could move over two pixels after each kernel application, as in the figure below. This is known as a *stride-2* convolution and will reduce the size in the example from 5$\times$5 to 3$\times$3.

In short, **stride-2** convolutions are useful for decreasing the size of our outputs, and **stride-1** convolutions (with padding) are useful for adding layers while preserving the image size.

<img alt="A 33 kernel with 55 input, stride-2 convolution, and 1 pixel of padding" width="774" caption="A 33 kernel with 55 input, stride-2 convolution, and 1 pixel of padding (courtesy of Vincent Dumoulin and Francesco Visin)" id="three_by_five_conv" src="https://github.com/fastai/fastbook/blob/master/images/att_00030.png?raw=1">

In general, if the input image has size `h` $\times$ `w`, using a padding of 1 and a stride of 2 (as in the example above) will output an image of size `(h+1)//2` $\times$  `(w+1)//2`. The general formula for each dimension is `(n + 2*pad - ks)//stride + 1`, where `pad` is the padding, `ks`, the size of our kernel, and `stride` is the stride.

**Pooling** is a type of convolution with a fixed operation (not trainable) as illustrated in the example below. This can be used to reduce the size of a layer. However, pooling can be replaced by convolution with stride larger than 1 (see paper "Striving for Simplicity: The All Convolutional Net" at https://arxiv.org/abs/1412.6806).

<img src="https://epynn.net/_images/pool-01.svg"  width="600">


Next, we apply the operations described above to create a convolutional neural network. To that end, we adapt the script that implemented a neural network for classifying the examples in the 8$\times$8 MNIST data set.

We re-use all the auxiliary functions `train` (main function for training the neural network), `plot_accuracy_from_predictions` and `plot_losses`from the previous script. There are two main changes:
1. Additional pre-processing is need to reshape the examples to NCHW (batch size, channels, height and width);  
2. The model is expanded with *2D-convolutional* and *maxpool* layers.

You need to run the previous NN script first to define the auxiliary functions that this script uses.


```python
#@title Script that implements a convolutional neural network with PyTorch over the mnist 8 by 8 practice data set
# code adapted from https://github.com/rasbt/machine-learning-book/blob/main/ch14/ch14_part1.py

'''
This code does the following:
    Splits the dataset into training and testing sets.
    Standardizes the features using StandardScaler.
    Reshapes dataset to fit the model
    Instantiates the model (CNN)
    Defines the loss function (Cross Entropy Loss) and optimizer (Adam).
    Trains the model for num_epochs epochs.
    Tests the trained model on the test set and evaluates the accuracy.
'''

import torch
import torch.nn as nn
import torch.optim as optim
from torchsummary import summary
from torch.utils.data import DataLoader, TensorDataset
from sklearn.datasets import  load_digits
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import random
import numpy as np

################################################################################ Data and parameters
SHOW=False # plot some digit for mnist 8*8

examples = load_digits() # https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html; 10 digits;  1797 examples
if SHOW:
    idx=random.randint(0,len(examples.target))
    print(examples.data[idx])
    print(examples.data[idx].reshape(8,8))
    print(examples.target[idx])
    plt.matshow(examples.data[idx].reshape(8,8), cmap=plt.cm.gray_r)
    plt.show()

X = examples.data # np.ndarray (1797, 64)
y = examples.target # (1797,)

# parameter constants
test_size=0.2
hidden_size = 8
batch_size= 256
num_epochs = 50
# Optimizer specific options
learning_rate=0.1
regularization_param=0.001
# Dropout: if p>0
dropout_p=0.1 # During training, randomly zeroes some of the elements of the input tensor with probability p.

########################################################################### train and test, pre-processing
# Splitting data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)

# Standardize features
scaler = StandardScaler()
print(X_train.shape)
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# mnist data set has examples with 64 attributes
# We need to reshape that information into NCHW (batch size, channels, height, width)
def reshape_mnist(X,W,H):
    X=X.reshape((X.shape[0],W,H))
    return np.expand_dims(X,1) # one channel

# Convert numpy arrays to PyTorch tensors of the right shape (labels do not need to be reshaped)
X_train_tensor = torch.tensor(reshape_mnist(X_train,8,8), dtype=torch.float32)
X_test_tensor = torch.tensor(reshape_mnist(X_test,8,8), dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.long)
y_test_tensor = torch.tensor(y_test, dtype=torch.long)
print('Number of examples in training set:',X_train_tensor.shape)
print('Number of examples in test set:', X_test_tensor.shape)

# Instantiate the model
input_size = X_train_tensor.shape[1]
output_size = len(examples.target_names)

# Create dataloader and determine batch size (note: batchsize is the first parameter in NCHW)
train_dl=DataLoader(TensorDataset(X_train_tensor,y_train_tensor), batch_size, shuffle=True)
test_dl=DataLoader(TensorDataset(X_test_tensor,y_test_tensor), batch_size, shuffle=True)

if SHOW:
    class_names = [str(i) for i in range(10)]
    # Plot the images
    plt.figure(figsize=(10, 5))
    image_count = 0
    for images, labels in train_dl:
        for i in range(len(images)):
            plt.subplot(4, 5, image_count + 1)
            plt.imshow(np.transpose(images[i], (1, 2, 0)), cmap="gray")
            plt.title(class_names[labels[i]])
            plt.axis('off')
            image_count += 1
            if image_count >= 20:
                break
        if image_count >= 20:
            break
    plt.show()

###################################################################################### CNN  model
model=nn.Sequential(
    nn.Conv2d(in_channels=1,out_channels=8,kernel_size=3,padding=1),
    nn.ReLU(),
    nn.MaxPool2d(kernel_size=2),
    nn.Flatten(),
    nn.Linear(8*4*4, hidden_size),
    nn.BatchNorm1d(hidden_size),
    nn.ReLU(),
    nn.Linear(hidden_size, hidden_size),
    nn.BatchNorm1d(hidden_size),
    nn.ReLU(),
    nn.Dropout(p=dropout_p),
    nn.Linear(hidden_size, output_size)
)

'''
Compare with NN from previous script:
model=nn.Sequential(
    nn.Linear(input_size, hidden_size),
    nn.BatchNorm1d(hidden_size),
    nn.ReLU(),
    nn.Dropout(p=dropout_p),
    nn.Linear(hidden_size, hidden_size),
    nn.BatchNorm1d(hidden_size),
    nn.ReLU(),
    nn.Dropout(p=dropout_p),
    nn.Linear(hidden_size, output_size)
)
'''
# model description
summary(model,(1,8,8)) # C, H, W

# Define loss function and optimizer
# Either torch.nn.NLLLoss or torch.nn.CrossEntropyLoss can be used: CrossEntropyLoss (https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) implements softmax internally
loss_fn = nn.CrossEntropyLoss()

# Optimizer: optimizer object that will hold the current state and will update the parameters based on the computed gradients
# for param in model.parameters(): print(param.data)
optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=regularization_param)

# Train the model and predict on test samples to estimate accuracy
# history stores losses, accuracy, actual labels and predictions
history = train(model, optimizer, loss_fn, num_epochs, train_dl, test_dl)

# plot losses along epochs
plot_losses(history)
# plot confusion matrix
plot_accuracy_from_predictions(history)
#plot_accuracy(hist)



```

    (1437, 64)
    Number of examples in training set: torch.Size([1437, 1, 8, 8])
    Number of examples in test set: torch.Size([360, 1, 8, 8])
    ----------------------------------------------------------------
            Layer (type)               Output Shape         Param #
    ================================================================
                Conv2d-1              [-1, 8, 8, 8]              80
                  ReLU-2              [-1, 8, 8, 8]               0
             MaxPool2d-3              [-1, 8, 4, 4]               0
               Flatten-4                  [-1, 128]               0
                Linear-5                    [-1, 8]           1,032
           BatchNorm1d-6                    [-1, 8]              16
                  ReLU-7                    [-1, 8]               0
                Linear-8                    [-1, 8]              72
           BatchNorm1d-9                    [-1, 8]              16
                 ReLU-10                    [-1, 8]               0
              Dropout-11                    [-1, 8]               0
               Linear-12                   [-1, 10]              90
    ================================================================
    Total params: 1,306
    Trainable params: 1,306
    Non-trainable params: 0
    ----------------------------------------------------------------
    Input size (MB): 0.00
    Forward/backward pass size (MB): 0.01
    Params size (MB): 0.00
    Estimated Total Size (MB): 0.02
    ----------------------------------------------------------------
    


    
![png](ML_overview_with_examples_files/ML_overview_with_examples_179_1.png)
    


    Accuracy on test set: 0.9217
    


    
![png](ML_overview_with_examples_files/ML_overview_with_examples_179_3.png)
    


### CNN model parameters

The number of parameters depend on the kernel size, the number of input channels and the number of output features.

In the previous example, the architecture returned by `summary` is:

|        Layer (type)       |        Output Shape     |    Param #|
| --- | ---: | ---: |
|            Conv2d-1       |       [-1, 8, 8, 8]     |         80|
|              ReLU-2       |       [-1, 8, 8, 8]     |          0|
|         MaxPool2d-3       |       [-1, 8, 4, 4]     |          0|
|           Flatten-4       |           [-1, 128]     |          0|
|            Linear-5       |             [-1, 8]     |     1,032|
| BatchNorm1d-6 |  [-1, 8]        |      16 |
|              ReLU-7       |             [-1, 8]     |         0|
|            Linear-8       |             [-1, 8]     |         72|
| BatchNorm1d-9 |  [-1, 8]        |      16 |
|              ReLU-10       |             [-1, 8]     |          0|
|           Dropout-11       |             [-1, 8]     |          0|
|           Linear-12       |            [-1, 10]     |         90|

In the summary above, the output shape format is NCHW and therefore `-1` refers to the batch size which can be replaced by some arbitrary value. The input, before applying the `Conv2d` layer, has C=1 since its a gray image.

The summary shows we have 80 parameters for the first convolution, since there are 9 parameters for the 3$\times$3 kernel plus one additive parameters (bias). In total there are 10 parameters for each convolution map. However, since the depth of the output of the convulational layer is 8, i.e., the model extracts 8 different feature maps, and hence uses 80 parameters for the `Conv2d`layer.

`ReLu`, `MaxPool2d` or `Flatten` do not add new parameters. However, `MaxPool2d` reduces the size of each feature map to 4$\times$4, which means that after flattenning it, there are only 16 values per map to be fed into the fully connected linear layers. For instance, `Linear-5` ingests those 16$\times$8=128 values and maps them to a 8-node layer. The number of parameters for `Linear-5` is then (128+1)$\times$8=1032.

### Adapt parameters to a different dataset

The code above shows how to adapt the script that *that implements a convolutional neural network with PyTorch over the mnist 8 by 8 practice data set* to a much larger data set called [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html). Each example in `CIFAR10` is a color image of size 32$\times$32. This means that C=3, H=32 and W=32, while in the previous example with `MNIST`, C=1, H=8 and W=8.

At this point we use the same small convolutional neural network as before. The output size, which is the number of classes is still 10 for `CIFAR10`.

Firstly, **change your runtime to CPU** (we will see in the following example how to use GPU) and run the *Improved (more modular) script that implements a neural network with PyTorch over the mnist 8 by 8 practice data set* to define the auxiliary functions that this script uses.


```python
#@title Script that adapts the CNN designed with PyTorch for MNIST to the CIFAR10 data set

'''
This code does the following:
    Splits the dataset into training and testing sets.
    Standardizes the features using StandardScaler.
    Reshapes dataset to fit the model
    Instantiates the model (NN or CNN)
    Defines the loss function (Cross Entropy Loss) and optimizer (Adam).
    Trains the model for num_epochs epochs.
    Tests the trained model on the test set and evaluates the accuracy.
'''

import torch
import torch.nn as nn
import torch.optim as optim
from torchsummary import summary
from torch.utils.data import DataLoader, TensorDataset
import torchvision
import torchvision.transforms as transforms
from sklearn.datasets import  load_digits
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import random
import numpy as np

################################################################################ Data and parameters
SHOW=True # show some images

# parameter constants
test_size=0.2
hidden_size = 8
batch_size= 250
num_epochs = 5
# Optimizer specific options
learning_rate=0.001
regularization_param=0.001
# Dropout: if p>0
dropout_p=0.1 # During training, randomly zeroes some of the elements of the input tensor with probability p.

transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

# CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class
train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)

test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)

train_dl = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,shuffle=True)

test_dl = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,shuffle=False)

classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

def imshow(img, labels):
        img = img * 0.5 + 0.5  # unnormalize
        npimg = img.numpy()
        print(npimg.shape)
        plt.imshow(np.transpose(npimg, (1, 2, 0)))
        plt.title(' '.join('%5s' % classes[labels[j]] for j in range(batch_size)))
        plt.show()

if SHOW:
    # get some random training images
    dataiter = iter(train_dl)
    images, labels = next(dataiter)
    print(images[0].shape) # 3*32*32
    print(type(images[0])) # torch tensor
    # show images
    image_batch=torchvision.utils.make_grid(images)
    #print(image_batch[0].shape)
    #imshow(image_batch, labels)


# Instantiate the model
dataiter = iter(train_dl)
images, labels = next(dataiter)
(C,H,W)=images[0].shape # 3*32*32
output_size = len(classes)

########################################################################### train and test, pre-processing

if SHOW:
    class_names = [str(i) for i in range(10)]
    # Plot the images
    plt.figure(figsize=(10, 5))
    image_count = 0
    for images, labels in train_dl:
        for i in range(len(images)):
            plt.subplot(4, 5, image_count + 1)
            plt.imshow(np.transpose(images[i], (1, 2, 0)), cmap="gray")
            plt.title(class_names[labels[i]])
            plt.axis('off')
            image_count += 1
            if image_count >= 20:
                break
        if image_count >= 20:
            break
    plt.show()

###################################################################################### CNN  model
model=nn.Sequential(
    nn.Conv2d(in_channels=C,out_channels=8,kernel_size=3,padding=1),
    nn.ReLU(),
    nn.MaxPool2d(kernel_size=2),
    nn.Flatten(),
    nn.Linear(2*W*H, hidden_size),
    nn.ReLU(),
    nn.Linear(hidden_size, hidden_size),
    nn.ReLU(),
    nn.Dropout(p=dropout_p),
    nn.Linear(hidden_size, output_size)
)

# to the correct processor:: 'cpu' or 'cuda'
#model=model.to('cpu')

# model description
summary(model,(C,H,W)) # C, H, W

# Define loss function and optimizer
# Either torch.nn.NLLLoss or torch.nn.CrossEntropyLoss can be used: CrossEntropyLoss (https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) implements softmax internally
loss_fn = nn.CrossEntropyLoss()

# Optimizer: optimizer object that will hold the current state and will update the parameters based on the computed gradients
# for param in model.parameters(): print(param.data)
optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=regularization_param)

# Train the model and predict on test samples to estimate accuracy
# history stores losses, accuracy, actual labels and predictions
history = train(model, optimizer, loss_fn, num_epochs, train_dl, test_dl)

# plot losses along epochs
plot_losses(history)
# plot confusion matrix
plot_accuracy_from_predictions(history)
#plot_accuracy(hist)

```

    Files already downloaded and verified
    Files already downloaded and verified
    

    WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
    WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
    WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
    

    torch.Size([3, 32, 32])
    <class 'torch.Tensor'>
    

    WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
    WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
    WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
    WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
    WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
    WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
    WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
    WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
    WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
    WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
    WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
    WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
    WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
    WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
    WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
    WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
    WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
    


    
![png](ML_overview_with_examples_files/ML_overview_with_examples_184_4.png)
    


    ----------------------------------------------------------------
            Layer (type)               Output Shape         Param #
    ================================================================
                Conv2d-1            [-1, 8, 32, 32]             224
                  ReLU-2            [-1, 8, 32, 32]               0
             MaxPool2d-3            [-1, 8, 16, 16]               0
               Flatten-4                 [-1, 2048]               0
                Linear-5                    [-1, 8]          16,392
                  ReLU-6                    [-1, 8]               0
                Linear-7                    [-1, 8]              72
                  ReLU-8                    [-1, 8]               0
               Dropout-9                    [-1, 8]               0
               Linear-10                   [-1, 10]              90
    ================================================================
    Total params: 16,778
    Trainable params: 16,778
    Non-trainable params: 0
    ----------------------------------------------------------------
    Input size (MB): 0.01
    Forward/backward pass size (MB): 0.16
    Params size (MB): 0.06
    Estimated Total Size (MB): 0.23
    ----------------------------------------------------------------
    


    
![png](ML_overview_with_examples_files/ML_overview_with_examples_184_6.png)
    


    Accuracy on test set: 0.4247
    


    
![png](ML_overview_with_examples_files/ML_overview_with_examples_184_8.png)
    


### Receptive Fields

The *receptive field* is the area of an image that is involved in the calculation of a layer.

<img alt="Secondary precedents of conv2 layer" width="700" caption="Secondary precedents of Conv2 layer" id="preced2" src="https://github.com/fastai/fastbook/blob/master/images/att_00069.png?raw=1">

In this example, we have just two convolutional layers, each of stride 2, so this is now tracing right back to the input image. We can see that a 77 area of cells in the input layer is used to calculate the single green cell in the Conv2 layer. This 77 area is the *receptive field* in the input of the green activation in Conv2. We can also see that a second filter kernel is needed now, since we have two layers.

As you see from this example, the deeper we are in the network (specifically, the more stride-2 convs we have before a layer), the larger the receptive field for an activation in that layer. A large receptive field means that a large amount of the input image is used to calculate each activation in that layer is. We now know that in the deeper layers of the network we have semantically rich features, corresponding to larger receptive fields. Therefore, we'd expect that we'd need more weights for each of our features to handle this increasing complexity. This is another way of saying the same thing we mentioned in the previous section: when we introduce a stride-2 conv in our network, we should also increase the number of channels.

### CNN as Encoders

[LeNet](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf),  [AlexNet](https://dl.acm.org/doi/10.1145/3065386) and  [VGG Net](https://arxiv.org/abs/1409.1556) are examples of convolutional neural networks for image classification. Typically, they have an input layer which is a tensor that represents an image with dimension *(number rows, number columns, number channels)*, followed by sets of *convolutional* layers, *ReLu* layers, and *pooling* layers, and at the end they have a couple of   *fully connected layers*  followed by a *sofwmax* or a *sigmoid layer*.

The idea behind those kind of architectures is to extrat *deep* features through the sequential application of convolutional and maxpool (or stride larger than 1) layers. In that sense, the network is designed to encode deep features of the input into its deep layers.

LeNet:

<img src="https://drive.google.com/uc?export=view&id=1-Nxj4sRWiWmjWcPgVnqcoSItiuJo38oH" width="700" >


VGG-net:

<img src="https://drive.google.com/uc?export=view&id=1hoR6Qxda7Ls0VsY-xAzSz1ddlL6IEAnT" width="700" >



Those kind of networks exhibit the following structure:
1.  reduction of width and height dimensions of input through each layer in this network;
2. accompanied by an organized increment in the number of features (channels) in each layer.




### Resnets

The early convolutional neural network had a few layers and possibly large kernels (up to 11$\times$11) for the  [AlexNet](https://dl.acm.org/doi/10.1145/3065386). Later influential models like the [VGG Net](https://arxiv.org/abs/1409.1556) used very small convolution filters and deeper networks which are more powerfull but also are more difficult to train due to the so-called *degradation problem*:  with the network depth increasing, accuracy gets saturated and then degrades rapidly.

That problem can be overcome with residual networks known as [resnets](https://arxiv.org/abs/1512.03385). The main idea is that it is easier to train neural network to find a vanishing mapping than to approximate more complicated functions. The residual block (shown below) converts the problem of training $F(x)$ for an arbitrary expected output $H(x)$ into the problem of training $F(x)$ for $H(x)-x$, which is a residual and should ideally vanish.

<img src="https://miro.medium.com/v2/resize:fit:1140/format:webp/1*D0F3UitQ2l5Q0Ak-tjEdJg.png" width="400" >


As an exemple, see below the full diagram that describes `resnet18`. The arrows represent the application of the identity function. That architecture, proposed in https://arxiv.org/abs/1512.03385, reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. Those residual networks are easier to optimize, and can gain accuracy from considerably increased depth.

<img src="https://www.researchgate.net/profile/Sajid-Iqbal-13/publication/336642248/figure/fig1/AS:839151377203201@1577080687133/Original-ResNet-18-Architecture_W640.jpg" width="700" >

## Image segmentation

Image segmentation (see for instance https://www.ibm.com/topics/image-segmentation) is a computer vision technique that partitions a digital image into discrete groups of pixels called image segments.

Unlike image classification, where the entire image is one example to be labeled, image segmentation processes visual data at the pixel level, using various techniques to annotate individual pixels as belonging to a specific class or instance.

Traditional image segmentation techniques use information from a pixel's color values and related characteristics like brightness, contrast or intensity  for feature extraction. Neural networks of deep learning image segmentation models are trained on annotated dataset of images and tend to require much more computational resourses. Despite those tradeoffs in computing requirements and training time, deep learning models consistently outperform traditional models and form the basis of most ongoing advancements in computer vision.

According to https://www.ibm.com/topics/image-segmentation, prominent deep learning models used in image segmentation include:

1. Fully Convolutional Networks (FCNs): FCNs, often used for semantic segmentation, are a type of convolutional neural network (CNN) with no fixed layers. An encoder network passes visual input data through convolutional layers to extract features relevant to segmentation or classification, and compresses (or downsamples) this feature data to remove non-essential information. This compressed data is then fed into decoder layers, upsampling the extracted feature data to reconstruct the input image with segmentation masks.

2. U-Nets: U-Nets modify FCN architecture to reduce data loss during downsampling with skip connections, preserving greater detail by selectively bypassing some convolutional layers as information and gradients move through the neural network. Its name is derived from the shape of diagrams demonstrating the arrangement of its layers.

3. Deeplab: Like U-Nets, Deeplab is a modified FCN architecture. In addition to skip connections, it uses diluted (or atrous) convolution to yield larger output maps without necessitating additional computational power.

4. Mask R-CNNs: Mask R-CNNs are a leading model for instance segmentation. Mask R-CNNs combine a region proposal network (RPN) that generates bounding boxes for each potential instance with an FCN-based mask head that generates segmentation masks within each confirmed bounding box.

5. Transformers: inspired by the success of transformer models in natural language processing, new models like Vision Transformer (ViT) using attention mechanisms in place of convolutional layers have matched or exceeded CNN performance for computer vision tasks.

Below, the architecture of U-nets (https://arxiv.org/abs/1505.04597) is illustrated. The U shape of the diagram represntes the encoder part, that converts input into a small and deep feature map, followed by a decoder part that generates a output with the same number of rows and columns as the input. The output's pixels are labeled such that the it provides a segmentation of the input image.


<img src="https://www.frontiersin.org/files/Articles/841297/fnagi-14-841297-HTML-r2/image_m/fnagi-14-841297-g001.jpg" width="700" >

This approach for image segmentation can be applied to large images by an *overlap-tile strategy* as illustrated by Figure 2 in https://arxiv.org/abs/1505.04597.

A commented example of the use of a U-Net to segment street photos for  self-driving cars is available at [Image_Segmentation_with_Unet.ipynb](Image_Segmentation_with_Unet.ipynb). This is implemented with the `fastai` package.





## Pre-trained models and transfer learning

One of the main tools available im Machine Learning is called *Transfer learning* which allows us to leverage powerful resources that are already available. Currently, there are many pre-trained models freely available, which have been trained with large amounts of data. We can access those pre-trained models to solve the problem at hand. In order to do that, we need:
1. To adapt the input size;



```python
#@title PyTorch script that uploads a pre-trained Resnet for the Cifar10 data set
# code adapted from https://github.com/rasbt/machine-learning-book/blob/main/ch14/ch14_part1.py


# todo -- just with PyTorch
# 1. Read CIFAR10
# 2. Upload pre-trained resnet
# 3. Customize model to the data
# 4. Fine tune
# 5. Save tuned model
# 6. predict with savel model

'''
This code does the following:
    Splits the dataset into training and testing sets.
    Standardizes the features using StandardScaler.
    Reshapes dataset to fit the model
    Instantiates the model (CNN)
    Defines the loss function (Cross Entropy Loss) and optimizer (Adam).
    Trains the model for num_epochs epochs.
    Tests the trained model on the test set and evaluates the accuracy.
'''

import torch
import torch.nn as nn
import torch.optim as optim
from torchsummary import summary
from torch.utils.data import DataLoader, TensorDataset
import torchvision
import torchvision.transforms as transforms
from torchvision.models import resnet18, ResNet18_Weights
from sklearn.datasets import  load_digits
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import random
import numpy as np
from pathlib import Path
from tqdm import tqdm

################################################################################  functions

def train(model, optimizer, loss_fn, num_epochs, train_dl, valid_dl):
    '''
    Main function to train and test the model
    '''
    # lists to strore losses and accuracies
    loss_hist_train = [0] * num_epochs
    accuracy_hist_train = [0] * num_epochs
    loss_hist_valid = [0] * num_epochs
    accuracy_hist_valid = [0] * num_epochs
    # main loop through epochs
    for epoch in range(num_epochs):
        # training mode
        model.train()
        for x_batch, y_batch in tqdm(train_dl):
            x_batch,y_batch=x_batch.to(device),y_batch.to(device)  # edited
            # core of the learning process: predict and fit
            pred = model(x_batch)
            loss = loss_fn(pred, y_batch)
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            # compute train loss and accuracy
            loss_hist_train[epoch] += loss.item()*y_batch.size(0)
            is_correct = (torch.argmax(pred, dim=1) == y_batch).float()
            accuracy_hist_train[epoch] += is_correct.sum()
        # compute average loss per epoch
        loss_hist_train[epoch] /= len(train_dl.dataset)
        accuracy_hist_train[epoch] /= len(train_dl.dataset)
        # we also put the model in evaluation mode, so that specific layers such as dropout or batch normalization layers behave correctly.
        model.eval()
        with torch.no_grad():
            for x_batch, y_batch in valid_dl:
                x_batch,y_batch=x_batch.to(device),y_batch.to(device) # edited
                # predict
                pred = model(x_batch)
                loss = loss_fn(pred, y_batch)
                loss_hist_valid[epoch] += loss.item()*y_batch.size(0)
                is_correct = (torch.argmax(pred, dim=1) == y_batch).float()
                accuracy_hist_valid[epoch] += is_correct.sum()
                if epoch==0:
                    preds,actuals=torch.argmax(pred, dim=1),y_batch
                else:
                    preds=torch.cat((preds,torch.argmax(pred, dim=1)),dim=0)
                    actuals=torch.cat((actuals,y_batch),dim=0)
        # compute average loss per epoch
        loss_hist_valid[epoch] /= len(valid_dl.dataset)
        accuracy_hist_valid[epoch] /= len(valid_dl.dataset)
        # print accuracy
        if (epoch+1) % 100==0:
            print(f'Epoch {epoch+1} accuracy: {accuracy_hist_train[epoch]:.4f} val_accuracy: {accuracy_hist_valid[epoch]:.4f}')
    return loss_hist_train, loss_hist_valid, accuracy_hist_train, accuracy_hist_valid, preds,actuals


def plot_accuracy_from_predictions(hist):
    ''' Creates and prints confusion matrix from a model and a set of examples
    Inputs
    ------
    hist: tuple
        where hist[4] is the list of predicted values for test and hist[5] are the actual labels
    '''
    pred=[t.item() for t in hist[4]]
    actual=[t.item() for t in hist[5]]
    labels = np.unique(actual)
    disp = ConfusionMatrixDisplay.from_predictions(actual,pred,labels=labels)
    # print global accuracy
    accuracy=np.sum(np.diagonal(disp.confusion_matrix))/np.sum(disp.confusion_matrix)
    print(f'Accuracy on test set: {accuracy:.4f}')
    plt.show()

# TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
def plot_losses(hist):
    ''' plots train and test loss
    Input
    ------
    history, the output of function train()
    '''
    x_arr = np.arange(len(hist[0])) + 1
    fig = plt.figure(figsize=(12, 4))
    ax = fig.add_subplot(1, 2, 1)
    ax.plot(x_arr, hist[0] , '-o', label='Train loss')
    ax.plot(x_arr, hist[1], '--<', label='Test loss')
    ax.set_xlabel('Epoch', size=15)
    ax.set_ylabel('Loss', size=15)
    ax.legend(fontsize=15)
    ax = fig.add_subplot(1, 2, 2)
    ax.plot(x_arr, [t.item() for t in hist[2]], '-o', label='Train acc.') # tensors in cuda cannot be plotted; .item extracts the value to cpu
    ax.plot(x_arr, [t.item() for t in hist[3]], '--<', label='Test acc.')
    ax.legend(fontsize=15)
    ax.set_xlabel('Epoch', size=15)
    ax.set_ylabel('Accuracy', size=15)
    plt.show()

################################################################################ Data and parameters
SHOW=False # show some images
DOWNLOAD=True

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # checks is cuda (GPU) is available
print('device',device)

# parameter constants
test_size=0.2
hidden_size = 8
batch_size= 500
num_epochs = 3
# CIFAR-10 images
NUM_CLASSES=10
# Optimizer specific options
learning_rate=0.001
regularization_param=0.001
# Dropout: if p>0
dropout_p=0.1 # During training, randomly zeroes some of the elements of the input tensor with probability p.

# Data augmentation and normalization for training
# https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_illustrations.html#sphx-glr-auto-examples-transforms-plot-transforms-illustrations-py
# Just normalization for validation
data_transforms = {
    'train': transforms.Compose([
        transforms.RandomResizedCrop(224), # see "Size matters" https://arxiv.org/pdf/2102.01582.pdf
        transforms.RandomHorizontalFlip(),  #optional
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ]),
    'test': transforms.Compose([
        transforms.Resize(256), # bilinear by default
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
}


# Create directory to store images
path=Path('./gdrive/MyDrive/PML_2024/cifar10')
if not path.exists():
    path.mkdir(exist_ok=True, parents=True)

# read CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class
train_dataset = torchvision.datasets.CIFAR10(root=path, train=True,download=DOWNLOAD, transform=data_transforms['train'])
test_dataset = torchvision.datasets.CIFAR10(root=path, train=False,download=DOWNLOAD, transform=data_transforms['test'])
#after transform
C=3
H=224
W=224

# CIFAR10 classes
class_names = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

train_dl = DataLoader(train_dataset, batch_size=batch_size,shuffle=True)
test_dl = DataLoader(test_dataset, batch_size=batch_size,shuffle=False)

def imshow(inp, title=None):
    """Display image for Tensor."""
    inp = inp.numpy().transpose((1, 2, 0))
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    inp = std * inp + mean
    inp = np.clip(inp, 0, 1)
    plt.imshow(inp)
    if title is not None:
        plt.title(title)
    plt.pause(0.001)  # pause a bit so that plots are updated

if SHOW:
    # Get a batch of training data
    inputs, classes = next(iter(train_dl))
    # Make a grid from batch
    out = torchvision.utils.make_grid(inputs)
    imshow(out, title=[class_names[x] for x in classes])
    plt.show()

###################################################################################### upload resnet model


model = resnet18(pretrained=True)
model.fc=nn.Linear(512, NUM_CLASSES)

model.requires_grad_(False)
model.fc.requires_grad_(True)

# to the correct processor
model=model.to(device)

# model description
summary(model,(C,H,W)) # C, H, W

# Define loss function and optimizer
# Either torch.nn.NLLLoss or torch.nn.CrossEntropyLoss can be used: CrossEntropyLoss (https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) implements softmax internally
loss_fn = nn.CrossEntropyLoss()

# Optimizer: optimizer object that will hold the current state and will update the parameters based on the computed gradients
# for param in model.parameters(): print(param.data)
optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=regularization_param)

# Train the model and predict on test samples to estimate accuracy
# history stores losses, accuracy, actual labels and predictions
history = train(model, optimizer, loss_fn, num_epochs, train_dl, test_dl)

# plot losses along epochs
plot_losses(history)
# plot confusion matrix
plot_accuracy_from_predictions(history)
#plot_accuracy(hist)

```

    device cuda
    Files already downloaded and verified
    Files already downloaded and verified
    

    /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
      warnings.warn(
    /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
      warnings.warn(msg)
    

    ----------------------------------------------------------------
            Layer (type)               Output Shape         Param #
    ================================================================
                Conv2d-1         [-1, 64, 112, 112]           9,408
           BatchNorm2d-2         [-1, 64, 112, 112]             128
                  ReLU-3         [-1, 64, 112, 112]               0
             MaxPool2d-4           [-1, 64, 56, 56]               0
                Conv2d-5           [-1, 64, 56, 56]          36,864
           BatchNorm2d-6           [-1, 64, 56, 56]             128
                  ReLU-7           [-1, 64, 56, 56]               0
                Conv2d-8           [-1, 64, 56, 56]          36,864
           BatchNorm2d-9           [-1, 64, 56, 56]             128
                 ReLU-10           [-1, 64, 56, 56]               0
           BasicBlock-11           [-1, 64, 56, 56]               0
               Conv2d-12           [-1, 64, 56, 56]          36,864
          BatchNorm2d-13           [-1, 64, 56, 56]             128
                 ReLU-14           [-1, 64, 56, 56]               0
               Conv2d-15           [-1, 64, 56, 56]          36,864
          BatchNorm2d-16           [-1, 64, 56, 56]             128
                 ReLU-17           [-1, 64, 56, 56]               0
           BasicBlock-18           [-1, 64, 56, 56]               0
               Conv2d-19          [-1, 128, 28, 28]          73,728
          BatchNorm2d-20          [-1, 128, 28, 28]             256
                 ReLU-21          [-1, 128, 28, 28]               0
               Conv2d-22          [-1, 128, 28, 28]         147,456
          BatchNorm2d-23          [-1, 128, 28, 28]             256
               Conv2d-24          [-1, 128, 28, 28]           8,192
          BatchNorm2d-25          [-1, 128, 28, 28]             256
                 ReLU-26          [-1, 128, 28, 28]               0
           BasicBlock-27          [-1, 128, 28, 28]               0
               Conv2d-28          [-1, 128, 28, 28]         147,456
          BatchNorm2d-29          [-1, 128, 28, 28]             256
                 ReLU-30          [-1, 128, 28, 28]               0
               Conv2d-31          [-1, 128, 28, 28]         147,456
          BatchNorm2d-32          [-1, 128, 28, 28]             256
                 ReLU-33          [-1, 128, 28, 28]               0
           BasicBlock-34          [-1, 128, 28, 28]               0
               Conv2d-35          [-1, 256, 14, 14]         294,912
          BatchNorm2d-36          [-1, 256, 14, 14]             512
                 ReLU-37          [-1, 256, 14, 14]               0
               Conv2d-38          [-1, 256, 14, 14]         589,824
          BatchNorm2d-39          [-1, 256, 14, 14]             512
               Conv2d-40          [-1, 256, 14, 14]          32,768
          BatchNorm2d-41          [-1, 256, 14, 14]             512
                 ReLU-42          [-1, 256, 14, 14]               0
           BasicBlock-43          [-1, 256, 14, 14]               0
               Conv2d-44          [-1, 256, 14, 14]         589,824
          BatchNorm2d-45          [-1, 256, 14, 14]             512
                 ReLU-46          [-1, 256, 14, 14]               0
               Conv2d-47          [-1, 256, 14, 14]         589,824
          BatchNorm2d-48          [-1, 256, 14, 14]             512
                 ReLU-49          [-1, 256, 14, 14]               0
           BasicBlock-50          [-1, 256, 14, 14]               0
               Conv2d-51            [-1, 512, 7, 7]       1,179,648
          BatchNorm2d-52            [-1, 512, 7, 7]           1,024
                 ReLU-53            [-1, 512, 7, 7]               0
               Conv2d-54            [-1, 512, 7, 7]       2,359,296
          BatchNorm2d-55            [-1, 512, 7, 7]           1,024
               Conv2d-56            [-1, 512, 7, 7]         131,072
          BatchNorm2d-57            [-1, 512, 7, 7]           1,024
                 ReLU-58            [-1, 512, 7, 7]               0
           BasicBlock-59            [-1, 512, 7, 7]               0
               Conv2d-60            [-1, 512, 7, 7]       2,359,296
          BatchNorm2d-61            [-1, 512, 7, 7]           1,024
                 ReLU-62            [-1, 512, 7, 7]               0
               Conv2d-63            [-1, 512, 7, 7]       2,359,296
          BatchNorm2d-64            [-1, 512, 7, 7]           1,024
                 ReLU-65            [-1, 512, 7, 7]               0
           BasicBlock-66            [-1, 512, 7, 7]               0
    AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0
               Linear-68                   [-1, 10]           5,130
    ================================================================
    Total params: 11,181,642
    Trainable params: 5,130
    Non-trainable params: 11,176,512
    ----------------------------------------------------------------
    Input size (MB): 0.57
    Forward/backward pass size (MB): 62.79
    Params size (MB): 42.65
    Estimated Total Size (MB): 106.01
    ----------------------------------------------------------------
    

    100%|| 100/100 [01:36<00:00,  1.04it/s]
    100%|| 100/100 [01:35<00:00,  1.04it/s]
    100%|| 100/100 [01:35<00:00,  1.04it/s]
    


    
![png](ML_overview_with_examples_files/ML_overview_with_examples_197_4.png)
    


    Accuracy on test set: 0.7216
    


    
![png](ML_overview_with_examples_files/ML_overview_with_examples_197_6.png)
    


# Deploying ML models in production


<img src="https://dezyre.gumlet.io/images/blog/machine-learning-model-deployment/Machine_Learning_Model_Deployment_Tutorial.png?w=1100&dpr=1.5" width="700" >

Some useful links:

1. Saving and loading `PyTorch`models:  https://pytorch.org/tutorials/beginner/saving_loading_models.html


2. Steps to create a web application with `Gradio` to deploy a `PyTorch` model for image classification: https://www.gradio.app/guides/image-classification-in-pytorch

3. Gradio + HuggingFace Spaces: A Tutorial: https://www.tanishq.ai/blog/posts/2021-11-16-gradio-huggingface.html


```python
#@title Script for deploying the corn disease classifier with Gradio

import gradio as gr
import torch
from torchvision import transforms
from torchvision.models import resnet18, ResNet18_Weights
from torch import nn
from PIL import Image # pip install pillow

labels = ['Blight','Common_Rust','Gray_Leaf_Spot','Healthy']

# Same data transformation that was used for inputs (except data augmentation)
data_transform = transforms.Compose([
    transforms.Resize(size=(256, 256)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                        std=[0.229, 0.224, 0.225])
])

# https://pytorch.org/tutorials/beginner/saving_loading_models.html
# Loading Model for Inference with state_dict (recommended)
model = resnet18(weights=ResNet18_Weights.DEFAULT)
model.fc = nn.Linear(in_features=512, out_features=len(labels))
model.load_state_dict(torch.load("model.pth",map_location=torch.device('cpu')))
model.eval()

def predict(img):
    X = data_transform(img).unsqueeze(0) # returns tensor
    with torch.no_grad():
            predictions = model(X).flatten()
            predictions = torch.nn.functional.softmax(predictions)
            confidences = {labels[i]: float(predictions[i]) for i in range(len(labels))}
    return confidences

demo=gr.Interface(fn=predict,
             inputs=gr.Image(type="pil"),
             outputs=gr.Label(num_top_classes=len(labels)),
             examples=["Corn_Blight.jpg", "Corn_Common_Rust.jpg","Corn_Gray_Spot.jpg","Corn_Health.jpg"])

demo.launch('share=True')
```

The following figure shows the list of files that were created/uploaded to the Hugging Face space in order to create the app available at

<img src="https://drive.google.com/uc?export=view&id=1--lEvbmxYxZN8kWsaBc6nNbdi-jLvYoD" width="600" >


