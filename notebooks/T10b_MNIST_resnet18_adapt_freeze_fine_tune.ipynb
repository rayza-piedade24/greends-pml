{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyO0aRB/WwCPFfP5YZy1mBRm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isa-ulisboa/greends-pml/blob/main/notebooks/T10b_MNIST_resnet18_adapt_freeze_fine_tune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load pre-trained model (`resnet 18`) and fine-tune. Freeze layers for speed-up."
      ],
      "metadata": {
        "id": "0hP-7N--05QU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following script shows how to access a pre-trained `resnet18` model and  view its architecture and the names of its layers in PyTorch."
      ],
      "metadata": {
        "id": "t7NYqQbLoIvB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-w-z8Uh8oFAk"
      },
      "outputs": [],
      "source": [
        "import torchvision.models as models\n",
        "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Changes to be made on the model's design"
      ],
      "metadata": {
        "id": "QKpsbLVCpsl9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the input layer is described as `(conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)`. For the MNIST input data, there is a single channel (instead of 3). Therefore, the input layer will need to be replaced by `(conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)`.\n",
        "\n",
        "In addition, since there are only 10 classes (0 to 9) in the MNIST classification problem, the last layer `(fc): Linear(in_features=512, out_features=1000, bias=True)`needs to be adapted to `(fc): Linear(in_features=512, out_features=10, bias=True)`"
      ],
      "metadata": {
        "id": "MHNpC4X2pL4K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline data preparation, loading and adapting pre-trained model and fine-tuning"
      ],
      "metadata": {
        "id": "lOj5qgVaqH-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: if you try to execute the code below, you will see that training takes a long time since we are using the full MNIST train dataset."
      ],
      "metadata": {
        "id": "kA7q1mDM0JNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms, models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 1. Data preprocessing: resize to 224x224 and normalize\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# 2. Load ResNet and modify layers\n",
        "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)  # 1 input channel\n",
        "model.fc = nn.Linear(model.fc.in_features, 10)  # 10 output classes\n",
        "\n",
        "# 3. Training setup\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 4. Training loop (simplified)\n",
        "model.train()\n",
        "for epoch in range(3):\n",
        "    print('epoch',epoch)\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n"
      ],
      "metadata": {
        "id": "b1_lNrR_oj-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Freezing layers to speed-up fine-tuning"
      ],
      "metadata": {
        "id": "-KgVprPFrPaw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Freezing part of the adapted ResNet18 can speed up fine-tuning and is a common strategy in transfer learning.\n",
        "\n",
        "- **Why Freeze Layers?**\n",
        "Early layers of ResNet18 typically learn generic features (edges, textures) that are useful for many vision tasks, including MNIST digit classification. By freezing these layers (setting `requires_grad=False`), you reduce the number of parameters that need updating, which speeds up training and lowers memory usage.\n",
        "- **Which Layers to Freeze?**\n",
        "For a simple dataset like MNIST, you can freeze most of the early layers and only fine-tune the later layers and the final classification head. This is because the later layers capture more task-specific features, which are more likely to need adaptation for your new dataset.\n",
        "- **How to Freeze in PyTorch?**\n",
        "In practice, we start by freezing all parameters, and then we unfreeze the parameters in the later layers."
      ],
      "metadata": {
        "id": "UhBp11nZrYiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters():\n",
        "    param.requires_grad = False  # Freeze all layers\n",
        "\n",
        "# Unfreeze the last block and the classifier\n",
        "for param in model.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in model.fc.parameters():\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "id": "mxPmBD-yrsAg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In short, freezing part of ResNet18 (especially the early layers) is recommended to speed up fine-tuning on MNIST and similar tasks. Fine-tune only the last few layers and the classifier for efficient and effective adaptation."
      ],
      "metadata": {
        "id": "yfZpmEV0zeGm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In practice, one needs to add the code above after loading and adapting the model but **before** setting the **optimizer** since it should only include **trainable parameters** (see script below).\n",
        "\n",
        "If you include non-trainable (frozen) parameters in the optimizer, the optimizer will waste resources tracking parameters that are not meant to be updated, which can lead to inefficiency and unnecessary memory usage. More critically, if you change which parameters are trainable after creating the optimizer, the optimizer may still try to update the frozen parameters, potentially causing unexpected behavior or errors"
      ],
      "metadata": {
        "id": "iQtQgIljv9gt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Full pipeline for data preparation, loading and adapting pre-trained model, freezing layers and fine-tuning."
      ],
      "metadata": {
        "id": "dPbbdhCWzJR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms, models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 1. Data preprocessing: resize to 224x224 and normalize\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# 2. Load ResNet and modify layers\n",
        "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)  # 1 input channel\n",
        "model.fc = nn.Linear(model.fc.in_features, 10)  # 10 output classes\n",
        "\n",
        "# Freeze all parameters\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False  # Freeze all layers <<<<<<<<<<< Try False (to not freeze any parameters) and compare the time needed for training\n",
        "\n",
        "# Unfreeze the last block and the classifier\n",
        "for param in model.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# 3. Training setup\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# create optimizer using only trainable parameters\n",
        "optimizer = torch.optim.Adam(\n",
        "    [p for p in model.parameters() if p.requires_grad],\n",
        "    lr=0.001\n",
        ")\n",
        "\n",
        "# 4. Training loop\n",
        "model.train()\n",
        "for epoch in range(3):\n",
        "    print('epoch',epoch)\n",
        "    count=0\n",
        "    for images, labels in train_loader:\n",
        "        count+=64\n",
        "        print(count)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n"
      ],
      "metadata": {
        "id": "NlI8TTO_uznP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}